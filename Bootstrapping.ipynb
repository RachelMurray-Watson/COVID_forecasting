{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from num2words import num2words\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, KFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score\n",
    "import word2number\n",
    "from word2number import w2n\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pickle\n",
    "\n",
    "hfont = {'fontname':'Helvetica'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set wd to be a folder not on github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_directory = '/Users/rem76/Documents/COVID_projections/Bootstrapping/'\n",
    "os.chdir(new_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_names(categories_for_subsetting, num_of_weeks):\n",
    "    column_names = ['HSA_ID']\n",
    "\n",
    "    for week in range(1, num_of_weeks + 1):\n",
    "        week = num2words(week)\n",
    "        for category in categories_for_subsetting:\n",
    "            column_name = f'week_{week}_{category}'\n",
    "            column_names.append(column_name)\n",
    "\n",
    "    return column_names\n",
    "\n",
    "def create_collated_weekly_data(pivoted_table, original_data, categories_for_subsetting, geography, column_names):\n",
    "    collated_data = pd.DataFrame(index=range(51), columns=column_names)\n",
    "\n",
    "    x = 0\n",
    "    for geo in original_data[geography].unique():\n",
    "        #matching_indices = [i for i, geo_col in enumerate(pivoted_table) if geo_col == geo]\n",
    "        collated_data.loc[x, geography] = geo\n",
    "        columns_to_subset = [f'{geo}_{category}' for category in categories_for_subsetting]\n",
    "        j = 1\n",
    "        try:\n",
    "            for row in range(len(pivoted_table.loc[:, columns_to_subset])):\n",
    "                collated_data.iloc[x, j:j + len(categories_for_subsetting)] = pivoted_table.loc[row, columns_to_subset]\n",
    "                j += len(categories_for_subsetting)\n",
    "        except:\n",
    "            pass\n",
    "        x += 1\n",
    "\n",
    "    return collated_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(confusion_matrix):\n",
    "    # Extract values from the confusion matrix\n",
    "    TP = confusion_matrix[1, 1]\n",
    "    FP = confusion_matrix[0, 1]\n",
    "    TN = confusion_matrix[0, 0]\n",
    "    FN = confusion_matrix[1, 0]\n",
    "\n",
    "    # Calculate Sensitivity (True Positive Rate) and Specificity (True Negative Rate)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "\n",
    "    # Calculate PPV (Precision) and NPV\n",
    "    ppv = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    npv = TN / (TN + FN) if (TN + FN) > 0 else 0.0\n",
    "\n",
    "    return sensitivity, specificity, ppv, npv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_training_test_data_period(data, no_weeks, weeks_in_futre, if_train, geography, weight_col, keep_output):\n",
    "## Get the weeks for the x and y datasets   \n",
    "    x_weeks = []  \n",
    "    y_weeks = []\n",
    "    y_weeks_to_check = [] #check these weeks to see if any of them are equal to 1\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_futre\n",
    "        x_weeks.append('_' + num2words(week) + '_')\n",
    "        for week_y in range(week+1, test_week+1):\n",
    "                y_weeks_to_check.append('_' + num2words(week_y) + '_')\n",
    "        y_weeks.append('_' + num2words(test_week) + '_')\n",
    "    \n",
    "## Divide up the test/train split\n",
    "    #if is_geographic:\n",
    "        # Calculate the index to start slicing from\n",
    "    #    start_index = len(data['county']) // proportion[0] * proportion[1]\n",
    "        # Divide up the dataset based on this proportion\n",
    "    #    first_two_thirds = data['county'][:start_index]\n",
    "    #    last_third = data['county'][start_index:]\n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all =  pd.DataFrame()\n",
    "    missing_data = []\n",
    "    ## Now get the training data \n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "            y_week = y_weeks[k]\n",
    "            k +=1\n",
    "\n",
    "            weeks_x = [col for col in data.columns if x_week in col]\n",
    "            columns_x  = [geography] + weeks_x + [weight_col]\n",
    "            data_x = data[columns_x]\n",
    "\n",
    "            weeks_y = [col for col in data.columns if y_week in col]\n",
    "            columns_y  = [geography] + weeks_y\n",
    "            data_y = data[columns_y]\n",
    "            ### now add the final column to the y data that has it so that it's if any week in the trhee week perdiod exceeded 15\n",
    "            train_week = w2n.word_to_num(x_week.replace(\"_\", \"\"))\n",
    "            target_week =  w2n.word_to_num(y_week.replace(\"_\", \"\"))\n",
    "            y_weeks_to_check = []\n",
    "            for week_to_check in range(train_week + 1, target_week + 1):\n",
    "                y_weeks_to_check.append('_' + num2words(week_to_check) + '_')\n",
    "\n",
    "            y_weeks_to_check = [week + 'beds_over_15_100k' for week in y_weeks_to_check]\n",
    "            columns_to_check = [col for col in data.columns if any(week in col for week in y_weeks_to_check)]\n",
    "            y_over_in_period = data[columns_to_check].apply(max, axis=1)\n",
    "            data_y = pd.concat([data_y, y_over_in_period], axis=1)\n",
    "            # ensure they have the same amount of data\n",
    "            #remove rows in test_data1 with NA in test_data2\n",
    "            data_x = data_x.dropna()\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            # remove rows in test_data2 with NA in test_data1\n",
    "            data_y = data_y.dropna()\n",
    "            data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "            missing_data.append(((len(data[geography].unique()) - data_x_no_HSA)/len(data[geography].unique())) * 100)\n",
    "            # get weights \n",
    "            #weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "            X_week = data_x.iloc[:, 1:len(columns_x)]  # take away y, leave weights for mo\n",
    "            y_week = data_y.iloc[:, -1] \n",
    "            \n",
    "            y_week = y_week.astype(int)\n",
    "            if if_train:\n",
    "\n",
    "                 X_week, y_week = oversample.fit_resample(X_week, y_week)\n",
    "            weights = X_week.iloc[:, -1] \n",
    "            if keep_output:\n",
    "                X_week = X_week.iloc[:, :len(X_week.columns)-1] # remove the weights and leave \"target\" for that week\n",
    "\n",
    "                #rename columns for concatenation \n",
    "                X_week.columns = range(1, len(data_x.columns) -1)\n",
    "            else:\n",
    "                X_week = X_week.iloc[:, :len(X_week.columns)-2] # remove the weights and  \"target\" for that week\n",
    "\n",
    "                X_week.columns = range(1, len(data_x.columns) -2)# remove the weights and  \"target\" for that week\n",
    "\n",
    "            y_week.columns = range(1, len(data_y.columns) -2)\n",
    "            X_data = pd.concat([X_data, X_week])\n",
    "            y_data = pd.concat([y_data, y_week]) \n",
    "        \n",
    "            weights_all =  pd.concat([weights_all, weights]) \n",
    "\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return(X_data, y_data, weights_all, missing_data)\n",
    "\n",
    "\n",
    "### this code it's ANY in the x week period \n",
    "def prep_training_test_data(data, no_weeks, weeks_in_futre, if_train, geography, weight_col, keep_output):\n",
    "## Get the weeks for the x and y datasets   \n",
    "    x_weeks = []  \n",
    "    y_weeks = []\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_futre\n",
    "        x_weeks.append('_' + num2words(week) + '_')\n",
    "        y_weeks.append('_' + num2words(test_week) + '_')\n",
    "    \n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all =  pd.DataFrame()\n",
    "    missing_data = []\n",
    "    ## Now get the training data \n",
    "    for x_week in x_weeks:\n",
    "            y_week = y_weeks[0]\n",
    "            weeks_x = [col for col in data.columns if x_week in col]\n",
    "            columns_x  = [geography] + weeks_x + [weight_col]\n",
    "            data_x = data[columns_x]\n",
    "\n",
    "            weeks_y = [col for col in data.columns if y_week in col]\n",
    "            columns_y  = [geography] + weeks_y\n",
    "            data_y = data[columns_y]\n",
    "            # ensure they have the same amount of data\n",
    "            #remove rows in test_data1 with NA in test_data2\n",
    "            data_x = data_x.dropna()\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            # remove rows in test_data2 with NA in test_data1\n",
    "            data_y = data_y.dropna()\n",
    "            data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "            missing_data.append(((len(data[geography].unique()) - data_x_no_HSA)/len(data[geography].unique())) * 100)\n",
    "            # get weights \n",
    "            #weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "            X_week = data_x.iloc[:, 1:len(columns_x)]  # take away y, leave weights for mo\n",
    "            y_week = data_y.iloc[:, -1] \n",
    "            \n",
    "            y_week = y_week.astype(int)\n",
    "            if if_train:\n",
    "\n",
    "                 X_week, y_week = oversample.fit_resample(X_week, y_week)\n",
    "            weights = X_week.iloc[:, -1] \n",
    "            if keep_output:\n",
    "                X_week = X_week.iloc[:, :len(X_week.columns)-1] # remove the weights and leave \"target\" for that week\n",
    "\n",
    "                #rename columns for concatenation \n",
    "                X_week.columns = range(1, len(data_x.columns) -1)\n",
    "            else:\n",
    "                X_week = X_week.iloc[:, :len(X_week.columns)-2] # remove the weights and  \"target\" for that week\n",
    "\n",
    "                X_week.columns = range(1, len(data_x.columns) -2)# remove the weights and  \"target\" for that week\n",
    "\n",
    "                #rename columns for concatenation \n",
    "            y_week.columns = range(1, len(data_y.columns) -1)\n",
    "            X_data = pd.concat([X_data, X_week])\n",
    "            y_data = pd.concat([y_data, y_week]) \n",
    "        \n",
    "            weights_all =  pd.concat([weights_all, weights]) \n",
    "\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return(X_data, y_data, weights_all, missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentiles(iterations, model_name, ROC_actual, accuracy_actual, sensitivity_actual, specificity_actual, ppv_actual, npv_actual, X_test ,y_test):\n",
    "        bootstrapped_stats_ROC = []\n",
    "        bootstrapped_stats_accuracy = []\n",
    "        bootstrapped_stats_sesitivity = []\n",
    "        bootstrapped_stats_specificity = []\n",
    "        bootstrapped_stats_ppv = []\n",
    "        bootstrapped_stats_npv = []\n",
    "\n",
    "        for j in range(iterations):\n",
    "            model_name_to_load = model_name + \"_\" + str(j)+ \".sav\" \n",
    "            model_fit = pickle.load(open(model_name_to_load, 'rb'))\n",
    "            y_bootstrap_predict = model_fit.predict(X_test)\n",
    "\n",
    "            ROC_AUC_bootstrap_test_performance = metrics.roc_auc_score(y_test, y_bootstrap_predict) \n",
    "            accuracy_bootstrap_test_performance  = accuracy_score(y_test, y_bootstrap_predict)\n",
    "\n",
    "            sensitivity_bootstrap_test_performance, specificity_bootstrap_test_performance, ppv_bootstrap_test_performance, npv_bootstrap_test_performance = calculate_metrics(confusion_matrix(y_test, y_bootstrap_predict))\n",
    "        ### (D) Calculate estimate fo variance  by getting (B) - (D) \n",
    "\n",
    "            bootstrapped_stats_ROC.append({'Difference': ROC_AUC_bootstrap_test_performance - ROC_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            bootstrapped_stats_accuracy.append({'Difference': accuracy_bootstrap_test_performance - accuracy_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            bootstrapped_stats_sesitivity.append({'Difference': sensitivity_bootstrap_test_performance - sensitivity_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            bootstrapped_stats_specificity.append({'Difference': specificity_bootstrap_test_performance - specificity_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            bootstrapped_stats_ppv.append({'Difference': ppv_bootstrap_test_performance - ppv_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            bootstrapped_stats_npv.append({'Difference': npv_bootstrap_test_performance - npv_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "\n",
    "\n",
    "        bootstrapped_stats_ROC = pd.DataFrame(bootstrapped_stats_ROC)\n",
    "        bootstrapped_stats_accuracy = pd.DataFrame(bootstrapped_stats_accuracy)\n",
    "        bootstrapped_stats_sesitivity = pd.DataFrame(bootstrapped_stats_sesitivity)\n",
    "        bootstrapped_stats_specificity = pd.DataFrame(bootstrapped_stats_specificity)\n",
    "        bootstrapped_stats_ppv = pd.DataFrame(bootstrapped_stats_ppv)\n",
    "        bootstrapped_stats_npv = pd.DataFrame(bootstrapped_stats_npv)\n",
    "\n",
    "    ## Step 3: Get percentile\n",
    "        alpha = 0.05\n",
    "\n",
    "        upper_quartile_ROC, lower_quartile_ROC = ROC_actual - np.percentile(bootstrapped_stats_ROC[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        upper_quartile_accuracy, lower_quartile_accuracy = accuracy_actual - np.percentile(bootstrapped_stats_accuracy[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        upper_quartile_sensitivity, lower_quartile_sensitivity = sensitivity_actual - np.percentile(bootstrapped_stats_sesitivity[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        upper_quartile_specificity, lower_quartile_specificity = specificity_actual - np.percentile(bootstrapped_stats_specificity[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        upper_quartile_ppv, lower_quartile_ppv = ppv_actual - np.percentile(bootstrapped_stats_ppv[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        upper_quartile_npv, lower_quartile_npv = npv_actual - np.percentile(bootstrapped_stats_npv[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        ## Step 4: Get optimization-corrected performance\n",
    "\n",
    "        return upper_quartile_ROC, lower_quartile_ROC, upper_quartile_accuracy, lower_quartile_accuracy, upper_quartile_sensitivity, lower_quartile_sensitivity, upper_quartile_specificity, lower_quartile_specificity, upper_quartile_ppv, lower_quartile_ppv, upper_quartile_npv, lower_quartile_npv\n",
    "\n",
    "### now try bootstrapping w/o feature selection\n",
    "iterations = 100\n",
    "## DO NOT SAMPLE THE TARGET DATA\n",
    "def bootstrap_no_dev(iterations, model, data,model_name, time_period, no_weeks, keep_output, weeks_in_futre, if_train, geography, weight_col):\n",
    "      #1. Get dataset\n",
    "    for j in range(iterations):\n",
    "        #2. (A) Sample all individuals from training data w/replacement\n",
    "          if time_period == 'period':\n",
    "                X_sample_train, y_sample_train, weights_train, missing_data_train_HSA = prep_training_test_data_period(data, no_weeks = no_weeks, weeks_in_futre = weeks_in_futre, if_train = if_train, geography = geography, weight_col = weight_col,keep_output = keep_output)\n",
    "          elif time_period == 'exact':\n",
    "                X_sample_train, y_sample_train, weights_train, missing_data_train_HSA = prep_training_test_data(data, no_weeks = no_weeks, weeks_in_futre = weeks_in_futre, if_train = if_train, geography = geography, weight_col =weight_col,keep_output = keep_output)\n",
    "          elif time_period == 'shifted':\n",
    "                X_sample_train, y_sample_train, weights_train, missing_data_train_HSA = prep_training_test_data_period(data, no_weeks = no_weeks, weeks_in_futre = weeks_in_futre, if_train = if_train, geography = geography, weight_col = weight_col,keep_output = keep_output)\n",
    "                y_sample_train = y_sample_train.shift(-1)\n",
    "                y_sample_train.drop(index=y_sample_train.index[-1], inplace=True)\n",
    "                X_sample_train.drop(index=X_sample_train.index[-1], inplace=True)# want these data to stay the same\n",
    "                weights_train.drop(index=weights_train.index[-1], inplace=True)\n",
    "\n",
    "        # need to rejoin, resample, then seperate \n",
    "          training_data = pd.merge(X_sample_train, y_sample_train, left_index=True, right_index=True)\n",
    "          training_data = pd.merge(training_data, weights_train, left_index=True, right_index=True)\n",
    "\n",
    "          training_data_resampled = training_data.sample(frac = 1, replace=True)\n",
    "          weights_train = training_data_resampled.iloc[:,-1:] ##(a) sample n individuals with replacement\n",
    "          weights_train = weights_train.iloc[:,0]\n",
    "          training_data_resampled.drop(training_data_resampled.columns[-1],axis=1, inplace = True)\n",
    "          y_sample_train = training_data_resampled.iloc[:,-1:]\n",
    "          training_data_resampled.drop(training_data_resampled.columns[-1],axis=1, inplace = True)\n",
    "          X_sample_train = training_data_resampled\n",
    "        #  (B) Predictive model w/o feature selection \n",
    "\n",
    "          model_fit = model.fit(X_sample_train, y_sample_train, sample_weight = weights_train)\n",
    "\n",
    "          model_name_to_save = model_name + \"_\" + str(j)+ \".sav\" \n",
    "          X_data_name = model_name + \"_X_data_\" + str(j) + \".csv\" \n",
    "          y_data_name = model_name + \"_y_data_\" + str(j) + \".csv\" \n",
    "          weights_data_name = model_name + \"_weights_\" + str(j) + \".csv\" \n",
    "          \n",
    "          weights_train.to_csv(weights_data_name)\n",
    "          X_sample_train.to_csv(X_data_name)\n",
    "          y_sample_train.to_csv(y_data_name)\n",
    "          pickle.dump(model_fit, open(model_name_to_save, 'wb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive classifier bootstrapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_58822/3591632256.py:1: DtypeWarning: Columns (41,43,44,45,46,50,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_by_HSA = pd.read_csv('/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates.csv')\n"
     ]
    }
   ],
   "source": [
    "data_by_HSA = pd.read_csv('/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates.csv')\n",
    "data_by_HSA['health_service_area_number']\n",
    "data_by_HSA['health_service_area']\n",
    "#data_by_HSA['HSA_ID'] = data_by_HSA['health_service_area_number'].astype(str) + '' + data_by_HSA['health_service_area'].apply(lambda x: x.split()[0])\n",
    "data_by_HSA.rename(columns={'health_service_area_number': 'HSA_ID'}, inplace=True)\n",
    "\n",
    "data_by_HSA['beds_over_15_100k'] = (data_by_HSA['beds_weekly'] > 15)*1\n",
    "\n",
    "# remove HSAs that have missing data in specific columns\n",
    "\n",
    "data_by_HSA = data_by_HSA.dropna(subset=['admits_weekly', 'deaths_weekly', 'cases_weekly', 'icu_weekly', 'beds_weekly'])\n",
    "\n",
    "\n",
    "for i, week in enumerate(data_by_HSA['date'].unique()):\n",
    "    data_by_HSA.loc[data_by_HSA['date'] == week, 'week'] = i\n",
    "\n",
    "## naive with only above/belwo 15 per 199k in current week \n",
    "\n",
    "data_by_HSA_over_15_100k = data_by_HSA[['HSA_ID', 'week', 'beds_over_15_100k']]\n",
    "data_by_HSA_over_15_100k = data_by_HSA_over_15_100k.pivot_table(index= 'week', columns='HSA_ID', values='beds_over_15_100k')\n",
    "\n",
    "old_column_names = data_by_HSA_over_15_100k.columns\n",
    "new_column_names = [str(col) + '_beds_over_15_100k' for col in old_column_names]\n",
    "new_column_names = dict(zip(old_column_names, new_column_names))\n",
    "data_by_HSA_over_15_100k.rename(columns=new_column_names, inplace=True)\n",
    "data_by_HSA_over_15_100k = data_by_HSA_over_15_100k.reset_index()\n",
    "\n",
    "categories_for_subsetting =  ['beds_over_15_100k']\n",
    "num_of_weeks = len(data_by_HSA_over_15_100k)\n",
    "column_names = create_column_names(categories_for_subsetting, num_of_weeks)\n",
    "all_HSA_ID_weekly_data = create_collated_weekly_data(data_by_HSA_over_15_100k, data_by_HSA, categories_for_subsetting, 'HSA_ID', column_names)\n",
    "\n",
    "\n",
    "weights_df = data_by_HSA[data_by_HSA['HSA_ID'].isin(all_HSA_ID_weekly_data['HSA_ID'])][['HSA_ID','weight_alt']]\n",
    "weights_df = weights_df.rename(columns = {'HSA_ID': 'HSA_ID', 'weight_alt':'weight'})\n",
    "weights_df = weights_df.drop_duplicates()\n",
    "weights_df['weight'].unique()\n",
    "all_HSA_ID_weekly_data = all_HSA_ID_weekly_data.join(weights_df['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_naive_period = pickle.load(open(\"/Users/rem76/Documents/COVID_projections/COVID_forecasting/Naive_model_auroc_0.8134582.sav\" , 'rb'))\n",
    "\n",
    "# only bootstrap the training data\n",
    "\n",
    "\n",
    "bootstrap_no_dev(iterations =100, model = clf_naive_period, data = all_HSA_ID_weekly_data, model_name = \"Naive_classifier_period_boostrap\", time_period = 'period', no_weeks = range(1, int(123*2/3) + 1), weeks_in_futre = 3, if_train = False, geography = 'HSA_ID', weight_col = 'weight',keep_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, weights_test, missing_data_test_HSA = prep_training_test_data(all_HSA_ID_weekly_data, no_weeks = range(int(123*2/3) + 1, (123 - 3)), weeks_in_futre = 3, if_train = False, geography = 'HSA_ID',  weight_col = 'weight', keep_output = True) #only goes to 120 as it's weeks in the future\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m calculate_percentiles(iterations \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m, model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNaive_classifier_period_boostrap\u001b[39m\u001b[39m\"\u001b[39m, ROC_actual \u001b[39m=\u001b[39m \u001b[39m0.813\u001b[39m, accuracy_actual \u001b[39m=\u001b[39m \u001b[39m0.832\u001b[39m, sensitivity_actual \u001b[39m=\u001b[39m \u001b[39m0.845\u001b[39m, specificity_actual \u001b[39m=\u001b[39m \u001b[39m0.782\u001b[39m, ppv_actual \u001b[39m=\u001b[39m \u001b[39m0.933\u001b[39m, npv_actual \u001b[39m=\u001b[39m \u001b[39m0.581\u001b[39m, X_test \u001b[39m=\u001b[39m X_test, y_test \u001b[39m=\u001b[39my_test)\n",
      "Cell \u001b[0;32mIn[83], line 14\u001b[0m, in \u001b[0;36mcalculate_percentiles\u001b[0;34m(iterations, model_name, ROC_actual, accuracy_actual, sensitivity_actual, specificity_actual, ppv_actual, npv_actual, X_test, y_test)\u001b[0m\n\u001b[1;32m     11\u001b[0m model_fit \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(model_name_to_load, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     12\u001b[0m y_bootstrap_predict \u001b[39m=\u001b[39m model_fit\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m---> 14\u001b[0m ROC_AUC_bootstrap_test_performance \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39mroc_auc_score(y_test, y_bootstrap_predict) \n\u001b[1;32m     15\u001b[0m accuracy_bootstrap_test_performance  \u001b[39m=\u001b[39m accuracy_score(y_test, y_bootstrap_predict)\n\u001b[1;32m     17\u001b[0m sensitivity_bootstrap_test_performance, specificity_bootstrap_test_performance, ppv_bootstrap_test_performance, npv_bootstrap_test_performance \u001b[39m=\u001b[39m calculate_metrics(confusion_matrix(y_test, y_bootstrap_predict))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "calculate_percentiles(iterations = 100, model_name = \"Naive_classifier_period_boostrap\", ROC_actual = 0.813, accuracy_actual = 0.832, sensitivity_actual = 0.845, specificity_actual = 0.782, ppv_actual = 0.933, npv_actual = 0.581, X_test = X_test, y_test =y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COVID_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a72d884e172d11118bfdfb578e108fb6b63a679c74d3d7d2f9d493a9a72737c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
