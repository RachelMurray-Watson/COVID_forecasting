{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from num2words import num2words\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, KFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score\n",
    "import word2number\n",
    "from word2number import w2n\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "hfont = {'fontname':'Helvetica'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set wd to be a folder not on github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_directory = '/Users/rem76/Documents/COVID_projections/Bootstrapping/'\n",
    "os.chdir(new_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_names(categories_for_subsetting, num_of_weeks):\n",
    "    column_names = ['HSA_ID']\n",
    "\n",
    "    for week in range(1, num_of_weeks + 1):\n",
    "        week = num2words(week)\n",
    "        for category in categories_for_subsetting:\n",
    "            column_name = f'week_{week}_{category}'\n",
    "            column_names.append(column_name)\n",
    "\n",
    "    return column_names\n",
    "\n",
    "def create_collated_weekly_data(pivoted_table, original_data, categories_for_subsetting, geography, column_names):\n",
    "    collated_data = pd.DataFrame(index=range(51), columns=column_names)\n",
    "\n",
    "    x = 0\n",
    "    for geo in original_data[geography].unique():\n",
    "        #matching_indices = [i for i, geo_col in enumerate(pivoted_table) if geo_col == geo]\n",
    "        collated_data.loc[x, geography] = geo\n",
    "        columns_to_subset = [f'{geo}_{category}' for category in categories_for_subsetting]\n",
    "        j = 1\n",
    "        try:\n",
    "            for row in range(len(pivoted_table.loc[:, columns_to_subset])):\n",
    "                collated_data.iloc[x, j:j + len(categories_for_subsetting)] = pivoted_table.loc[row, columns_to_subset]\n",
    "                j += len(categories_for_subsetting)\n",
    "        except:\n",
    "            pass\n",
    "        x += 1\n",
    "\n",
    "    return collated_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(confusion_matrix):\n",
    "    # Extract values from the confusion matrix\n",
    "    TP = confusion_matrix[1, 1]\n",
    "    FP = confusion_matrix[0, 1]\n",
    "    TN = confusion_matrix[0, 0]\n",
    "    FN = confusion_matrix[1, 0]\n",
    "\n",
    "    # Calculate Sensitivity (True Positive Rate) and Specificity (True Negative Rate)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "\n",
    "    # Calculate PPV (Precision) and NPV\n",
    "    ppv = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    npv = TN / (TN + FN) if (TN + FN) > 0 else 0.0\n",
    "\n",
    "    return sensitivity, specificity, ppv, npv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_training_test_data_period(data, no_weeks, weeks_in_future, geography, weight_col, keep_output):\n",
    "## Get the weeks for the x and y datasets   \n",
    "    x_weeks = []  \n",
    "    y_weeks = []\n",
    "    y_weeks_to_check = [] #check these weeks to see if any of them are equal to 1\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_future\n",
    "        x_weeks.append('_' + num2words(week) + '_')\n",
    "        for week_y in range(week+1, test_week+1):\n",
    "                y_weeks_to_check.append('_' + num2words(week_y) + '_')\n",
    "        y_weeks.append('_' + num2words(test_week) + '_')\n",
    "    \n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all =  pd.DataFrame()\n",
    "    missing_data = []\n",
    "    HSA_IDs_list = []\n",
    "    ## Now get the training data \n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "            y_week = y_weeks[k]\n",
    "            k +=1\n",
    "\n",
    "            weeks_x = [col for col in data.columns if x_week in col]\n",
    "            columns_x  = [geography] + weeks_x + [weight_col]\n",
    "            data_x = data[columns_x]\n",
    "\n",
    "            weeks_y = [col for col in data.columns if y_week in col]\n",
    "            columns_y  = [geography] + weeks_y\n",
    "            data_y = data[columns_y]\n",
    "            ### now add the final column to the y data that has it so that it's if any week in the trhee week perdiod exceeded 15\n",
    "            train_week = w2n.word_to_num(x_week.replace(\"_\", \"\"))\n",
    "            target_week =  w2n.word_to_num(y_week.replace(\"_\", \"\"))\n",
    "            y_weeks_to_check = []\n",
    "            for week_to_check in range(train_week + 1, target_week + 1):\n",
    "                y_weeks_to_check.append('_' + num2words(week_to_check) + '_')\n",
    "\n",
    "            y_weeks_to_check = [week + 'beds_over_15_100k' for week in y_weeks_to_check]\n",
    "            columns_to_check = [col for col in data.columns if any(week in col for week in y_weeks_to_check)]\n",
    "            y_over_in_period = data[columns_to_check].apply(max, axis=1)\n",
    "            data_y = pd.concat([data_y, y_over_in_period], axis=1)\n",
    "            # ensure they have the same amount of data\n",
    "            #remove rows in test_data1 with NA in test_data2\n",
    "            data_x = data_x.dropna()\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            # remove rows in test_data2 with NA in test_data1\n",
    "            data_y = data_y.dropna()\n",
    "            data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "            missing_data.append(((len(data[geography].unique()) - data_x_no_HSA)/len(data[geography].unique())) * 100)\n",
    "            # get weights \n",
    "            #weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "            X_week = data_x.iloc[:, 1:len(columns_x)]  # take away y, leave weights for mo\n",
    "            y_week = data_y.iloc[:, -1] \n",
    "            \n",
    "            y_week = y_week.astype(int)\n",
    "\n",
    "            weights = X_week.iloc[:, -1] \n",
    "            if keep_output:\n",
    "                X_week = X_week.iloc[:, :len(X_week.columns)-1] # remove the weights and leave \"target\" for that week\n",
    "\n",
    "                #rename columns for concatenation \n",
    "                X_week.columns = range(1, len(data_x.columns) -1)\n",
    "            else:\n",
    "                X_week = X_week.iloc[:, :len(X_week.columns)-2] # remove the weights and  \"target\" for that week\n",
    "\n",
    "                X_week.columns = range(1, len(data_x.columns) -2)# remove the weights and  \"target\" for that week\n",
    "\n",
    "            y_week.columns = range(1, len(data_y.columns) -2)\n",
    "            X_data = pd.concat([X_data, X_week])\n",
    "            y_data = pd.concat([y_data, y_week]) \n",
    "            weights_all =  pd.concat([weights_all, weights]) \n",
    "            HSA_IDs_list.append(data_x[geography].reset_index(drop=True))\n",
    "\n",
    "\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "    HSA_IDs = pd.DataFrame({'HSA_ID': pd.concat(HSA_IDs_list, ignore_index=True)})\n",
    "    HSA_IDs.reset_index(drop=True, inplace=True)\n",
    "    return(X_data, y_data, weights_all, missing_data, HSA_IDs)\n",
    "\n",
    "\n",
    "### this code it's ANY in the x week period \n",
    "def prep_training_test_data(data, no_weeks, weeks_in_future, geography, weight_col, keep_output):\n",
    "## Get the weeks for the x and y datasets   \n",
    "    x_weeks = []  \n",
    "    y_weeks = []\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_future\n",
    "        x_weeks.append('_' + num2words(week) + '_')\n",
    "        y_weeks.append('_' + num2words(test_week) + '_')\n",
    "    \n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all =  pd.DataFrame()\n",
    "    missing_data = []\n",
    "    ## Now get the training data \\\n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "            y_week = y_weeks[k]\n",
    "            k += 1\n",
    "            weeks_x = [col for col in data.columns if x_week in col]\n",
    "            columns_x  = [geography] + weeks_x + [weight_col]\n",
    "            data_x = data[columns_x]\n",
    "\n",
    "            weeks_y = [col for col in data.columns if y_week in col]\n",
    "            columns_y  = [geography] + weeks_y\n",
    "            data_y = data[columns_y]\n",
    "            # ensure they have the same amount of data\n",
    "            #remove rows in test_data1 with NA in test_data2\n",
    "            data_x = data_x.dropna()\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            # remove rows in test_data2 with NA in test_data1\n",
    "            data_y = data_y.dropna()\n",
    "            data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "            missing_data.append(((len(data[geography].unique()) - data_x_no_HSA)/len(data[geography].unique())) * 100)\n",
    "            # get weights \n",
    "            #weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "            X_week = data_x.iloc[:, 1:len(columns_x)]  # take away y, leave weights for mo\n",
    "            y_week = data_y.iloc[:, -1] \n",
    "            \n",
    "            y_week = y_week.astype(int)\n",
    "\n",
    "            weights = X_week.iloc[:, -1] \n",
    "            if keep_output:\n",
    "                X_week = X_week.iloc[:, :len(X_week.columns)-1] # remove the weights and leave \"target\" for that week\n",
    "\n",
    "                #rename columns for concatenation \n",
    "                X_week.columns = range(1, len(data_x.columns) -1)\n",
    "            else:\n",
    "                X_week = X_week.iloc[:, :len(X_week.columns)-2] # remove the weights and  \"target\" for that week\n",
    "\n",
    "                X_week.columns = range(1, len(data_x.columns) -2)# remove the weights and  \"target\" for that week\n",
    "\n",
    "                #rename columns for concatenation \n",
    "            y_week.columns = range(1, len(data_y.columns) -1)\n",
    "            X_data = pd.concat([X_data, X_week])\n",
    "            y_data = pd.concat([y_data, y_week]) \n",
    "        \n",
    "            weights_all =  pd.concat([weights_all, weights]) \n",
    "\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return(X_data, y_data, weights_all, missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs cross-validation using a leave-geography-out approach with bootstrap sampling.\n",
    "# It trains and evaluates a classifier on subsets of the data for multiple iterations.\n",
    "\n",
    "def cross_validation_leave_geo_out_bootstrap(data, geography_column, geo_split, no_iterations, cv, classifier, param_grid, no_iterations_param, no_weeks_train, no_weeks_test, weeks_in_future, weight_col, keep_output, time_period):\n",
    "    \n",
    "    # Lists to store best hyperparameters and area under ROC curve (auROC) scores for each iteration\n",
    "    best_hyperparameters_per_iter = []\n",
    "    auROC_per_iter = []\n",
    "\n",
    "    # Loop through the specified number of iterations\n",
    "    for i in range(no_iterations):\n",
    "        \n",
    "        # Subset the HSAs (Health Service Areas) from the full dataset based on geography\n",
    "        geo_names = data[geography_column].unique()\n",
    "        num_names_to_select = int(geo_split * len(geo_names))\n",
    "        geos_for_sample = random.sample(list(geo_names), num_names_to_select)\n",
    "        subset_HSAs_for_train = data[data[geography_column].isin(geos_for_sample)]\n",
    "        subset_HSAs_for_test = data[~data[geography_column].isin(geos_for_sample)]\n",
    "\n",
    "        # Resample and prepare training data\n",
    "        subset_HSAs_for_train_data_resampled = subset_HSAs_for_train.sample(frac=1, replace=True)\n",
    "        subset_HSAs_for_train_data_resampled_copy = subset_HSAs_for_train_data_resampled.copy()\n",
    "        weights_train = subset_HSAs_for_train_data_resampled_copy.loc[:, 'weight']\n",
    "        y_sample_train = subset_HSAs_for_train_data_resampled_copy.loc[:, 'y']  # Assuming the last column is the target column\n",
    "        X_sample_train = subset_HSAs_for_train_data_resampled_copy.drop(subset_HSAs_for_train_data_resampled_copy.columns[-3:], axis=1) #final three are HSA, y, and weight\n",
    "\n",
    "        # Resample and prepare test data\n",
    "        subset_HSAs_for_test_data_resampled = subset_HSAs_for_test.sample(frac=1, replace=True)\n",
    "        subset_HSAs_for_test_data_resampled_copy = subset_HSAs_for_test_data_resampled.copy()\n",
    "        weights_test = subset_HSAs_for_test_data_resampled_copy.loc[:, 'weight']\n",
    "\n",
    "        y_sample_test = subset_HSAs_for_test_data_resampled_copy.loc[:, 'y']  # Assuming the last column is the target column\n",
    "        X_sample_test = subset_HSAs_for_test_data_resampled_copy.drop(subset_HSAs_for_test_data_resampled_copy.columns[-3:], axis=1) # final three are HSA, y, and weight\n",
    "\n",
    "        # Perform random search for hyperparameter tuning\n",
    "        random_search = RandomizedSearchCV(classifier, param_grid, n_iter=no_iterations_param, cv=cv, random_state=10)\n",
    "        random_search.fit(X_sample_train, y_sample_train, sample_weight=weights_train)\n",
    "        best_params = random_search.best_params_\n",
    "\n",
    "        # Create the Decision Tree classifier with the best hyperparameters\n",
    "        model = DecisionTreeClassifier(**best_params, random_state=10, class_weight='balanced')\n",
    "        model_fit = model.fit(X_sample_train, y_sample_train, sample_weight=weights_train)\n",
    "        y_pred = model_fit.predict_proba(X_sample_test)\n",
    "\n",
    "        # Evaluate the performance of the model and store results\n",
    "        best_hyperparameters_per_iter.append(best_params)\n",
    "        auROC_per_iter.append(roc_auc_score(y_sample_test, y_pred[:, 1]))\n",
    "        print(roc_auc_score(y_sample_test, y_pred[:, 1]))\n",
    "    # Return the best hyperparameters based on the iteration with the highest auROC score\n",
    "    return best_hyperparameters_per_iter[np.argmax(np.array(auROC_per_iter))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentiles(iterations, model_name, ROC_actual, accuracy_actual, sensitivity_actual, specificity_actual, ppv_actual, npv_actual, X_test ,y_test):\n",
    "        bootstrapped_stats_ROC = []\n",
    "        bootstrapped_stats_accuracy = []\n",
    "        bootstrapped_stats_sesitivity = []\n",
    "        bootstrapped_stats_specificity = []\n",
    "        bootstrapped_stats_ppv = []\n",
    "        bootstrapped_stats_npv = []\n",
    "\n",
    "        for j in iterations:\n",
    "            model_name_to_load = model_name + \"_\" + str(j)+ \".sav\" \n",
    "            model_fit = pickle.load(open(model_name_to_load, 'rb'))\n",
    "            y_bootstrap_predict = model_fit.predict(X_test)\n",
    "            y_bootstrap_predict_proba = model_fit.predict_proba(X_test)\n",
    "\n",
    "            ROC_AUC_bootstrap_test_performance = metrics.roc_auc_score(y_test, y_bootstrap_predict_proba[:,1]) \n",
    "            accuracy_bootstrap_test_performance  = accuracy_score(y_test, y_bootstrap_predict)\n",
    "\n",
    "            sensitivity_bootstrap_test_performance, specificity_bootstrap_test_performance, ppv_bootstrap_test_performance, npv_bootstrap_test_performance = calculate_metrics(confusion_matrix(y_test, y_bootstrap_predict))\n",
    "        ### (D) Calculate estimate fo variance  by getting (B) - (D) \n",
    "\n",
    "            bootstrapped_stats_ROC.append({'Difference': ROC_AUC_bootstrap_test_performance - ROC_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            bootstrapped_stats_accuracy.append({'Difference': accuracy_bootstrap_test_performance - accuracy_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            bootstrapped_stats_sesitivity.append({'Difference': sensitivity_bootstrap_test_performance - sensitivity_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            bootstrapped_stats_specificity.append({'Difference': specificity_bootstrap_test_performance - specificity_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            bootstrapped_stats_ppv.append({'Difference': ppv_bootstrap_test_performance - ppv_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            bootstrapped_stats_npv.append({'Difference': npv_bootstrap_test_performance - npv_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "\n",
    "\n",
    "        bootstrapped_stats_ROC = pd.DataFrame(bootstrapped_stats_ROC)\n",
    "        bootstrapped_stats_accuracy = pd.DataFrame(bootstrapped_stats_accuracy)\n",
    "        bootstrapped_stats_sesitivity = pd.DataFrame(bootstrapped_stats_sesitivity)\n",
    "        bootstrapped_stats_specificity = pd.DataFrame(bootstrapped_stats_specificity)\n",
    "        bootstrapped_stats_ppv = pd.DataFrame(bootstrapped_stats_ppv)\n",
    "        bootstrapped_stats_npv = pd.DataFrame(bootstrapped_stats_npv)\n",
    "\n",
    "    ## Step 3: Get percentile\n",
    "        alpha = 0.05\n",
    "\n",
    "        upper_quartile_ROC, lower_quartile_ROC = ROC_actual - np.percentile(bootstrapped_stats_ROC[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        upper_quartile_accuracy, lower_quartile_accuracy = accuracy_actual - np.percentile(bootstrapped_stats_accuracy[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        upper_quartile_sensitivity, lower_quartile_sensitivity = sensitivity_actual - np.percentile(bootstrapped_stats_sesitivity[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        upper_quartile_specificity, lower_quartile_specificity = specificity_actual - np.percentile(bootstrapped_stats_specificity[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        upper_quartile_ppv, lower_quartile_ppv = ppv_actual - np.percentile(bootstrapped_stats_ppv[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        upper_quartile_npv, lower_quartile_npv = npv_actual - np.percentile(bootstrapped_stats_npv[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        ## Step 4: Get optimization-corrected performance\n",
    "\n",
    "        return upper_quartile_ROC, lower_quartile_ROC, upper_quartile_accuracy, lower_quartile_accuracy, upper_quartile_sensitivity, lower_quartile_sensitivity, upper_quartile_specificity, lower_quartile_specificity, upper_quartile_ppv, lower_quartile_ppv, upper_quartile_npv, lower_quartile_npv\n",
    "\n",
    "### now try bootstrapping w/o feature selection\n",
    "iterations = 100\n",
    "## DO NOT SAMPLE THE TARGET DATA\n",
    "def bootstrap_no_dev(iterations, training_data, clf, param_grid, cv, geo_split, iterations_param_search, model_name, time_period, keep_output, weeks_in_future, geography, weight_col, no_iterations_CV, no_weeks_train, no_weeks_test):\n",
    "      #1. Get dataset\n",
    "    for j in iterations:\n",
    "        # sample \n",
    "\n",
    "          training_data_resampled = training_data.sample(frac = 1, replace=True)\n",
    "\n",
    "          training_data_resampled_copy = training_data_resampled.copy()\n",
    "          weights_train = training_data_resampled_copy.loc[:, 'weight']\n",
    "          y_sample_train = training_data_resampled_copy.loc[:, 'y']\n",
    "\n",
    "          X_sample_train = training_data_resampled_copy.drop(['HSA_ID', 'weight', 'y'],axis=1) \n",
    "\n",
    "          best_params = cross_validation_leave_geo_out_bootstrap(training_data_resampled, geography_column=geography, geo_split=geo_split, no_iterations=no_iterations_CV, cv=cv, classifier=clf, param_grid=param_grid, no_iterations_param=iterations_param_search, no_weeks_train=no_weeks_train, no_weeks_test=no_weeks_test, weeks_in_future=weeks_in_future, weight_col=weight_col, keep_output=keep_output, time_period=time_period)\n",
    "\n",
    "\n",
    "\n",
    "# Create the Decision Tree classifier with the best hyperparameters\n",
    "          model = DecisionTreeClassifier(**best_params,random_state=10, class_weight='balanced')\n",
    "          model_fit = model.fit(X_sample_train, y_sample_train, sample_weight=weights_train)\n",
    "          path = model_fit.cost_complexity_pruning_path(X_sample_train, y_sample_train)\n",
    "          ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "          clfs = []\n",
    "          for ccp_alpha in ccp_alphas:\n",
    "              clf = DecisionTreeClassifier(**best_params,random_state=10, ccp_alpha=ccp_alpha,  class_weight='balanced')\n",
    "              clf.fit(X_sample_train, y_sample_train,  sample_weight = weights_train )\n",
    "              clfs.append(clf)\n",
    "\n",
    "          clfs = clfs[:-1]\n",
    "          ccp_alphas = ccp_alphas[:-1]\n",
    "          train_scores = [roc_auc_score(y_sample_train, clf.predict_proba(X_sample_train)[:, 1]) for clf in clfs]\n",
    "          model_pruned = clfs[train_scores.index(max(train_scores))]\n",
    "\n",
    "          model_fit = model_pruned.fit(X_sample_train, y_sample_train,  sample_weight = weights_train )\n",
    "          model_name_to_save = model_name + \"_\" + str(j)+ \".sav\" \n",
    "          X_data_name = model_name + \"_X_data_\" + str(j) + \".csv\" \n",
    "          y_data_name = model_name + \"_y_data_\" + str(j) + \".csv\" \n",
    "          weights_data_name = model_name + \"_weights_\" + str(j) + \".csv\" \n",
    "          \n",
    "          weights_train.to_csv(weights_data_name, index=False)\n",
    "          X_sample_train.to_csv(X_data_name,index=False)\n",
    "          y_sample_train.to_csv(y_data_name, index=False)\n",
    "          pickle.dump(model_fit, open(model_name_to_save, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_rename_data(data1, data2, on_column, suffix1, suffix2):\n",
    "    merged_data = pd.merge(data1, data2, on=on_column, suffixes=('_'+suffix1, '_'+suffix2))\n",
    "\n",
    "    new_column_names = [col.replace(f'_{on_column}_{suffix1}', f'_{suffix1}').replace(f'_{on_column}_{suffix2}', f'_{suffix2}') for col in merged_data.columns]\n",
    "    merged_data.rename(columns=dict(zip(merged_data.columns, new_column_names)), inplace=True)\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "def pivot_data_by_HSA(data, index_column, columns_column, values_column):\n",
    "    data_by_HSA = data[[index_column, columns_column, values_column]]\n",
    "    pivot_table = data_by_HSA.pivot_table(index=index_column, columns=columns_column, values=values_column)\n",
    "    return pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_changes_by_week(weekly_data_frame, outcome_column):\n",
    "\n",
    "    for column in weekly_data_frame.columns[1:]:\n",
    "        # Calculate the difference between each row and the previous row\n",
    "        if outcome_column not in column.lower(): #want to leave out the outcome column\n",
    "            diff = weekly_data_frame[column].diff()\n",
    "            \n",
    "            # Create a new column with the original column name and \"delta\"\n",
    "            new_column_name = column + \"_delta\"\n",
    "            \n",
    "            column_index = weekly_data_frame.columns.get_loc(column)\n",
    "            \n",
    "            # Insert the new column just after the original column\n",
    "            weekly_data_frame.insert(column_index + 1, new_column_name, diff)\n",
    "            weekly_data_frame[new_column_name] = diff\n",
    "    return weekly_data_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive classifier bootstrapping \n",
    "- Cannot bootstrap - single, binary predictor "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Needed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['gini'],#,  'entropy'],\n",
    "    'max_depth': np.arange(1, 10),\n",
    "    'min_samples_split':  np.arange(2, 300), #[100, 200, 300, 400, 500], #np.arange(50, 200),\n",
    "    'min_samples_leaf':  np.arange(2, 400)}#, #100, 200, 300, 400, 500], #np.arange(500, 200)\n",
    "    #'ccp_alpha': np.arange(0.0001, 0.0035, 0.0001) }\n",
    "\n",
    "\n",
    "# Create the Decision Tree classifier\n",
    "cv = RepeatedStratifiedKFold(n_splits=10,  n_repeats=10,random_state=1) ## 10-fold cross validations\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try CDC Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_1472/1805026620.py:1: DtypeWarning: Columns (41,43,44,45,46,50,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_by_HSA = pd.read_csv('/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates.csv')\n"
     ]
    }
   ],
   "source": [
    "data_by_HSA = pd.read_csv('/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates.csv')\n",
    "data_by_HSA['health_service_area_number']\n",
    "data_by_HSA['health_service_area']\n",
    "#data_by_HSA['HSA_ID'] = data_by_HSA['health_service_area_number'].astype(str) + '' + data_by_HSA['health_service_area'].apply(lambda x: x.split()[0])\n",
    "data_by_HSA.rename(columns={'health_service_area_number': 'HSA_ID'}, inplace=True)\n",
    "\n",
    "data_by_HSA['beds_over_15_100k'] = (data_by_HSA['beds_weekly'] > 15)*1\n",
    "\n",
    "# remove HSAs that have missing data in specific columns\n",
    "\n",
    "data_by_HSA = data_by_HSA.dropna(subset=['admits_weekly', 'deaths_weekly', 'cases_weekly', 'icu_weekly', 'beds_weekly', 'perc_covid'])\n",
    "\n",
    "for i, week in enumerate(data_by_HSA['date'].unique()):\n",
    "    data_by_HSA.loc[data_by_HSA['date'] == week, 'week'] = i\n",
    "\n",
    "    ## pivot \n",
    "data_by_HSA_cases = pivot_data_by_HSA(data_by_HSA, 'week', 'HSA_ID', 'cases_weekly')\n",
    "data_by_HSA_admissions = pivot_data_by_HSA(data_by_HSA, 'week', 'HSA_ID', 'admits_weekly')\n",
    "data_by_HSA_percent_beds = pivot_data_by_HSA(data_by_HSA, 'week', 'HSA_ID', 'perc_covid')\n",
    "data_by_HSA_over_15_100k = pivot_data_by_HSA(data_by_HSA, 'week', 'HSA_ID', 'beds_over_15_100k')\n",
    "\n",
    "## merge \n",
    "data_by_HSA_cases_admits = merge_and_rename_data(data_by_HSA_cases, data_by_HSA_admissions,'week','cases', 'admits')\n",
    "data_by_HSA_admits_perc_outcome = merge_and_rename_data(data_by_HSA_percent_beds, data_by_HSA_over_15_100k,'week','perc_covid', 'beds_over_15_100k')\n",
    "data_by_HSA_cases_admits_perc_outcome= pd.merge(data_by_HSA_cases_admits, data_by_HSA_admits_perc_outcome, on='week')\n",
    "\n",
    "\n",
    "data_by_HSA_cases_admits_perc_outcome = data_by_HSA_cases_admits_perc_outcome.reset_index()\n",
    "data_by_HSA_cases_admits_perc_outcome.columns = data_by_HSA_cases_admits_perc_outcome.columns.str.replace(',', '')\n",
    "\n",
    "categories_for_subsetting = ['cases', 'admits','perc_covid', 'beds_over_15_100k']\n",
    "num_of_weeks = len(data_by_HSA_cases_admits_perc_outcome)\n",
    "column_names = create_column_names(categories_for_subsetting, num_of_weeks)\n",
    "\n",
    "all_HSA_ID_weekly_data = create_collated_weekly_data(data_by_HSA_cases_admits_perc_outcome, data_by_HSA, categories_for_subsetting, 'HSA_ID', column_names)\n",
    "\n",
    "weights_df = data_by_HSA[data_by_HSA['HSA_ID'].isin(all_HSA_ID_weekly_data['HSA_ID'])][['HSA_ID','weight_alt']]\n",
    "weights_df = weights_df.rename(columns = {'HSA_ID': 'HSA_ID', 'weight_alt':'weight'})\n",
    "weights_df = weights_df.drop_duplicates()\n",
    "weights_df['weight'].unique()\n",
    "all_HSA_ID_weekly_data = all_HSA_ID_weekly_data.join(weights_df['weight'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimized classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, weights, missing_data_train_HSA = prep_training_test_data(all_HSA_ID_weekly_data,  no_weeks = range(1, int(123*2/3) + 1), weeks_in_future = 3,   geography = 'HSA_ID', weight_col = 'weight', keep_output = False)\n",
    "\n",
    "X_test, y_test, weights_test, missing_data_test_HSA = prep_training_test_data(all_HSA_ID_weekly_data,  no_weeks = range(int(123*2/3) + 1, 120), weeks_in_future = 3,   geography = 'HSA_ID',  weight_col = 'weight', keep_output = False)\n",
    "weights = weights[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDC_exact = pickle.load(open(\"/Users/rem76/Documents/COVID_projections/COVID_forecasting/CDC_optimized_exact_auroc_0.8269_pruned.sav\", 'rb'))\n",
    "bootstrap_no_dev(iterations =range(0,100), clf = CDC_exact,  param_grid = param_grid,  cv = cv , iterations_param_search = 10,data = all_HSA_ID_weekly_data, model_name = \"Optimized_CDC_classifier_exact_boostrap\", time_period = 'exact', no_weeks = range(1, int(123*2/3) + 1),keep_output = False, weeks_in_future = 3,   geography = 'HSA_ID', weight_col = 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8237764055669351,\n",
       " 0.8713994771420005,\n",
       " 0.7432406617945595,\n",
       " 0.7911488699418054,\n",
       " 0.7204712637938783,\n",
       " 0.8187186298497029,\n",
       " 0.72609098541295,\n",
       " 0.8105410851086158,\n",
       " 0.8610524852530571,\n",
       " 0.8916781952553877,\n",
       " 0.5711349068555243,\n",
       " 0.6465129179251549)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_percentiles(iterations = range(0,100), model_name = \"Optimized_CDC_classifier_exact_boostrap\", ROC_actual = 0.8269, accuracy_actual = 0.758, sensitivity_actual = 0.757, specificity_actual = 0.762, ppv_actual = 0.870, npv_actual = 0.599, X_test = X_test, y_test =y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced CDC optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDC_exact_enhanced = pickle.load(open(\"/Users/rem76/Documents/COVID_projections/COVID_forecasting/CDC_optimized_exact_enhanced_auroc_0.8280_pruned.sav\", 'rb'))\n",
    "X_train, y_train, weights, missing_data_train_HSA = prep_training_test_data(all_HSA_ID_weekly_data,  no_weeks = range(1, int(123*2/3) + 1), weeks_in_future = 3,   geography = 'HSA_ID', weight_col = 'weight', keep_output = True)\n",
    "\n",
    "X_test, y_test, weights_test, missing_data_test_HSA = prep_training_test_data(all_HSA_ID_weekly_data,  no_weeks = range(int(123*2/3) + 1, 120), weeks_in_future = 3,   geography = 'HSA_ID',  weight_col = 'weight', keep_output = True)\n",
    "weights = weights[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_no_dev(iterations =range(0,100), clf = CDC_exact_enhanced,  param_grid = param_grid,  cv = cv , iterations_param_search = 10,data = all_HSA_ID_weekly_data, model_name = \"Optimized_CDC_classifier_enhanced_exact_boostrap\", time_period = 'exact', no_weeks = range(1, int(123*2/3) + 1),keep_output = True, weeks_in_future = 3,   geography = 'HSA_ID', weight_col = 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8227005411620747,\n",
       " 0.8733634891878018,\n",
       " 0.7850713493030181,\n",
       " 0.8310738530247667,\n",
       " 0.8733780396464772,\n",
       " 0.9924061017626205,\n",
       " 0.5072775737223214,\n",
       " 0.6309789064959597,\n",
       " 0.7974115047131706,\n",
       " 0.8334923780487804,\n",
       " 0.7018748248482017,\n",
       " 0.8160039231066827)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_percentiles(iterations = range(0,100), model_name = \"Optimized_CDC_classifier_enhanced_exact_boostrap\", ROC_actual = 0.8280, accuracy_actual = 0.7899, sensitivity_actual =  0.873, specificity_actual = 0.631, ppv_actual = 0.833, npv_actual = 0.702, X_test = X_test, y_test =y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_HSA_ID_weekly_data = pd.read_csv('/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates_weekly.csv')\n",
    "all_HSA_ID_weekly_data.drop('Unnamed: 0', axis =1, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_period = pickle.load(open(\"/Users/rem76/Documents/COVID_projections/COVID_forecasting/Full_auroc_0.9080_period_pruned.sav\", 'rb'))\n",
    "\n",
    "X_train, y_train, weights, missing_data_train_HSA, HSA_ID_train = prep_training_test_data_period(all_HSA_ID_weekly_data,   no_weeks = range(1, int(123*2/3) + 1), weeks_in_future = 3,   geography = 'HSA_ID', weight_col = 'weight', keep_output = True)\n",
    "\n",
    "X_test, y_test, weights_test, missing_data_test_HSA, HSA_ID = prep_training_test_data_period(all_HSA_ID_weekly_data,   no_weeks = range(int(123*2/3) + 1, 120), weeks_in_future = 3,   geography = 'HSA_ID',  weight_col = 'weight', keep_output = True)\n",
    "#weights = weights[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512.0</td>\n",
       "      <td>9.047625</td>\n",
       "      <td>32.312948</td>\n",
       "      <td>5.652411</td>\n",
       "      <td>199.812717</td>\n",
       "      <td>0.174962</td>\n",
       "      <td>-294.0</td>\n",
       "      <td>-0.230807</td>\n",
       "      <td>7.253927</td>\n",
       "      <td>0.376827</td>\n",
       "      <td>-51.436937</td>\n",
       "      <td>-0.046292</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>485.0</td>\n",
       "      <td>4.443280</td>\n",
       "      <td>27.332835</td>\n",
       "      <td>3.356098</td>\n",
       "      <td>267.229286</td>\n",
       "      <td>0.225213</td>\n",
       "      <td>-444.0</td>\n",
       "      <td>0.996180</td>\n",
       "      <td>-13.327581</td>\n",
       "      <td>-2.033021</td>\n",
       "      <td>-93.293064</td>\n",
       "      <td>-0.091366</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316.0</td>\n",
       "      <td>7.416448</td>\n",
       "      <td>26.571134</td>\n",
       "      <td>3.771927</td>\n",
       "      <td>189.937503</td>\n",
       "      <td>0.134545</td>\n",
       "      <td>-287.0</td>\n",
       "      <td>0.222963</td>\n",
       "      <td>-6.538008</td>\n",
       "      <td>-1.903925</td>\n",
       "      <td>-81.641275</td>\n",
       "      <td>-0.057976</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>197.0</td>\n",
       "      <td>4.758360</td>\n",
       "      <td>16.994142</td>\n",
       "      <td>2.446220</td>\n",
       "      <td>128.706901</td>\n",
       "      <td>0.079590</td>\n",
       "      <td>-154.0</td>\n",
       "      <td>-0.271734</td>\n",
       "      <td>-7.116836</td>\n",
       "      <td>-1.324779</td>\n",
       "      <td>-73.583771</td>\n",
       "      <td>-0.045424</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>359.0</td>\n",
       "      <td>2.237073</td>\n",
       "      <td>9.211479</td>\n",
       "      <td>6.687050</td>\n",
       "      <td>166.934552</td>\n",
       "      <td>0.128750</td>\n",
       "      <td>-214.0</td>\n",
       "      <td>1.171174</td>\n",
       "      <td>-6.391638</td>\n",
       "      <td>-0.322267</td>\n",
       "      <td>-44.929457</td>\n",
       "      <td>-0.034437</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29551</th>\n",
       "      <td>185.0</td>\n",
       "      <td>0.660007</td>\n",
       "      <td>6.203074</td>\n",
       "      <td>0.531692</td>\n",
       "      <td>41.188410</td>\n",
       "      <td>0.063748</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.233107</td>\n",
       "      <td>0.496246</td>\n",
       "      <td>30.271000</td>\n",
       "      <td>0.046851</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29552</th>\n",
       "      <td>167.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.818053</td>\n",
       "      <td>0.297461</td>\n",
       "      <td>21.699011</td>\n",
       "      <td>0.012121</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.594922</td>\n",
       "      <td>-0.152085</td>\n",
       "      <td>-3.381664</td>\n",
       "      <td>-0.002004</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29553</th>\n",
       "      <td>163.0</td>\n",
       "      <td>0.360881</td>\n",
       "      <td>17.184788</td>\n",
       "      <td>3.191461</td>\n",
       "      <td>58.428279</td>\n",
       "      <td>0.034943</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>-0.120294</td>\n",
       "      <td>3.436958</td>\n",
       "      <td>-1.472982</td>\n",
       "      <td>-17.184788</td>\n",
       "      <td>-0.010277</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29554</th>\n",
       "      <td>167.0</td>\n",
       "      <td>0.604786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29555</th>\n",
       "      <td>44.0</td>\n",
       "      <td>3.047896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>3.047896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29556 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2          3         4           5         6      7   \\\n",
       "0      512.0  9.047625  32.312948  5.652411  199.812717  0.174962 -294.0   \n",
       "1      485.0  4.443280  27.332835  3.356098  267.229286  0.225213 -444.0   \n",
       "2      316.0  7.416448  26.571134  3.771927  189.937503  0.134545 -287.0   \n",
       "3      197.0  4.758360  16.994142  2.446220  128.706901  0.079590 -154.0   \n",
       "4      359.0  2.237073   9.211479  6.687050  166.934552  0.128750 -214.0   \n",
       "...      ...       ...        ...       ...         ...       ...    ...   \n",
       "29551  185.0  0.660007   6.203074  0.531692   41.188410  0.063748  141.0   \n",
       "29552  167.0  0.000000   2.818053  0.297461   21.699011  0.012121   24.0   \n",
       "29553  163.0  0.360881  17.184788  3.191461   58.428279  0.034943  -16.0   \n",
       "29554  167.0  0.604786   0.000000  0.000000    0.000000  0.000000   66.0   \n",
       "29555   44.0  3.047896   0.000000  0.000000    0.000000  0.000000  -58.0   \n",
       "\n",
       "             8          9         10         11        12   13  \n",
       "0     -0.230807   7.253927  0.376827 -51.436937 -0.046292  1.0  \n",
       "1      0.996180 -13.327581 -2.033021 -93.293064 -0.091366  1.0  \n",
       "2      0.222963  -6.538008 -1.903925 -81.641275 -0.057976  1.0  \n",
       "3     -0.271734  -7.116836 -1.324779 -73.583771 -0.045424  1.0  \n",
       "4      1.171174  -6.391638 -0.322267 -44.929457 -0.034437  1.0  \n",
       "...         ...        ...       ...        ...       ...  ...  \n",
       "29551  0.000000   2.233107  0.496246  30.271000  0.046851  1.0  \n",
       "29552  0.000000   0.594922 -0.152085  -3.381664 -0.002004  1.0  \n",
       "29553 -0.120294   3.436958 -1.472982 -17.184788 -0.010277  1.0  \n",
       "29554  0.000000   0.000000  0.000000   0.000000  0.000000  0.0  \n",
       "29555  3.047896   0.000000  0.000000   0.000000  0.000000  0.0  \n",
       "\n",
       "[29556 rows x 13 columns]"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.merge(X_train, y_train, left_index=True, right_index=True)\n",
    "training_data.rename(columns={0: 'y'}, inplace=True)\n",
    "training_data = pd.merge(training_data, weights, left_index=True, right_index=True)\n",
    "training_data.rename(columns={0: 'weight'}, inplace=True)\n",
    "training_data = pd.merge(training_data, HSA_ID_train, left_index=True, right_index=True)\n",
    "training_data.rename(columns={0: 'HSA_ID'}, inplace=True)\n",
    "weights = weights[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_iterations_CV = 10\n",
    "geography_column = 'HSA_ID'  \n",
    "geo_split = 0.9  \n",
    "time_period = 'period'  # Choose 'period', 'exact', or 'shifted'\n",
    "no_weeks_train =  range(1, int(123*2/3) + 1)  # across entire training period\n",
    "no_weeks_test = range(int(123*2/3) + 1, 120) # across entire training period\n",
    "weeks_in_future = 3 \n",
    "weight_col = 'weight'  \n",
    "keep_output = True  \n",
    "no_iterations_param = 10  # Replace with the number of iterations for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': np.arange(2, 5, 1),\n",
    "    'min_samples_split': np.arange(200, 2000, 50), #[100, 200, 300, 400, 500], #np.arange(50, 200),\n",
    "    'min_samples_leaf':  np.arange(200, 2000, 50)} #100, 200, 300, 400, 500], #np.arange(500, 200)\n",
    "    #'ccp_alpha': np.arange(0.0001, 0.0035, 0.0001) }\n",
    "clf = DecisionTreeClassifier(random_state=10, class_weight='balanced')\n",
    "cv = RepeatedStratifiedKFold(n_splits=10,  n_repeats=10,random_state=1)  ## 10-fold cross validations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9121318423522052\n",
      "0.9183639280841102\n",
      "0.952154658904642\n",
      "0.9213625284515935\n",
      "0.9171756136168485\n",
      "0.9203018587237357\n",
      "0.9373555735164932\n",
      "0.9357281991539474\n",
      "0.9349485770220741\n",
      "0.9382653881193959\n",
      "0.9075730760187882\n",
      "0.9049364489076455\n",
      "0.9312833867521367\n",
      "0.9183379137949543\n",
      "0.9174085936384493\n",
      "0.9187583065092302\n",
      "0.9276132509689923\n",
      "0.9398175075782055\n",
      "0.93511723355651\n",
      "0.9104426227274294\n"
     ]
    }
   ],
   "source": [
    "bootstrap_no_dev(training_data = training_data,iterations =range(0,2), clf = clf,  geography = geography_column, no_iterations_CV = no_iterations_CV, param_grid = param_grid,  cv = cv , iterations_param_search = no_iterations_param, model_name = \"Full_classifier_period_boostrap\", geo_split=geo_split,  no_weeks_train=no_weeks_train, no_weeks_test=no_weeks_test, weeks_in_future=weeks_in_future, weight_col=weight_col, keep_output=keep_output, time_period=time_period )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9158042117136911,\n",
       " 0.9323113848356361,\n",
       " 0.8798075179320611,\n",
       " 0.9803167208011909,\n",
       " 0.9055975357610025,\n",
       " 1.0509496603232205,\n",
       " 0.6968869728529964,\n",
       " 0.7778243127881167,\n",
       " 0.919324907245478,\n",
       " 0.940667220242741,\n",
       " 0.6365379962393015,\n",
       " 0.7441305355804008)"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_percentiles(iterations = range(0,2), model_name = \"Full_classifier_period_boostrap\", ROC_actual = 0.909, accuracy_actual = 0.843, sensitivity_actual = 0.847, specificity_actual = 0.828, ppv_actual = 0.952, npv_actual = 0.572, X_test = X_test, y_test =y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_exact = pickle.load(open(\"/Users/rem76/Documents/COVID_projections/COVID_forecasting/Full_auroc_0.8594_exact_pruned.sav\", 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7379752859927271\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7380002522882282\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7541161676463399\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7372190867965774\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7379113860052178\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7346276382482348\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.736286444090697\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7381102727185651\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7374137395364195\n",
      "0.7294756523021763\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7557334774044127\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7514570226825121\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7372190867965774\n",
      "0.7553864880794807\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7374035842880869\n",
      "0.7518215269287816\n",
      "0.7543814852991801\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n",
      "0.7518215269287816\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test, y_test, weights_test, missing_data_test_HSA = prep_training_test_data(all_HSA_ID_weekly_data,  no_weeks = range(int(123*2/3) + 1, 120), weeks_in_future = 3,   geography = 'HSA_ID',  weight_col = 'weight', keep_output = True)\n",
    "\n",
    "bootstrap_no_dev(iterations =range(0,100), clf = Full_exact,  param_grid = param_grid,  cv = cv , iterations_param_search = 10,data = all_HSA_ID_weekly_data, model_name = \"Full_classifier_exact_boostrap\", time_period = 'exact', no_weeks = range(1, int(123*2/3) + 1),keep_output = True, weeks_in_future = 3,   geography = 'HSA_ID', weight_col = 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_percentiles(iterations = range(0,100), model_name = \"Full_classifier_exact_boostrap\", ROC_actual = 0.783, accuracy_actual = 0.762, sensitivity_actual = 0.722, specificity_actual = 0.846, ppv_actual = .908, npv_actual = 0.591, X_test = X_test, y_test =y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_shifted = pickle.load(open(\"/Users/rem76/Documents/COVID_projections/COVID_forecasting/Full_auroc_0.7268_shifted_pruned.sav\", 'rb'))\n",
    "bootstrap_no_dev(iterations =range(0,100), clf = Full_shifted,  param_grid = param_grid,  cv = cv , iterations_param_search = 10,data = all_HSA_ID_weekly_data, model_name = \"Full_classifier_shifted_boostrap\", time_period = 'shifted', no_weeks = range(1, int(123*2/3) + 1),keep_output = True, weeks_in_future = 3,   geography = 'HSA_ID', weight_col = 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, weights_test, missing_data_test_HSA = prep_training_test_data_period(all_HSA_ID_weekly_data,  no_weeks = range(int(123*2/3) + 1, 120), weeks_in_future = 3,   geography = 'HSA_ID',  weight_col = 'weight', keep_output = True)\n",
    "\n",
    "y_test = y_test.shift(-1)\n",
    "\n",
    "y_test.drop(index=y_test.index[-1], inplace=True)\n",
    "X_test.drop(index=X_test.index[-1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6921497173691434,\n",
       " 0.7336733737276049,\n",
       " 0.5184196565209997,\n",
       " 0.6347663662642407,\n",
       " 0.43228632007463325,\n",
       " 0.5958390297684675,\n",
       " 0.7883209685729005,\n",
       " 0.8971136871028679,\n",
       " 0.914165804249595,\n",
       " 0.9415585029396056,\n",
       " 0.26846141179683686,\n",
       " 0.32056097066054867)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_percentiles(iterations = range(0,100), model_name = \"Full_classifier_shifted_boostrap\", ROC_actual = 0.7268, accuracy_actual = 0.573, sensitivity_actual = 0.509, specificity_actual = 0.833, ppv_actual = 0.925, npv_actual = 0.295, X_test = X_test, y_test =y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COVID_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a72d884e172d11118bfdfb578e108fb6b63a679c74d3d7d2f9d493a9a72737c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
