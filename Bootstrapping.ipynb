{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from num2words import num2words\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, KFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score\n",
    "import word2number\n",
    "from word2number import w2n\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "hfont = {'fontname':'Helvetica'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set wd to be a folder not on github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_directory = '/Users/rem76/Documents/COVID_projections/Bootstrapping/'\n",
    "os.chdir(new_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_names(categories_for_subsetting, num_of_weeks):\n",
    "    column_names = ['HSA_ID']\n",
    "\n",
    "    for week in range(1, num_of_weeks + 1):\n",
    "        week = num2words(week)\n",
    "        for category in categories_for_subsetting:\n",
    "            column_name = f'week_{week}_{category}'\n",
    "            column_names.append(column_name)\n",
    "\n",
    "    return column_names\n",
    "\n",
    "def create_collated_weekly_data(pivoted_table, original_data, categories_for_subsetting, geography, column_names):\n",
    "    collated_data = pd.DataFrame(index=range(51), columns=column_names)\n",
    "\n",
    "    x = 0\n",
    "    for geo in original_data[geography].unique():\n",
    "        #matching_indices = [i for i, geo_col in enumerate(pivoted_table) if geo_col == geo]\n",
    "        collated_data.loc[x, geography] = geo\n",
    "        columns_to_subset = [f'{geo}_{category}' for category in categories_for_subsetting]\n",
    "        j = 1\n",
    "        try:\n",
    "            for row in range(len(pivoted_table.loc[:, columns_to_subset])):\n",
    "                collated_data.iloc[x, j:j + len(categories_for_subsetting)] = pivoted_table.loc[row, columns_to_subset]\n",
    "                j += len(categories_for_subsetting)\n",
    "        except:\n",
    "            pass\n",
    "        x += 1\n",
    "\n",
    "    return collated_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(confusion_matrix):\n",
    "    # Extract values from the confusion matrix\n",
    "    TP = confusion_matrix[1, 1]\n",
    "    FP = confusion_matrix[0, 1]\n",
    "    TN = confusion_matrix[0, 0]\n",
    "    FN = confusion_matrix[1, 0]\n",
    "\n",
    "    # Calculate Sensitivity (True Positive Rate) and Specificity (True Negative Rate)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "\n",
    "    # Calculate PPV (Precision) and NPV\n",
    "    ppv = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    npv = TN / (TN + FN) if (TN + FN) > 0 else 0.0\n",
    "\n",
    "    return sensitivity, specificity, ppv, npv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_training_test_data_period(data, no_weeks, weeks_in_future, geography, weight_col, keep_output):\n",
    "## Get the weeks for the x and y datasets   \n",
    "    x_weeks = []  \n",
    "    y_weeks = []\n",
    "    y_weeks_to_check = [] #check these weeks to see if any of them are equal to 1\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_future\n",
    "        x_weeks.append('_' + num2words(week) + '_')\n",
    "        for week_y in range(week+1, test_week+1):\n",
    "                y_weeks_to_check.append('_' + num2words(week_y) + '_')\n",
    "        y_weeks.append('_' + num2words(test_week) + '_')\n",
    "    \n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all =  pd.DataFrame()\n",
    "    missing_data = []\n",
    "    HSA_IDs_list = []\n",
    "    ## Now get the training data \n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "            y_week = y_weeks[k]\n",
    "            k +=1\n",
    "\n",
    "            weeks_x = [col for col in data.columns if x_week in col]\n",
    "            columns_x  = [geography] + weeks_x + [weight_col]\n",
    "            data_x = data[columns_x]\n",
    "\n",
    "            weeks_y = [col for col in data.columns if y_week in col]\n",
    "            columns_y  = [geography] + weeks_y\n",
    "            data_y = data[columns_y]\n",
    "            ### now add the final column to the y data that has it so that it's if any week in the trhee week perdiod exceeded 15\n",
    "            train_week = w2n.word_to_num(x_week.replace(\"_\", \"\"))\n",
    "            target_week =  w2n.word_to_num(y_week.replace(\"_\", \"\"))\n",
    "            y_weeks_to_check = []\n",
    "            for week_to_check in range(train_week + 1, target_week + 1):\n",
    "                y_weeks_to_check.append('_' + num2words(week_to_check) + '_')\n",
    "\n",
    "            y_weeks_to_check = [week + 'beds_over_15_100k' for week in y_weeks_to_check]\n",
    "            columns_to_check = [col for col in data.columns if any(week in col for week in y_weeks_to_check)]\n",
    "            y_over_in_period = data[columns_to_check].apply(max, axis=1)\n",
    "            data_y = pd.concat([data_y, y_over_in_period], axis=1)\n",
    "            # ensure they have the same amount of data\n",
    "            #remove rows in test_data1 with NA in test_data2\n",
    "            data_x = data_x.dropna()\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            # remove rows in test_data2 with NA in test_data1\n",
    "            data_y = data_y.dropna()\n",
    "            data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "            missing_data.append(((len(data[geography].unique()) - data_x_no_HSA)/len(data[geography].unique())) * 100)\n",
    "            # get weights \n",
    "            #weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "            X_week = data_x.iloc[:, 1:len(columns_x)]  # take away y, leave weights for mo\n",
    "            y_week = data_y.iloc[:, -1] \n",
    "            \n",
    "            y_week = y_week.astype(int)\n",
    "\n",
    "            weights = X_week.iloc[:, -1] \n",
    "            if keep_output:\n",
    "                X_week = X_week.iloc[:, :len(X_week.columns)-1] # remove the weights and leave \"target\" for that week\n",
    "\n",
    "                #rename columns for concatenation \n",
    "                X_week.columns = range(1, len(data_x.columns) -1)\n",
    "            else:\n",
    "                X_week = X_week.iloc[:, :len(X_week.columns)-2] # remove the weights and  \"target\" for that week\n",
    "\n",
    "                X_week.columns = range(1, len(data_x.columns) -2)# remove the weights and  \"target\" for that week\n",
    "\n",
    "            y_week.columns = range(1, len(data_y.columns) -2)\n",
    "            X_data = pd.concat([X_data, X_week])\n",
    "            y_data = pd.concat([y_data, y_week]) \n",
    "            weights_all =  pd.concat([weights_all, weights]) \n",
    "            HSA_IDs_list.append(data_x[geography].reset_index(drop=True))\n",
    "\n",
    "\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "    HSA_IDs = pd.DataFrame({'HSA_ID': pd.concat(HSA_IDs_list, ignore_index=True)})\n",
    "    HSA_IDs.reset_index(drop=True, inplace=True)\n",
    "    return(X_data, y_data, weights_all, missing_data, HSA_IDs)\n",
    "\n",
    "\n",
    "### this code it's ANY in the x week period \n",
    "def prep_training_test_data(data, no_weeks, weeks_in_future, geography, weight_col, keep_output):\n",
    "## Get the weeks for the x and y datasets   \n",
    "    x_weeks = []  \n",
    "    y_weeks = []\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_future\n",
    "        x_weeks.append('_' + num2words(week) + '_')\n",
    "        y_weeks.append('_' + num2words(test_week) + '_')\n",
    "    \n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all =  pd.DataFrame()\n",
    "    missing_data = []\n",
    "    ## Now get the training data \\\n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "            y_week = y_weeks[k]\n",
    "            k += 1\n",
    "            weeks_x = [col for col in data.columns if x_week in col]\n",
    "            columns_x  = [geography] + weeks_x + [weight_col]\n",
    "            data_x = data[columns_x]\n",
    "\n",
    "            weeks_y = [col for col in data.columns if y_week in col]\n",
    "            columns_y  = [geography] + weeks_y\n",
    "            data_y = data[columns_y]\n",
    "            # ensure they have the same amount of data\n",
    "            #remove rows in test_data1 with NA in test_data2\n",
    "            data_x = data_x.dropna()\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            # remove rows in test_data2 with NA in test_data1\n",
    "            data_y = data_y.dropna()\n",
    "            data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "            missing_data.append(((len(data[geography].unique()) - data_x_no_HSA)/len(data[geography].unique())) * 100)\n",
    "            # get weights \n",
    "            #weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "            X_week = data_x.iloc[:, 1:len(columns_x)]  # take away y, leave weights for mo\n",
    "            y_week = data_y.iloc[:, -1] \n",
    "            \n",
    "            y_week = y_week.astype(int)\n",
    "\n",
    "            weights = X_week.iloc[:, -1] \n",
    "            if keep_output:\n",
    "                X_week = X_week.iloc[:, :len(X_week.columns)-1] # remove the weights and leave \"target\" for that week\n",
    "\n",
    "                #rename columns for concatenation \n",
    "                X_week.columns = range(1, len(data_x.columns) -1)\n",
    "            else:\n",
    "                X_week = X_week.iloc[:, :len(X_week.columns)-2] # remove the weights and  \"target\" for that week\n",
    "\n",
    "                X_week.columns = range(1, len(data_x.columns) -2)# remove the weights and  \"target\" for that week\n",
    "\n",
    "                #rename columns for concatenation \n",
    "            y_week.columns = range(1, len(data_y.columns) -1)\n",
    "            X_data = pd.concat([X_data, X_week])\n",
    "            y_data = pd.concat([y_data, y_week]) \n",
    "        \n",
    "            weights_all =  pd.concat([weights_all, weights]) \n",
    "\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return(X_data, y_data, weights_all, missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs cross-validation using a leave-geography-out approach with bootstrap sampling.\n",
    "# It trains and evaluates a classifier on subsets of the data for multiple iterations.\n",
    "\n",
    "def cross_validation_leave_geo_out_bootstrap(data, geography_column, geo_split, no_iterations, cv, classifier, param_grid, no_iterations_param, no_weeks_train, no_weeks_test, weeks_in_future, weight_col, keep_output, time_period):\n",
    "    \n",
    "    # Lists to store best hyperparameters and area under ROC curve (auROC) scores for each iteration\n",
    "    best_hyperparameters_per_iter = []\n",
    "    auROC_per_iter = []\n",
    "\n",
    "    # Loop through the specified number of iterations\n",
    "    for i in range(no_iterations):\n",
    "        \n",
    "        # Subset the HSAs (Health Service Areas) from the full dataset based on geography\n",
    "        geo_names = data[geography_column].unique()\n",
    "        num_names_to_select = int(geo_split * len(geo_names))\n",
    "        geos_for_sample = random.sample(list(geo_names), num_names_to_select)\n",
    "        subset_HSAs_for_train = data[data[geography_column].isin(geos_for_sample)]\n",
    "        subset_HSAs_for_test = data[~data[geography_column].isin(geos_for_sample)]\n",
    "\n",
    "        # Resample and prepare training data\n",
    "        subset_HSAs_for_train_data_resampled = subset_HSAs_for_train.sample(frac=1, replace=True)\n",
    "        subset_HSAs_for_train_data_resampled_copy = subset_HSAs_for_train_data_resampled.copy()\n",
    "        weights_train = subset_HSAs_for_train_data_resampled_copy.loc[:, 'weight']\n",
    "        y_sample_train = subset_HSAs_for_train_data_resampled_copy.loc[:, 'y']  # Assuming the last column is the target column\n",
    "        X_sample_train = subset_HSAs_for_train_data_resampled_copy.drop(subset_HSAs_for_train_data_resampled_copy.columns[-3:], axis=1) #final three are HSA, y, and weight\n",
    "\n",
    "        # Resample and prepare test data\n",
    "        subset_HSAs_for_test_data_resampled = subset_HSAs_for_test.sample(frac=1, replace=True)\n",
    "        subset_HSAs_for_test_data_resampled_copy = subset_HSAs_for_test_data_resampled.copy()\n",
    "        weights_test = subset_HSAs_for_test_data_resampled_copy.loc[:, 'weight']\n",
    "\n",
    "        y_sample_test = subset_HSAs_for_test_data_resampled_copy.loc[:, 'y']  # Assuming the last column is the target column\n",
    "        X_sample_test = subset_HSAs_for_test_data_resampled_copy.drop(subset_HSAs_for_test_data_resampled_copy.columns[-3:], axis=1) # final three are HSA, y, and weight\n",
    "\n",
    "        # Perform random search for hyperparameter tuning\n",
    "        random_search = RandomizedSearchCV(classifier, param_grid, n_iter=no_iterations_param, cv=cv, random_state=10)\n",
    "        random_search.fit(X_sample_train, y_sample_train, sample_weight=weights_train)\n",
    "        best_params = random_search.best_params_\n",
    "\n",
    "        # Create the Decision Tree classifier with the best hyperparameters\n",
    "        model = DecisionTreeClassifier(**best_params, random_state=10, class_weight='balanced')\n",
    "        model_fit = model.fit(X_sample_train, y_sample_train, sample_weight=weights_train)\n",
    "        y_pred = model_fit.predict_proba(X_sample_test)\n",
    "\n",
    "        # Evaluate the performance of the model and store results\n",
    "        best_hyperparameters_per_iter.append(best_params)\n",
    "        print(best_hyperparameters_per_iter)\n",
    "        auROC_per_iter.append(roc_auc_score(y_sample_test, y_pred[:, 1]))\n",
    "        print(roc_auc_score(y_sample_test, y_pred[:, 1]))\n",
    "    # Return the best hyperparameters based on the iteration with the highest auROC score\n",
    "    return best_hyperparameters_per_iter[np.argmax(np.array(auROC_per_iter))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentiles(iterations, model_name, ROC_actual, accuracy_actual, sensitivity_actual, specificity_actual, ppv_actual, npv_actual, X_test ,y_test):\n",
    "        bootstrapped_stats_ROC = []\n",
    "        bootstrapped_stats_accuracy = []\n",
    "        bootstrapped_stats_sesitivity = []\n",
    "        bootstrapped_stats_specificity = []\n",
    "        bootstrapped_stats_ppv = []\n",
    "        bootstrapped_stats_npv = []\n",
    "\n",
    "        for j in iterations:\n",
    "            model_name_to_load = model_name + \"_\" + str(j)+ \".sav\" \n",
    "            model_fit = pickle.load(open(model_name_to_load, 'rb'))\n",
    "            y_bootstrap_predict = model_fit.predict(X_test)\n",
    "            y_bootstrap_predict_proba = model_fit.predict_proba(X_test)\n",
    "\n",
    "            ROC_AUC_bootstrap_test_performance = metrics.roc_auc_score(y_test, y_bootstrap_predict_proba[:,1]) \n",
    "            accuracy_bootstrap_test_performance  = accuracy_score(y_test, y_bootstrap_predict)\n",
    "\n",
    "            sensitivity_bootstrap_test_performance, specificity_bootstrap_test_performance, ppv_bootstrap_test_performance, npv_bootstrap_test_performance = calculate_metrics(confusion_matrix(y_test, y_bootstrap_predict))\n",
    "        ### (D) Calculate estimate fo variance  by getting (B) - (D) \n",
    "\n",
    "            bootstrapped_stats_ROC.append({'Difference': ROC_AUC_bootstrap_test_performance - ROC_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            bootstrapped_stats_accuracy.append({'Difference': accuracy_bootstrap_test_performance - accuracy_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            bootstrapped_stats_sesitivity.append({'Difference': sensitivity_bootstrap_test_performance - sensitivity_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            bootstrapped_stats_specificity.append({'Difference': specificity_bootstrap_test_performance - specificity_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            bootstrapped_stats_ppv.append({'Difference': ppv_bootstrap_test_performance - ppv_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            bootstrapped_stats_npv.append({'Difference': npv_bootstrap_test_performance - npv_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "\n",
    "\n",
    "        bootstrapped_stats_ROC = pd.DataFrame(bootstrapped_stats_ROC)\n",
    "        bootstrapped_stats_accuracy = pd.DataFrame(bootstrapped_stats_accuracy)\n",
    "        bootstrapped_stats_sesitivity = pd.DataFrame(bootstrapped_stats_sesitivity)\n",
    "        bootstrapped_stats_specificity = pd.DataFrame(bootstrapped_stats_specificity)\n",
    "        bootstrapped_stats_ppv = pd.DataFrame(bootstrapped_stats_ppv)\n",
    "        bootstrapped_stats_npv = pd.DataFrame(bootstrapped_stats_npv)\n",
    "\n",
    "    ## Step 3: Get percentile\n",
    "        alpha = 0.05\n",
    "\n",
    "        upper_quartile_ROC, lower_quartile_ROC = ROC_actual - np.percentile(bootstrapped_stats_ROC[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        upper_quartile_accuracy, lower_quartile_accuracy = accuracy_actual - np.percentile(bootstrapped_stats_accuracy[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        upper_quartile_sensitivity, lower_quartile_sensitivity = sensitivity_actual - np.percentile(bootstrapped_stats_sesitivity[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        upper_quartile_specificity, lower_quartile_specificity = specificity_actual - np.percentile(bootstrapped_stats_specificity[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        upper_quartile_ppv, lower_quartile_ppv = ppv_actual - np.percentile(bootstrapped_stats_ppv[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        upper_quartile_npv, lower_quartile_npv = npv_actual - np.percentile(bootstrapped_stats_npv[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        ## Step 4: Get optimization-corrected performance\n",
    "\n",
    "        return upper_quartile_ROC, lower_quartile_ROC, upper_quartile_accuracy, lower_quartile_accuracy, upper_quartile_sensitivity, lower_quartile_sensitivity, upper_quartile_specificity, lower_quartile_specificity, upper_quartile_ppv, lower_quartile_ppv, upper_quartile_npv, lower_quartile_npv\n",
    "\n",
    "### now try bootstrapping w/o feature selection\n",
    "iterations = 100\n",
    "## DO NOT SAMPLE THE TARGET DATA\n",
    "def bootstrap_no_dev(iterations, training_data, clf, param_grid, cv, geo_split, iterations_param_search, model_name, time_period, keep_output, weeks_in_future, geography, weight_col, no_iterations_CV, no_weeks_train, no_weeks_test):\n",
    "      #1. Get dataset\n",
    "    for j in iterations:\n",
    "        # sample \n",
    "\n",
    "          training_data_resampled = training_data.sample(frac = 1, replace=True)\n",
    "\n",
    "          training_data_resampled_copy = training_data_resampled.copy()\n",
    "          weights_train = training_data_resampled_copy.loc[:, 'weight']\n",
    "          y_sample_train = training_data_resampled_copy.loc[:, 'y']\n",
    "\n",
    "          X_sample_train = training_data_resampled_copy.drop(['HSA_ID', 'weight', 'y'],axis=1) \n",
    "\n",
    "          best_params = cross_validation_leave_geo_out_bootstrap(training_data_resampled, geography_column=geography, geo_split=geo_split, no_iterations=no_iterations_CV, cv=cv, classifier=clf, param_grid=param_grid, no_iterations_param=iterations_param_search, no_weeks_train=no_weeks_train, no_weeks_test=no_weeks_test, weeks_in_future=weeks_in_future, weight_col=weight_col, keep_output=keep_output, time_period=time_period)\n",
    "\n",
    "\n",
    "\n",
    "# Create the Decision Tree classifier with the best hyperparameters\n",
    "          model = DecisionTreeClassifier(**best_params,random_state=10, class_weight='balanced')\n",
    "          model_fit = model.fit(X_sample_train, y_sample_train, sample_weight=weights_train)\n",
    "          path = model_fit.cost_complexity_pruning_path(X_sample_train, y_sample_train)\n",
    "          ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "          clfs = []\n",
    "          for ccp_alpha in ccp_alphas:\n",
    "              clf = DecisionTreeClassifier(**best_params,random_state=10, ccp_alpha=ccp_alpha,  class_weight='balanced')\n",
    "              clf.fit(X_sample_train, y_sample_train,  sample_weight = weights_train )\n",
    "              clfs.append(clf)\n",
    "\n",
    "          clfs = clfs[:-1]\n",
    "          ccp_alphas = ccp_alphas[:-1]\n",
    "          train_scores = [roc_auc_score(y_sample_train, clf.predict_proba(X_sample_train)[:, 1]) for clf in clfs]\n",
    "          model_pruned = clfs[train_scores.index(max(train_scores))]\n",
    "\n",
    "          model_fit = model_pruned.fit(X_sample_train, y_sample_train,  sample_weight = weights_train )\n",
    "          model_name_to_save = model_name + \"_\" + str(j)+ \".sav\" \n",
    "          X_data_name = model_name + \"_X_data_\" + str(j) + \".csv\" \n",
    "          y_data_name = model_name + \"_y_data_\" + str(j) + \".csv\" \n",
    "          weights_data_name = model_name + \"_weights_\" + str(j) + \".csv\" \n",
    "          \n",
    "          weights_train.to_csv(weights_data_name, index=False)\n",
    "          X_sample_train.to_csv(X_data_name,index=False)\n",
    "          y_sample_train.to_csv(y_data_name, index=False)\n",
    "          pickle.dump(model_fit, open(model_name_to_save, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_rename_data(data1, data2, on_column, suffix1, suffix2):\n",
    "    merged_data = pd.merge(data1, data2, on=on_column, suffixes=('_'+suffix1, '_'+suffix2))\n",
    "\n",
    "    new_column_names = [col.replace(f'_{on_column}_{suffix1}', f'_{suffix1}').replace(f'_{on_column}_{suffix2}', f'_{suffix2}') for col in merged_data.columns]\n",
    "    merged_data.rename(columns=dict(zip(merged_data.columns, new_column_names)), inplace=True)\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "def pivot_data_by_HSA(data, index_column, columns_column, values_column):\n",
    "    data_by_HSA = data[[index_column, columns_column, values_column]]\n",
    "    pivot_table = data_by_HSA.pivot_table(index=index_column, columns=columns_column, values=values_column)\n",
    "    return pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_changes_by_week(weekly_data_frame, outcome_column):\n",
    "\n",
    "    for column in weekly_data_frame.columns[1:]:\n",
    "        # Calculate the difference between each row and the previous row\n",
    "        if outcome_column not in column.lower(): #want to leave out the outcome column\n",
    "            diff = weekly_data_frame[column].diff()\n",
    "            \n",
    "            # Create a new column with the original column name and \"delta\"\n",
    "            new_column_name = column + \"_delta\"\n",
    "            \n",
    "            column_index = weekly_data_frame.columns.get_loc(column)\n",
    "            \n",
    "            # Insert the new column just after the original column\n",
    "            weekly_data_frame.insert(column_index + 1, new_column_name, diff)\n",
    "            weekly_data_frame[new_column_name] = diff\n",
    "    return weekly_data_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive classifier bootstrapping \n",
    "- Cannot bootstrap - single, binary predictor "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Needed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['gini'],#,  'entropy'],\n",
    "    'max_depth': np.arange(1, 10),\n",
    "    'min_samples_split':  np.arange(2, 300), #[100, 200, 300, 400, 500], #np.arange(50, 200),\n",
    "    'min_samples_leaf':  np.arange(2, 400)}#, #100, 200, 300, 400, 500], #np.arange(500, 200)\n",
    "    #'ccp_alpha': np.arange(0.0001, 0.0035, 0.0001) }\n",
    "\n",
    "\n",
    "# Create the Decision Tree classifier\n",
    "cv = RepeatedStratifiedKFold(n_splits=10,  n_repeats=10,random_state=1) ## 10-fold cross validations\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try CDC Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_1472/1805026620.py:1: DtypeWarning: Columns (41,43,44,45,46,50,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_by_HSA = pd.read_csv('/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates.csv')\n"
     ]
    }
   ],
   "source": [
    "data_by_HSA = pd.read_csv('/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates.csv')\n",
    "data_by_HSA['health_service_area_number']\n",
    "data_by_HSA['health_service_area']\n",
    "#data_by_HSA['HSA_ID'] = data_by_HSA['health_service_area_number'].astype(str) + '' + data_by_HSA['health_service_area'].apply(lambda x: x.split()[0])\n",
    "data_by_HSA.rename(columns={'health_service_area_number': 'HSA_ID'}, inplace=True)\n",
    "\n",
    "data_by_HSA['beds_over_15_100k'] = (data_by_HSA['beds_weekly'] > 15)*1\n",
    "\n",
    "# remove HSAs that have missing data in specific columns\n",
    "\n",
    "data_by_HSA = data_by_HSA.dropna(subset=['admits_weekly', 'deaths_weekly', 'cases_weekly', 'icu_weekly', 'beds_weekly', 'perc_covid'])\n",
    "\n",
    "for i, week in enumerate(data_by_HSA['date'].unique()):\n",
    "    data_by_HSA.loc[data_by_HSA['date'] == week, 'week'] = i\n",
    "\n",
    "    ## pivot \n",
    "data_by_HSA_cases = pivot_data_by_HSA(data_by_HSA, 'week', 'HSA_ID', 'cases_weekly')\n",
    "data_by_HSA_admissions = pivot_data_by_HSA(data_by_HSA, 'week', 'HSA_ID', 'admits_weekly')\n",
    "data_by_HSA_percent_beds = pivot_data_by_HSA(data_by_HSA, 'week', 'HSA_ID', 'perc_covid')\n",
    "data_by_HSA_over_15_100k = pivot_data_by_HSA(data_by_HSA, 'week', 'HSA_ID', 'beds_over_15_100k')\n",
    "\n",
    "## merge \n",
    "data_by_HSA_cases_admits = merge_and_rename_data(data_by_HSA_cases, data_by_HSA_admissions,'week','cases', 'admits')\n",
    "data_by_HSA_admits_perc_outcome = merge_and_rename_data(data_by_HSA_percent_beds, data_by_HSA_over_15_100k,'week','perc_covid', 'beds_over_15_100k')\n",
    "data_by_HSA_cases_admits_perc_outcome= pd.merge(data_by_HSA_cases_admits, data_by_HSA_admits_perc_outcome, on='week')\n",
    "\n",
    "\n",
    "data_by_HSA_cases_admits_perc_outcome = data_by_HSA_cases_admits_perc_outcome.reset_index()\n",
    "data_by_HSA_cases_admits_perc_outcome.columns = data_by_HSA_cases_admits_perc_outcome.columns.str.replace(',', '')\n",
    "\n",
    "categories_for_subsetting = ['cases', 'admits','perc_covid', 'beds_over_15_100k']\n",
    "num_of_weeks = len(data_by_HSA_cases_admits_perc_outcome)\n",
    "column_names = create_column_names(categories_for_subsetting, num_of_weeks)\n",
    "\n",
    "all_HSA_ID_weekly_data = create_collated_weekly_data(data_by_HSA_cases_admits_perc_outcome, data_by_HSA, categories_for_subsetting, 'HSA_ID', column_names)\n",
    "\n",
    "weights_df = data_by_HSA[data_by_HSA['HSA_ID'].isin(all_HSA_ID_weekly_data['HSA_ID'])][['HSA_ID','weight_alt']]\n",
    "weights_df = weights_df.rename(columns = {'HSA_ID': 'HSA_ID', 'weight_alt':'weight'})\n",
    "weights_df = weights_df.drop_duplicates()\n",
    "weights_df['weight'].unique()\n",
    "all_HSA_ID_weekly_data = all_HSA_ID_weekly_data.join(weights_df['weight'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimized classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, weights, missing_data_train_HSA = prep_training_test_data(all_HSA_ID_weekly_data,  no_weeks = range(1, int(123*2/3) + 1), weeks_in_future = 3,   geography = 'HSA_ID', weight_col = 'weight', keep_output = False)\n",
    "\n",
    "X_test, y_test, weights_test, missing_data_test_HSA = prep_training_test_data(all_HSA_ID_weekly_data,  no_weeks = range(int(123*2/3) + 1, 120), weeks_in_future = 3,   geography = 'HSA_ID',  weight_col = 'weight', keep_output = False)\n",
    "weights = weights[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDC_exact = pickle.load(open(\"/Users/rem76/Documents/COVID_projections/COVID_forecasting/CDC_optimized_exact_auroc_0.8269_pruned.sav\", 'rb'))\n",
    "bootstrap_no_dev(iterations =range(0,100), clf = CDC_exact,  param_grid = param_grid,  cv = cv , iterations_param_search = 10,data = all_HSA_ID_weekly_data, model_name = \"Optimized_CDC_classifier_exact_boostrap\", time_period = 'exact', no_weeks = range(1, int(123*2/3) + 1),keep_output = False, weeks_in_future = 3,   geography = 'HSA_ID', weight_col = 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8237764055669351,\n",
       " 0.8713994771420005,\n",
       " 0.7432406617945595,\n",
       " 0.7911488699418054,\n",
       " 0.7204712637938783,\n",
       " 0.8187186298497029,\n",
       " 0.72609098541295,\n",
       " 0.8105410851086158,\n",
       " 0.8610524852530571,\n",
       " 0.8916781952553877,\n",
       " 0.5711349068555243,\n",
       " 0.6465129179251549)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_percentiles(iterations = range(0,100), model_name = \"Optimized_CDC_classifier_exact_boostrap\", ROC_actual = 0.8269, accuracy_actual = 0.758, sensitivity_actual = 0.757, specificity_actual = 0.762, ppv_actual = 0.870, npv_actual = 0.599, X_test = X_test, y_test =y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced CDC optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDC_exact_enhanced = pickle.load(open(\"/Users/rem76/Documents/COVID_projections/COVID_forecasting/CDC_optimized_exact_enhanced_auroc_0.8280_pruned.sav\", 'rb'))\n",
    "X_train, y_train, weights, missing_data_train_HSA = prep_training_test_data(all_HSA_ID_weekly_data,  no_weeks = range(1, int(123*2/3) + 1), weeks_in_future = 3,   geography = 'HSA_ID', weight_col = 'weight', keep_output = True)\n",
    "\n",
    "X_test, y_test, weights_test, missing_data_test_HSA = prep_training_test_data(all_HSA_ID_weekly_data,  no_weeks = range(int(123*2/3) + 1, 120), weeks_in_future = 3,   geography = 'HSA_ID',  weight_col = 'weight', keep_output = True)\n",
    "weights = weights[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_no_dev(iterations =range(0,100), clf = CDC_exact_enhanced,  param_grid = param_grid,  cv = cv , iterations_param_search = 10,data = all_HSA_ID_weekly_data, model_name = \"Optimized_CDC_classifier_enhanced_exact_boostrap\", time_period = 'exact', no_weeks = range(1, int(123*2/3) + 1),keep_output = True, weeks_in_future = 3,   geography = 'HSA_ID', weight_col = 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8227005411620747,\n",
       " 0.8733634891878018,\n",
       " 0.7850713493030181,\n",
       " 0.8310738530247667,\n",
       " 0.8733780396464772,\n",
       " 0.9924061017626205,\n",
       " 0.5072775737223214,\n",
       " 0.6309789064959597,\n",
       " 0.7974115047131706,\n",
       " 0.8334923780487804,\n",
       " 0.7018748248482017,\n",
       " 0.8160039231066827)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_percentiles(iterations = range(0,100), model_name = \"Optimized_CDC_classifier_enhanced_exact_boostrap\", ROC_actual = 0.8280, accuracy_actual = 0.7899, sensitivity_actual =  0.873, specificity_actual = 0.631, ppv_actual = 0.833, npv_actual = 0.702, X_test = X_test, y_test =y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_HSA_ID_weekly_data = pd.read_csv('/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates_weekly.csv')\n",
    "all_HSA_ID_weekly_data.drop('Unnamed: 0', axis =1, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_period = pickle.load(open(\"/Users/rem76/Documents/COVID_projections/COVID_forecasting/Full_auroc_0.9092_period_pruned.sav\", 'rb'))\n",
    "\n",
    "X_train, y_train, weights, missing_data_train_HSA, HSA_ID_train = prep_training_test_data_period(all_HSA_ID_weekly_data,   no_weeks = range(1, int(123*2/3) + 1), weeks_in_future = 3,   geography = 'HSA_ID', weight_col = 'weight', keep_output = True)\n",
    "\n",
    "X_test, y_test, weights_test, missing_data_test_HSA, HSA_ID = prep_training_test_data_period(all_HSA_ID_weekly_data,   no_weeks = range(int(123*2/3) + 1, 120), weeks_in_future = 3,   geography = 'HSA_ID',  weight_col = 'weight', keep_output = True)\n",
    "#weights = weights[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.merge(X_train, y_train, left_index=True, right_index=True)\n",
    "training_data.rename(columns={0: 'y'}, inplace=True)\n",
    "training_data = pd.merge(training_data, weights, left_index=True, right_index=True)\n",
    "training_data.rename(columns={0: 'weight'}, inplace=True)\n",
    "training_data = pd.merge(training_data, HSA_ID_train, left_index=True, right_index=True)\n",
    "training_data.rename(columns={0: 'HSA_ID'}, inplace=True)\n",
    "weights = weights[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_iterations_CV = 10\n",
    "geography_column = 'HSA_ID'  \n",
    "geo_split = 0.9  \n",
    "time_period = 'period'  # Choose 'period', 'exact', or 'shifted'\n",
    "no_weeks_train =  range(1, int(123*2/3) + 1)  # across entire training period\n",
    "no_weeks_test = range(int(123*2/3) + 1, 120) # across entire training period\n",
    "weeks_in_future = 3 \n",
    "weight_col = 'weight'  \n",
    "keep_output = True  \n",
    "no_iterations_param = 20  # Replace with the number of iterations for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': np.arange(2, 5, 1),\n",
    "    'min_samples_split': np.arange(200, 2000, 50), #[100, 200, 300, 400, 500], #np.arange(50, 200),\n",
    "    'min_samples_leaf':  np.arange(200, 2000, 50)} #100, 200, 300, 400, 500], #np.arange(500, 200)\n",
    "    #'ccp_alpha': np.arange(0.0001, 0.0035, 0.0001) }\n",
    "clf = DecisionTreeClassifier(random_state=10, class_weight='balanced')\n",
    "cv = RepeatedStratifiedKFold(n_splits=10,  n_repeats=10,random_state=1)  ## 10-fold cross validations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9313755018441309\n",
      "0.941065952029627\n",
      "0.936976241475838\n",
      "0.9498619741142746\n",
      "0.9088579950226103\n",
      "0.9271055625411653\n",
      "0.9289600647885079\n",
      "0.9408022402595915\n",
      "0.9370066506347459\n",
      "0.9207916839179039\n",
      "0.9170433404351779\n",
      "0.9117436022607238\n",
      "0.9243117393929001\n",
      "0.908844890320583\n",
      "0.9165535237291362\n",
      "0.8888914158807721\n",
      "0.9245235123959383\n",
      "0.9298920266038022\n",
      "0.9156231493528652\n",
      "0.9205073915962139\n",
      "0.9079066387887955\n",
      "0.9091958740336026\n",
      "0.9292931685994859\n",
      "0.9284183172466649\n",
      "0.9287799352006155\n",
      "0.9090930769896728\n",
      "0.9253275845812517\n",
      "0.8969081786087866\n",
      "0.922526275911093\n",
      "0.9149927495851237\n",
      "0.9257409556439729\n",
      "0.9227604210816309\n",
      "0.9249236491918119\n",
      "0.9368443887726468\n",
      "0.9037813767539247\n",
      "0.9354429083461342\n",
      "0.9245557568922623\n",
      "0.900252373512799\n",
      "0.9343824643552064\n",
      "0.9239232568692496\n",
      "0.9028639968271601\n",
      "0.9482822459692538\n",
      "0.9337387844295887\n",
      "0.9151440288944832\n",
      "0.9094472151578206\n",
      "0.8899590295991504\n",
      "0.9115178795633645\n",
      "0.924937577746804\n",
      "0.9272959582208137\n",
      "0.9160110874615696\n",
      "0.902638424794241\n",
      "0.908262754524175\n",
      "0.9297315150524348\n",
      "0.9456726325740121\n",
      "0.9263105842059267\n",
      "0.9263637371711436\n",
      "0.9187887010960262\n",
      "0.9246741983714752\n",
      "0.8902217925198737\n",
      "0.9278104209685546\n",
      "0.9416684259725481\n",
      "0.9073293544761911\n",
      "0.9227599327386642\n",
      "0.9238343095342287\n",
      "0.916805325434767\n",
      "0.9151962715731082\n",
      "0.9215617049906438\n",
      "0.920551488988989\n",
      "0.9110985815632546\n",
      "0.9331468449060367\n",
      "0.9193626239197976\n",
      "0.9083067432223187\n",
      "0.8974641593592638\n",
      "0.9234236997674236\n",
      "0.9260294552102\n",
      "0.9229450428266195\n",
      "0.9235528144657139\n",
      "0.931726670480338\n",
      "0.9337589242557289\n",
      "0.9206752981786046\n",
      "0.9303326567958741\n",
      "0.9221751114181393\n",
      "0.9022090969676775\n",
      "0.9184366117543479\n",
      "0.9241794002078398\n",
      "0.9326365766432358\n",
      "0.9059135157878632\n",
      "0.9019301902844491\n",
      "0.9210744837550715\n",
      "0.9083925715001866\n",
      "0.9332969065232011\n",
      "0.9248858643901611\n",
      "0.9010189834935932\n",
      "0.9136831734064429\n",
      "0.9091894977620659\n",
      "0.9074478341330436\n",
      "0.9194265540164885\n",
      "0.9226171941252544\n",
      "0.921953930887277\n",
      "0.9272542605221865\n"
     ]
    }
   ],
   "source": [
    "bootstrap_no_dev(training_data = training_data,iterations =range(0,10), clf = clf,  geography = geography_column, no_iterations_CV = no_iterations_CV, param_grid = param_grid,  cv = cv , iterations_param_search = no_iterations_param, model_name = \"Full_classifier_period_boostrap\", geo_split=geo_split,  no_weeks_train=no_weeks_train, no_weeks_test=no_weeks_test, weeks_in_future=weeks_in_future, weight_col=weight_col, keep_output=keep_output, time_period=time_period )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.931621636931448,\n",
       " 0.9386890847618555,\n",
       " 0.8870082555149547,\n",
       " 0.9619372039518203,\n",
       " 0.9176128106671168,\n",
       " 1.0244274441959575,\n",
       " 0.7114550110978316,\n",
       " 0.7655441352228103,\n",
       " 0.923435888481974,\n",
       " 0.9376971124024269,\n",
       " 0.6479941659273787,\n",
       " 0.7259253046584826)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_percentiles(iterations = range(1,10), model_name = \"Full_classifier_period_boostrap\", ROC_actual = 0.909, accuracy_actual = 0.843, sensitivity_actual = 0.847, specificity_actual = 0.828, ppv_actual = 0.952, npv_actual = 0.572, X_test = X_test, y_test =y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = [1]\n",
    "geography = 'HSA_ID'\n",
    "iterations_param_search = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data_resampled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m best_params \u001b[39m=\u001b[39m cross_validation_leave_geo_out_bootstrap(\n\u001b[0;32m----> 2\u001b[0m     training_data_resampled,\n\u001b[1;32m      3\u001b[0m     geography_column\u001b[39m=\u001b[39mgeography,\n\u001b[1;32m      4\u001b[0m     geo_split\u001b[39m=\u001b[39mgeo_split,\n\u001b[1;32m      5\u001b[0m     no_iterations\u001b[39m=\u001b[39mno_iterations_CV,\n\u001b[1;32m      6\u001b[0m     cv\u001b[39m=\u001b[39mcv,\n\u001b[1;32m      7\u001b[0m     classifier\u001b[39m=\u001b[39mclf,\n\u001b[1;32m      8\u001b[0m     param_grid\u001b[39m=\u001b[39mparam_grid,\n\u001b[1;32m      9\u001b[0m     no_iterations_param\u001b[39m=\u001b[39miterations_param_search,\n\u001b[1;32m     10\u001b[0m     no_weeks_train\u001b[39m=\u001b[39mno_weeks_train,\n\u001b[1;32m     11\u001b[0m     no_weeks_test\u001b[39m=\u001b[39mno_weeks_test,\n\u001b[1;32m     12\u001b[0m     weeks_in_future\u001b[39m=\u001b[39mweeks_in_future,\n\u001b[1;32m     13\u001b[0m     weight_col\u001b[39m=\u001b[39mweight_col,\n\u001b[1;32m     14\u001b[0m     keep_output\u001b[39m=\u001b[39mkeep_output,\n\u001b[1;32m     15\u001b[0m     time_period\u001b[39m=\u001b[39mtime_period\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_data_resampled' is not defined"
     ]
    }
   ],
   "source": [
    "    best_params = cross_validation_leave_geo_out_bootstrap(\n",
    "        training_data_resampled,\n",
    "        geography_column=geography,\n",
    "        geo_split=geo_split,\n",
    "        no_iterations=no_iterations_CV,\n",
    "        cv=cv,\n",
    "        classifier=clf,\n",
    "        param_grid=param_grid,\n",
    "        no_iterations_param=iterations_param_search,\n",
    "        no_weeks_train=no_weeks_train,\n",
    "        no_weeks_test=no_weeks_test,\n",
    "        weeks_in_future=weeks_in_future,\n",
    "        weight_col=weight_col,\n",
    "        keep_output=keep_output,\n",
    "        time_period=time_period\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "    best_params = {'min_samples_split': 1250,\n",
    " 'min_samples_leaf': 1300,\n",
    " 'max_depth': 4,\n",
    " 'criterion': 'entropy'}\n",
    "\n",
    "    model = DecisionTreeClassifier(**best_params, random_state=10, class_weight='balanced')\n",
    "    model_fit = model.fit(X_sample_train, y_sample_train, sample_weight=weights_train)\n",
    "    \n",
    "    # Perform cost complexity pruning\n",
    "    path = model_fit.cost_complexity_pruning_path(X_sample_train, y_sample_train)\n",
    "    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "    clfs = []\n",
    "    \n",
    "    for ccp_alpha in ccp_alphas:\n",
    "        clf = DecisionTreeClassifier(**best_params, random_state=10, ccp_alpha=ccp_alpha, class_weight='balanced')\n",
    "        clf.fit(X_sample_train, y_sample_train, sample_weight=weights_train)\n",
    "        clfs.append(clf)\n",
    "    \n",
    "    clfs = clfs[:-1]\n",
    "    ccp_alphas = ccp_alphas[:-1]\n",
    "    train_scores = [roc_auc_score(y_sample_train, clf.predict_proba(X_sample_train)[:, 1]) for clf in clfs]\n",
    "    model_pruned = clfs[train_scores.index(max(train_scores))]\n",
    "\n",
    "    model_fit = model_pruned.fit(X_sample_train, y_sample_train, sample_weight=weights_train)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Full_classifier_period_boostrap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "        bootstrapped_stats_ROC = []\n",
    "        bootstrapped_stats_accuracy = []\n",
    "        bootstrapped_stats_sesitivity = []\n",
    "        bootstrapped_stats_specificity = []\n",
    "        bootstrapped_stats_ppv = []\n",
    "        bootstrapped_stats_npv = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_actual = 0.909\n",
    "accuracy_actual = 0.843\n",
    "sensitivity_actual = 0.847\n",
    "specificity_actual = 0.828\n",
    "ppv_actual = 0.952\n",
    "npv_actual = 0.572"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}]\n",
      "0.9121996263120155\n",
      "[{'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}]\n",
      "0.90166383517969\n",
      "[{'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}]\n",
      "0.9242068842999263\n",
      "[{'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}]\n",
      "0.9283527582098459\n",
      "[{'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}]\n",
      "0.9313688146911518\n",
      "[{'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}]\n",
      "0.9263371089483539\n",
      "[{'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}]\n",
      "0.9127923529868265\n",
      "[{'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}]\n",
      "0.9271298784014291\n",
      "[{'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}]\n",
      "0.913137284161367\n",
      "[{'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}, {'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}]\n",
      "0.9299053125085219\n",
      "{'min_samples_split': 1650, 'min_samples_leaf': 1950, 'max_depth': 2, 'criterion': 'gini'}\n",
      "DecisionTreeClassifier(class_weight='balanced', max_depth=2,\n",
      "                       min_samples_leaf=1950, min_samples_split=1650,\n",
      "                       random_state=10)\n",
      "sensitivity:  0.7758133254567703\n",
      "specificity:  0.8907290421717603\n",
      "ppv:  0.9663618206664564\n",
      "npv:  0.49544159544159544\n"
     ]
    }
   ],
   "source": [
    "for j in iterations:\n",
    "    \n",
    "    X_train, y_train, weights, missing_data_train_HSA, HSA_ID_train = prep_training_test_data_period(all_HSA_ID_weekly_data,   no_weeks = range(1, int(123*2/3) + 1), weeks_in_future = 3,   geography = 'HSA_ID', weight_col = 'weight', keep_output = True)\n",
    "\n",
    "    X_test, y_test, weights_test, missing_data_test_HSA, HSA_ID = prep_training_test_data_period(all_HSA_ID_weekly_data,   no_weeks = range(int(123*2/3) + 1, 120), weeks_in_future = 3,   geography = 'HSA_ID',  weight_col = 'weight', keep_output = True)\n",
    "#weights = weights[0].to_numpy()\n",
    "    training_data = pd.merge(X_train, y_train, left_index=True, right_index=True)\n",
    "    training_data.rename(columns={0: 'y'}, inplace=True)\n",
    "    training_data = pd.merge(training_data, weights, left_index=True, right_index=True)\n",
    "    training_data.rename(columns={0: 'weight'}, inplace=True)\n",
    "    training_data = pd.merge(training_data, HSA_ID_train, left_index=True, right_index=True)\n",
    "    training_data.rename(columns={0: 'HSA_ID'}, inplace=True)\n",
    "    weights = weights[0].to_numpy()\n",
    "    # Sample the training data\n",
    "    training_data_resampled = training_data.sample(frac=1, replace=True)\n",
    "    training_data_resampled_copy = training_data_resampled.copy()\n",
    "    weights_train = training_data_resampled_copy.loc[:, 'weight']\n",
    "    y_sample_train = training_data_resampled_copy.loc[:, 'y']\n",
    "    X_sample_train = training_data_resampled_copy.drop(['HSA_ID', 'weight', 'y'], axis=1)\n",
    "\n",
    "    # Find the best hyperparameters using cross-validation\n",
    "    best_params = cross_validation_leave_geo_out_bootstrap(\n",
    "        training_data_resampled,\n",
    "        geography_column=geography,\n",
    "        geo_split=geo_split,\n",
    "        no_iterations=no_iterations_CV,\n",
    "        cv=cv,\n",
    "        classifier=clf,\n",
    "        param_grid=param_grid,\n",
    "        no_iterations_param=iterations_param_search,\n",
    "        no_weeks_train=no_weeks_train,\n",
    "        no_weeks_test=no_weeks_test,\n",
    "        weeks_in_future=weeks_in_future,\n",
    "        weight_col=weight_col,\n",
    "        keep_output=keep_output,\n",
    "        time_period=time_period\n",
    "    )\n",
    "\n",
    "    # Create the Decision Tree classifier with the best hyperparameters\n",
    "    model = DecisionTreeClassifier(**best_params, random_state=10, class_weight='balanced')\n",
    "    model_fit = model.fit(X_sample_train, y_sample_train, sample_weight=weights_train)\n",
    "    print(best_params)\n",
    "    # Perform cost complexity pruning\n",
    "    path = model_fit.cost_complexity_pruning_path(X_sample_train, y_sample_train)\n",
    "    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "    clfs = []\n",
    "    \n",
    "    for ccp_alpha in ccp_alphas:\n",
    "        clf = DecisionTreeClassifier(**best_params, random_state=10, ccp_alpha=ccp_alpha, class_weight='balanced')\n",
    "        clf.fit(X_sample_train, y_sample_train, sample_weight=weights_train)\n",
    "        clfs.append(clf)\n",
    "    \n",
    "    clfs = clfs[:-1]\n",
    "    ccp_alphas = ccp_alphas[:-1]\n",
    "    train_scores = [roc_auc_score(y_sample_train, clf.predict_proba(X_sample_train)[:, 1]) for clf in clfs]\n",
    "    model_pruned = clfs[train_scores.index(max(train_scores))]\n",
    "\n",
    "    model_fit = model_pruned.fit(X_sample_train, y_sample_train, sample_weight=weights_train)\n",
    "    print(model_fit)\n",
    "    # Save the model and related data\n",
    "    model_name_to_save = model_name + \"_\" + str(j) + \".sav\"\n",
    "    X_data_name = model_name + \"_X_data_\" + str(j) + \".csv\"\n",
    "    y_data_name = model_name + \"_y_data_\" + str(j) + \".csv\"\n",
    "    weights_data_name = model_name + \"_weights_\" + str(j) + \".csv\"\n",
    "    weights_train.to_csv(weights_data_name, index=False)\n",
    "    X_sample_train.to_csv(X_data_name,index=False)\n",
    "    y_sample_train.to_csv(y_data_name, index=False)\n",
    "    pickle.dump(model_fit, open(model_name_to_save, 'wb'))\n",
    "    y_pred = model_pruned.predict(X_test)\n",
    "    y_pred_proba = model_pruned.predict_proba(X_test)\n",
    "\n",
    "    # Evaluate the accuracy of the model\n",
    "    accuracy_bootstrap_test_performance = accuracy_score(y_test, y_pred)\n",
    "    ROC_AUC_bootstrap_test_performance = roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    # Calculate sensitivity (True Positive Rate)\n",
    "    sensitivity_actual = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    print(\"sensitivity: \", sensitivity_actual)\n",
    "    # Calculate specificity (True Negative Rate)\n",
    "    specificity_actual = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    print(\"specificity: \", specificity_actual)\n",
    "\n",
    "    # Calculate PPV (Positive Predictive Value)\n",
    "    ppv_actual = conf_matrix[1, 1] / (conf_matrix[0, 1] + conf_matrix[1, 1])\n",
    "    print(\"ppv: \", ppv_actual)\n",
    "\n",
    "    # Calculate NPV (Negative Predictive Value)\n",
    "    npv_actual = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[1, 0])\n",
    "    print(\"npv: \", npv_actual)\n",
    "\n",
    "    sensitivity_bootstrap_test_performance, specificity_bootstrap_test_performance, ppv_bootstrap_test_performance, npv_bootstrap_test_performance = calculate_metrics(confusion_matrix(y_test, y_pred))\n",
    "        ### (D) Calculate estimate fo variance  by getting (B) - (D) \n",
    "\n",
    "    bootstrapped_stats_ROC.append({'Difference': ROC_AUC_bootstrap_test_performance - ROC_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "    bootstrapped_stats_accuracy.append({'Difference': accuracy_bootstrap_test_performance - accuracy_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "    bootstrapped_stats_sesitivity.append({'Difference': sensitivity_bootstrap_test_performance - sensitivity_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "    bootstrapped_stats_specificity.append({'Difference': specificity_bootstrap_test_performance - specificity_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "    bootstrapped_stats_ppv.append({'Difference': ppv_bootstrap_test_performance - ppv_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "    bootstrapped_stats_npv.append({'Difference': npv_bootstrap_test_performance - npv_actual}) ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "\n",
    "\n",
    "bootstrapped_stats_ROC = pd.DataFrame(bootstrapped_stats_ROC)\n",
    "bootstrapped_stats_accuracy = pd.DataFrame(bootstrapped_stats_accuracy)\n",
    "bootstrapped_stats_sesitivity = pd.DataFrame(bootstrapped_stats_sesitivity)\n",
    "bootstrapped_stats_specificity = pd.DataFrame(bootstrapped_stats_specificity)\n",
    "bootstrapped_stats_ppv = pd.DataFrame(bootstrapped_stats_ppv)\n",
    "bootstrapped_stats_npv = pd.DataFrame(bootstrapped_stats_npv)\n",
    "\n",
    "    ## Step 3: Get percentile\n",
    "alpha = 0.05\n",
    "\n",
    "upper_quartile_ROC, lower_quartile_ROC = ROC_actual - np.percentile(bootstrapped_stats_ROC[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "upper_quartile_accuracy, lower_quartile_accuracy = accuracy_actual - np.percentile(bootstrapped_stats_accuracy[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "upper_quartile_sensitivity, lower_quartile_sensitivity = sensitivity_actual - np.percentile(bootstrapped_stats_sesitivity[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "upper_quartile_specificity, lower_quartile_specificity = specificity_actual - np.percentile(bootstrapped_stats_specificity[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "upper_quartile_ppv, lower_quartile_ppv = ppv_actual - np.percentile(bootstrapped_stats_ppv[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "upper_quartile_npv, lower_quartile_npv = npv_actual - np.percentile(bootstrapped_stats_npv[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_full_classifier = '/Users/rem76/Documents/COVID_projections/COVID_forecasting/Full_auroc_0.9092_period_pruned.sav'\n",
    "model_fit_full_classifer = pickle.load(open(model_name_full_classifier, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8113073487616728\n",
      "0.9091874097110433\n",
      "sensitivity:  0.7948436642896325\n",
      "specificity:  0.8779238518012634\n",
      "ppv:  0.9634308510638298\n",
      "npv:  0.5139944022391043\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "\n",
    "y_pred = model_fit_full_classifer.predict(X_test)\n",
    "y_pred_proba = model_fit_full_classifer.predict_proba(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy_actual = accuracy_score(y_test, y_pred)\n",
    "ROC_actual = roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "print(accuracy_actual)\n",
    "print(ROC_actual)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# Calculate sensitivity (True Positive Rate)\n",
    "sensitivity_actual = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "print(\"sensitivity: \", sensitivity_actual)\n",
    "# Calculate specificity (True Negative Rate)\n",
    "specificity_actual = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "print(\"specificity: \", specificity_actual)\n",
    "\n",
    "# Calculate PPV (Positive Predictive Value)\n",
    "ppv_actual = conf_matrix[1, 1] / (conf_matrix[0, 1] + conf_matrix[1, 1])\n",
    "print(\"ppv: \", ppv_actual)\n",
    "\n",
    "# Calculate NPV (Negative Predictive Value)\n",
    "npv_actual = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[1, 0])\n",
    "print(\"npv: \", npv_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Full_classifier_period_boostrap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(0,10)\n",
    "feature_names = ['cases','delta_cases', 'deaths', 'delta_deaths', 'admits', 'delta_admits', 'icu', 'delta_icu',  'beds', 'delta_beds', 'perc_covid', 'delta_perc', 'beds_over_15_100k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, weights, missing_data_train_HSA, p = prep_training_test_data_period(all_HSA_ID_weekly_data,   no_weeks = range(1, int(123*2/3) + 1), weeks_in_future = 3,   geography = 'HSA_ID', weight_col = 'weight', keep_output = True)\n",
    "\n",
    "X_test, y_test, weights_test, missing_data_test_HSA, p = prep_training_test_data_period(all_HSA_ID_weekly_data,   no_weeks = range(int(123*2/3) + 1, 120), weeks_in_future = 3,   geography = 'HSA_ID',  weight_col = 'weight', keep_output = True)\n",
    "weights = weights[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7984842333197997\n",
      "0.886033636400372\n"
     ]
    }
   ],
   "source": [
    "model_fit = DecisionTreeClassifier(class_weight='balanced', max_depth=5,\n",
    "                       min_samples_leaf=200, min_samples_split=100,\n",
    "                       random_state=10)\n",
    "model_name_to_load = model_name + \"_\" + str(j) + \".sav\"\n",
    "model_fit = pickle.load(open(model_name_to_load, 'rb'))\n",
    "\n",
    "X_sample_train = pd.read_csv('Full_classifier_period_boostrap_X_data_1.csv')\n",
    "X_sample_train.columns = feature_names\n",
    "\n",
    "y_sample_train = pd.read_csv('Full_classifier_period_boostrap_y_data_1.csv')\n",
    "weights_train =  pd.read_csv('Full_classifier_period_boostrap_weights_1.csv')\n",
    "weights_train = weights_train['weight']\n",
    "model_fit.fit(X_train, y_train, sample_weight=weights)\n",
    "y_pred = model_fit.predict(X_test)\n",
    "y_pred_proba = model_fit.predict_proba(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy_actual = accuracy_score(y_test, y_pred)\n",
    "ROC_actual = roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "print(accuracy_actual)\n",
    "print(ROC_actual)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrapped_stats_ROC = []\n",
    "bootstrapped_stats_accuracy = []\n",
    "bootstrapped_stats_sensitivity = []\n",
    "bootstrapped_stats_specificity = []\n",
    "bootstrapped_stats_ppv = []\n",
    "bootstrapped_stats_npv = []\n",
    "\n",
    "for j in iterations:\n",
    "    model_name_to_load = model_name + \"_\" + str(j) + \".sav\"\n",
    "    model_fit = pickle.load(open(model_name_to_load, 'rb'))\n",
    "    y_bootstrap_predict = model_fit.predict(X_test)\n",
    "    y_bootstrap_predict_proba = model_fit.predict_proba(X_test)\n",
    "\n",
    "    ROC_AUC_bootstrap_test_performance = roc_auc_score(y_test, y_bootstrap_predict_proba[:, 1])\n",
    "    accuracy_bootstrap_test_performance = accuracy_score(y_test, y_bootstrap_predict)\n",
    "\n",
    "    sensitivity_bootstrap_test_performance, specificity_bootstrap_test_performance, ppv_bootstrap_test_performance, npv_bootstrap_test_performance = calculate_metrics(confusion_matrix(y_test, y_bootstrap_predict))\n",
    "\n",
    "    # (D) Calculate estimate of variance by getting (B) - (D)\n",
    "    bootstrapped_stats_ROC.append({'Difference': ROC_AUC_bootstrap_test_performance - ROC_actual})\n",
    "    bootstrapped_stats_accuracy.append({'Difference': accuracy_bootstrap_test_performance - accuracy_actual})\n",
    "    bootstrapped_stats_sensitivity.append({'Difference': sensitivity_bootstrap_test_performance - sensitivity_actual})\n",
    "    bootstrapped_stats_specificity.append({'Difference': specificity_bootstrap_test_performance - specificity_actual})\n",
    "    bootstrapped_stats_ppv.append({'Difference': ppv_bootstrap_test_performance - ppv_actual})\n",
    "    bootstrapped_stats_npv.append({'Difference': npv_bootstrap_test_performance - npv_actual})\n",
    "\n",
    "bootstrapped_stats_ROC = pd.DataFrame(bootstrapped_stats_ROC)\n",
    "bootstrapped_stats_accuracy = pd.DataFrame(bootstrapped_stats_accuracy)\n",
    "bootstrapped_stats_sensitivity = pd.DataFrame(bootstrapped_stats_sensitivity)\n",
    "bootstrapped_stats_specificity = pd.DataFrame(bootstrapped_stats_specificity)\n",
    "bootstrapped_stats_ppv = pd.DataFrame(bootstrapped_stats_ppv)\n",
    "bootstrapped_stats_npv = pd.DataFrame(bootstrapped_stats_npv)\n",
    "\n",
    "# Step 3: Get percentile\n",
    "alpha = 0.05\n",
    "\n",
    "upper_quartile_ROC, lower_quartile_ROC = ROC_actual - np.percentile(bootstrapped_stats_ROC[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "upper_quartile_accuracy, lower_quartile_accuracy = accuracy_actual - np.percentile(bootstrapped_stats_accuracy[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "upper_quartile_sensitivity, lower_quartile_sensitivity = sensitivity_actual - np.percentile(bootstrapped_stats_sensitivity[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "upper_quartile_specificity, lower_quartile_specificity = specificity_actual - np.percentile(bootstrapped_stats_specificity[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "upper_quartile_ppv, lower_quartile_ppv = ppv_actual - np.percentile(bootstrapped_stats_ppv[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "upper_quartile_npv, lower_quartile_npv = npv_actual - np.percentile(bootstrapped_stats_npv[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "\n",
    "# Step 4: Get optimization-corrected performance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COVID_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a72d884e172d11118bfdfb578e108fb6b63a679c74d3d7d2f9d493a9a72737c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
