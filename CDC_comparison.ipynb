{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from num2words import num2words\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, KFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score\n",
    "import word2number\n",
    "from word2number import w2n\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_covid_outcome(new_cases_per_100k, new_admits_per_100k, percent_beds_100k):\n",
    "    if new_cases_per_100k < 200:\n",
    "        if (new_admits_per_100k >= 10) | (percent_beds_100k > 10):\n",
    "            if (new_admits_per_100k >= 20) | (percent_beds_100k >= 15):\n",
    "                return \"High\"\n",
    "            else:\n",
    "                return \"Medium\"\n",
    "        else:\n",
    "            return \"Low\"\n",
    "    elif 200 <= new_cases_per_100k:\n",
    "        if (new_admits_per_100k >= 10) | (percent_beds_100k > 10):\n",
    "            if (new_admits_per_100k >= 10) | (percent_beds_100k >= 10):\n",
    "                return \"High\"\n",
    "            else:\n",
    "                return \"Medium\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this code it's ANY in the x week period \n",
    "\n",
    "def prep_training_test_data_period(data, proportion, no_weeks, weeks_in_futre, if_train, geography, weight_col):\n",
    "## Get the weeks for the x and y datasets   \n",
    "    x_weeks = []  \n",
    "    y_weeks = []\n",
    "    y_weeks_to_check = [] #check these weeks to see if any of them are equal to 1\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_futre\n",
    "        x_weeks.append('_' + num2words(week) + '_')\n",
    "        for week_y in range(week+1, test_week+1):\n",
    "                y_weeks_to_check.append('_' + num2words(week_y) + '_')\n",
    "        y_weeks.append('_' + num2words(test_week) + '_')\n",
    "    \n",
    "## Divide up the test/train split\n",
    "    #if is_geographic:\n",
    "        # Calculate the index to start slicing from\n",
    "    #    start_index = len(data['county']) // proportion[0] * proportion[1]\n",
    "        # Divide up the dataset based on this proportion\n",
    "    #    first_two_thirds = data['county'][:start_index]\n",
    "    #    last_third = data['county'][start_index:]\n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all =  pd.DataFrame()\n",
    "    missing_data = []\n",
    "    ## Now get the training data \n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "            y_week = y_weeks[k]\n",
    "            k +=1\n",
    "\n",
    "            weeks_x = [col for col in data.columns if x_week in col]\n",
    "            columns_x  = [geography] + weeks_x + [weight_col]\n",
    "            data_x = data[columns_x]\n",
    "\n",
    "            weeks_y = [col for col in data.columns if y_week in col]\n",
    "            columns_y  = [geography] + weeks_y\n",
    "            data_y = data[columns_y]\n",
    "            ### now add the final column to the y data that has it so that it's if any week in the trhee week perdiod exceeded 15\n",
    "            train_week = w2n.word_to_num(x_week.replace(\"_\", \"\"))\n",
    "            target_week =  w2n.word_to_num(y_week.replace(\"_\", \"\"))\n",
    "            y_weeks_to_check = []\n",
    "            for week_to_check in range(train_week + 1, target_week + 1):\n",
    "                y_weeks_to_check.append('_' + num2words(week_to_check) + '_')\n",
    "\n",
    "            y_weeks_to_check = [week + 'beds_over_15_100k' for week in y_weeks_to_check]\n",
    "            columns_to_check = [col for col in data.columns if any(week in col for week in y_weeks_to_check)]\n",
    "            y_over_in_period = data[columns_to_check].apply(max, axis=1)\n",
    "            data_y = pd.concat([data_y, y_over_in_period], axis=1)\n",
    "            # ensure they have the same amount of data\n",
    "            #remove rows in test_data1 with NA in test_data2\n",
    "            data_x = data_x.dropna()\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            # remove rows in test_data2 with NA in test_data1\n",
    "            data_y = data_y.dropna()\n",
    "            data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "            missing_data.append(((len(data[geography].unique()) - data_x_no_HSA)/len(data[geography].unique())) * 100)\n",
    "            # get weights \n",
    "            #weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "            X_week = data_x.iloc[:, 1:len(columns_x)]  # take away y, leave weights for mo\n",
    "            y_week = data_y.iloc[:, -1] \n",
    "            \n",
    "            y_week = y_week.astype(int)\n",
    "            if if_train:\n",
    "\n",
    "                 X_week, y_week = oversample.fit_resample(X_week, y_week)\n",
    "            weights = X_week.iloc[:, -1] \n",
    "            X_week = X_week.iloc[:, :len(X_week.columns)-1] # remove the weights and leave \"target\" for that week, inlcuding it as a feature\n",
    "\n",
    "            #rename columns for concatenation \n",
    "            X_week.columns = range(1, len(data_x.columns) -1)\n",
    "            y_week.columns = range(1, len(data_y.columns) -1)\n",
    "            X_data = pd.concat([X_data, X_week])\n",
    "            y_data = pd.concat([y_data, y_week]) \n",
    "        \n",
    "            weights_all =  pd.concat([weights_all, weights]) \n",
    "\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return(X_data, y_data, weights_all, missing_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prep\n",
    "For the CDC, they only use three metrics: new_cases_per_100k, new_admits_per_100k, percent_beds_100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_55623/2861294902.py:1: DtypeWarning: Columns (41,43,44,45,46,50,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_by_HSA = pd.read_csv('/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'date', 'HSA_ID', 'health_service_area',\n",
       "       'health_service_area_population', 'cases_avg', 'deaths_avg',\n",
       "       'cases_avg_per_100k', 'deaths_avg_per_100k', 'POPESTIMATE2019', 'ymd',\n",
       "       'year', 'week', 'year_wk', 'admits_confirmed_avg', 'perc_covid',\n",
       "       'admits_confirmed_100K', 'icu_confirmed_avg', 'icu_100K', 'beds_100k',\n",
       "       'cdc_flag_1', 'cdc_flag_2', 'cdc_flag', 'deaths_21_lag_100k',\n",
       "       'icu_21_lag_100K', 'cases_lag_21_100K', 'admits_7_lag', 'admits_7d_ago',\n",
       "       'admits_21d_ago', 'admits_28d_ago', 'dotw', 'chk', 'county_rank',\n",
       "       'deaths_21_lag_100k_14d', 'deaths_weekly', 'admits_weekly',\n",
       "       'cases_weekly', 'icu_weekly', 'beds_weekly', 'perc_covid_100', 'cfr',\n",
       "       'half_zeke_time_3', 'chk2', 'zeke_time_3', 'two_zeke_time_3',\n",
       "       'icu_2_time_3', 'perc_covid_10_time_3', 'change_admits', 'change_perc',\n",
       "       'change_cases', 'zeke_time_3_14d', 'two_zeke_time_3_14d', 'state',\n",
       "       'weight', 'weight_alt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_by_HSA = pd.read_csv('/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates.csv')\n",
    "data_by_HSA['health_service_area_number']\n",
    "data_by_HSA['health_service_area']\n",
    "#data_by_HSA['HSA_ID'] = data_by_HSA['health_service_area_number'].astype(str) + '' + data_by_HSA['health_service_area'].apply(lambda x: x.split()[0])\n",
    "data_by_HSA.rename(columns={'health_service_area_number': 'HSA_ID'}, inplace=True)\n",
    "data_by_HSA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_55623/1329533506.py:1: DtypeWarning: Columns (41,43,44,45,46,50,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_by_HSA = pd.read_csv('/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates.csv')\n"
     ]
    }
   ],
   "source": [
    "data_by_HSA = pd.read_csv('/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates.csv')\n",
    "data_by_HSA['health_service_area_number']\n",
    "data_by_HSA['health_service_area']\n",
    "#data_by_HSA['HSA_ID'] = data_by_HSA['health_service_area_number'].astype(str) + '' + data_by_HSA['health_service_area'].apply(lambda x: x.split()[0])\n",
    "data_by_HSA.rename(columns={'health_service_area_number': 'HSA_ID'}, inplace=True)\n",
    "\n",
    "data_by_HSA['beds_over_15_100k'] = (data_by_HSA['beds_weekly'] > 15)*1\n",
    "\n",
    "# remove HSAs that have missing data in specific columns\n",
    "\n",
    "data_by_HSA = data_by_HSA.dropna(subset=['admits_weekly', 'cases_weekly', 'perc_covid'])\n",
    "for i, week in enumerate(data_by_HSA['date'].unique()):\n",
    "    data_by_HSA.loc[data_by_HSA['date'] == week, 'week'] = i\n",
    "    \n",
    "data_by_HSA_cases = data_by_HSA[['HSA_ID', 'week', 'cases_weekly']]\n",
    "data_by_HSA_cases = data_by_HSA_cases.pivot_table(index= 'week', columns='HSA_ID', values='cases_weekly')\n",
    "\n",
    "data_by_HSA_admissions = data_by_HSA[['HSA_ID', 'week', 'admits_weekly']]\n",
    "data_by_HSA_admissions = data_by_HSA_admissions.pivot_table(index= 'week', columns='HSA_ID', values='admits_weekly')\n",
    "\n",
    "data_by_HSA_perc_covid = data_by_HSA[['HSA_ID', 'week', 'perc_covid']]\n",
    "data_by_HSA_perc_covid = data_by_HSA_perc_covid.pivot_table(index= 'week', columns='HSA_ID', values='perc_covid')\n",
    "\n",
    "\n",
    "\n",
    "data_data_cases_admits_weekly = pd.merge(data_by_HSA_cases, data_by_HSA_admissions, on='week')\n",
    "new_column_names = [col.replace('_x', '_cases') for col in data_data_cases_admits_weekly.columns]\n",
    "data_data_cases_admits_weekly.rename(columns=dict(zip(data_data_cases_admits_weekly.columns, new_column_names)), inplace=True)\n",
    "new_column_names = [col.replace('_y', '_admits') for col in data_data_cases_admits_weekly.columns]\n",
    "data_data_cases_admits_weekly.rename(columns=dict(zip(data_data_cases_admits_weekly.columns, new_column_names)), inplace=True)\n",
    "\n",
    "old_column_names = data_by_HSA_perc_covid.columns\n",
    "new_column_names = [str(col) + '_percent_beds_covid' for col in old_column_names]\n",
    "new_column_names = dict(zip(old_column_names, new_column_names))\n",
    "data_by_HSA_perc_covid.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "data_by_HSA_cases_admits_perc = pd.merge(data_data_cases_admits_weekly, data_by_HSA_perc_covid, on='week')\n",
    "data_by_HSA_cases_admits_perc = data_by_HSA_cases_admits_perc.reset_index()\n",
    "#data_by_HSA_cases_admits_perc.columns = data_by_HSA_cases_admits_perc.columns.str.replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['HSA_ID']\n",
    "categories_for_subsetting = ['cases', 'admits', 'percent_beds_covid']\n",
    "\n",
    "for week in range(1, len(data_by_HSA_cases_admits_perc) + 1):\n",
    "    week = num2words(week)\n",
    "    for category in categories_for_subsetting:\n",
    "        column_name = f'week_{week}_{category}'\n",
    "        column_names.append(column_name)\n",
    "\n",
    "all_HSA_ID_weekly_data = pd.DataFrame(index=range(51), columns=column_names)\n",
    "\n",
    "x = 0\n",
    "\n",
    "for HSA in data_by_HSA['HSA_ID'].unique():\n",
    "    matching_indices = [i for i, HSA_col in enumerate(data_by_HSA_cases_admits_perc) if HSA_col == HSA]\n",
    "    all_HSA_ID_weekly_data.loc[x, 'HSA_ID'] = HSA\n",
    "    columns_to_subset =  [f'{HSA}_{category}' for category in categories_for_subsetting]\n",
    " \n",
    "    j = 1\n",
    "    try:\n",
    "        for row in range(len(data_by_HSA_cases_admits_perc.loc[:, columns_to_subset])):\n",
    "            all_HSA_ID_weekly_data.iloc[x, j:j + len(categories_for_subsetting)] = data_by_HSA_cases_admits_perc.loc[row,columns_to_subset]\n",
    "            j += 3\n",
    "    except:\n",
    "        pass\n",
    "    x += 1\n",
    "\n",
    "\n",
    "\n",
    "weights_df = data_by_HSA[data_by_HSA['HSA_ID'].isin(all_HSA_ID_weekly_data['HSA_ID'])][['HSA_ID','weight_alt']]\n",
    "weights_df = weights_df.rename(columns = {'HSA_ID': 'HSA_ID', 'weight_alt':'weight'})\n",
    "weights_df = weights_df.drop_duplicates()\n",
    "weights_df['weight'].unique()\n",
    "all_HSA_ID_weekly_data = all_HSA_ID_weekly_data.join(weights_df['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('788_cases', '788_admits', '788_percent_beds_covid')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, weights, missing_data_train_HSA = prep_training_test_data_period(all_HSA_ID_weekly_data, [3,2], no_weeks = range(1, int(123*2/3) + 1), weeks_in_futre = 3, if_train = False, geography = 'HSA_ID', weight_col = 'weight')\n",
    "\n",
    "X_test, y_test, weights_test, missing_data_test_HSA = prep_training_test_data_period(all_HSA_ID_weekly_data, [3,2], no_weeks = range(int(123*2/3) + 1, 123), weeks_in_futre = 3, if_train = False, geography = 'HSA_ID',  weight_col = 'weight')\n",
    "weights = weights[0].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_covid_outcome(new_cases_per_100k, new_admits_per_100k, percent_beds_100k):\n",
    "\n",
    "    ## here we're saying that High = >15 per 100k\n",
    "    if new_cases_per_100k < 200:\n",
    "        if (new_admits_per_100k >= 10) | (percent_beds_100k > 10):\n",
    "            if (new_admits_per_100k >= 20) | (percent_beds_100k >= 15):\n",
    "                #return \"High\"\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "                #return \"Medium\"\n",
    "        else:\n",
    "                return 0\n",
    "            #return \"Low\"\n",
    "    elif 200 <= new_cases_per_100k:\n",
    "        if (new_admits_per_100k >= 10) | (percent_beds_100k > 10):\n",
    "            if (new_admits_per_100k >= 10) | (percent_beds_100k >= 10):\n",
    "                #return \"High\"\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "                #return \"Medium\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a list called 'categories_for_subsetting' that contains the categories you want to subset\n",
    "all_HSA_ID_weekly_data = pd.DataFrame(index=data_by_HSA_cases_admits_perc['week'])\n",
    "\n",
    "for HSA in data_by_HSA['HSA_ID'].unique():\n",
    "    matching_indices = [i for i, HSA_col in enumerate(data_by_HSA_cases_admits_perc.columns) if HSA_col == HSA]\n",
    "    columns_to_subset = [f'{HSA}_{category}' for category in categories_for_subsetting]\n",
    "    j = 0\n",
    "    try:\n",
    "        for idx, row in data_by_HSA_cases_admits_perc.iterrows():\n",
    "            # Pass each column from 'columns_to_subset' into the 'determine_covid_outcome' function\n",
    "            outcome = determine_covid_outcome(row[columns_to_subset[0]], row[columns_to_subset[1]], row[columns_to_subset[2]])\n",
    "            all_HSA_ID_weekly_data.loc[idx, f'{HSA}_outcome'] = outcome\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred for HSA {HSA}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_hospital_capacity = data_by_HSA.filter(like=\"beds_over_15_100k\")\n",
    "\n",
    "# Create a new DataFrame with the subset columns\n",
    "actual_hospital_capacity = pd.DataFrame(actual_hospital_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beds_over_15_100k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103709</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103710</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103711</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103712</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103713</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103714 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        beds_over_15_100k\n",
       "0                       0\n",
       "1                       0\n",
       "2                       0\n",
       "3                       0\n",
       "4                       0\n",
       "...                   ...\n",
       "103709                  0\n",
       "103710                  0\n",
       "103711                  0\n",
       "103712                  0\n",
       "103713                  0\n",
       "\n",
       "[103714 rows x 1 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_hospital_capacity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COVID_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a72d884e172d11118bfdfb578e108fb6b63a679c74d3d7d2f9d493a9a72737c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
