{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing to get cases, deaths, hospitalizations, change in hospitalizations, and admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta \n",
    "\n",
    "\n",
    "hfont = {'fontname':'Helvetica'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_dates(dataset, columns, date_column, start_date, number_weeks, is_weeks):\n",
    "    \"\"\"\n",
    "    Takes data frame with daily or weekly data and subsets rows between two dates. \n",
    "    \"\"\"\n",
    "\n",
    "    dataset = dataset[columns]\n",
    "\n",
    "    dataset = dataset.sort_values(date_column).reset_index(drop=True)\n",
    "\n",
    "    dataset['index'] = range(len(dataset[date_column]))\n",
    "    start_date_row = dataset[dataset[date_column] == start_date]['index'].min()\n",
    "\n",
    "    start_date_index_dates = int(np.where(dataset[date_column].unique() == start_date)[0])\n",
    "    if is_weeks: \n",
    "        six_month_date_index_dates = dataset[date_column].unique()[number_weeks + start_date_index_dates]\n",
    "    else:\n",
    "            six_month_date_index_dates = dataset[date_column].unique()[(number_weeks * 7) + start_date_index_dates]\n",
    "\n",
    "    end_date_row = dataset[dataset[date_column] == six_month_date_index_dates]['index'].max()\n",
    "    dataset = dataset[start_date_row:end_date_row]\n",
    "    return(dataset)\n",
    "\n",
    "def weekly_date_range(dataset):\n",
    "    dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "\n",
    "    start_date = dataset['date'].min()\n",
    "    end_date = dataset['date'].max()\n",
    "    dates = []\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        dates.append(current_date)\n",
    "        current_date += timedelta(days=7)\n",
    "    return dates\n",
    "\n",
    "def convert_state_name_to_abbreviation(state_name):\n",
    "    state_name = state_name.lower().strip()\n",
    "\n",
    "    state_abbreviations = {\n",
    "        'alabama': 'AL',\n",
    "        'alaska': 'AK',\n",
    "        'arizona': 'AZ',\n",
    "        'arkansas': 'AR',\n",
    "        'california': 'CA',\n",
    "        'colorado': 'CO',\n",
    "        'connecticut': 'CT',\n",
    "        'delaware': 'DE',\n",
    "        'florida': 'FL',\n",
    "        'georgia': 'GA',\n",
    "        'hawaii': 'HI',\n",
    "        'idaho': 'ID',\n",
    "        'illinois': 'IL',\n",
    "        'indiana': 'IN',\n",
    "        'iowa': 'IA',\n",
    "        'kansas': 'KS',\n",
    "        'kentucky': 'KY',\n",
    "        'louisiana': 'LA',\n",
    "        'maine': 'ME',\n",
    "        'maryland': 'MD',\n",
    "        'massachusetts': 'MA',\n",
    "        'michigan': 'MI',\n",
    "        'minnesota': 'MN',\n",
    "        'mississippi': 'MS',\n",
    "        'missouri': 'MO',\n",
    "        'montana': 'MT',\n",
    "        'nebraska': 'NE',\n",
    "        'nevada': 'NV',\n",
    "        'new hampshire': 'NH',\n",
    "        'new jersey': 'NJ',\n",
    "        'new mexico': 'NM',\n",
    "        'new york': 'NY',\n",
    "        'north carolina': 'NC',\n",
    "        'north dakota': 'ND',\n",
    "        'ohio': 'OH',\n",
    "        'oklahoma': 'OK',\n",
    "        'oregon': 'OR',\n",
    "        'pennsylvania': 'PA',\n",
    "        'rhode island': 'RI',\n",
    "        'south carolina': 'SC',\n",
    "        'south dakota': 'SD',\n",
    "        'tennessee': 'TN',\n",
    "        'texas': 'TX',\n",
    "        'utah': 'UT',\n",
    "        'vermont': 'VT',\n",
    "        'virginia': 'VA',\n",
    "        'washington': 'WA',\n",
    "        'washington dc': 'DC',\n",
    "        'west virginia': 'WV',\n",
    "        'wisconsin': 'WI',\n",
    "        'wyoming': 'WY',\n",
    "        'district of columbia': 'DC'\n",
    "    }\n",
    "\n",
    "    return state_abbreviations.get(state_name, None)\n",
    "\n",
    "def convert_daily_weekly(dataset, column_name, date_column, geography_column, hospital_cases):\n",
    "    dataset[date_column] = pd.to_datetime(dataset[date_column])\n",
    "\n",
    "    dates = weekly_date_range(dataset)\n",
    "\n",
    "    num_rows = len(dates)*len(dataset[geography_column].unique())\n",
    "    weekly_dataframe = pd.DataFrame(columns=dataset.columns[range(len(column_name))], index=range(num_rows))\n",
    "    weekly_dataframe.columns = column_name\n",
    "\n",
    "    x = -1\n",
    "    for geography in dataset[geography_column].unique():\n",
    "        state_data = dataset[dataset[geography_column] == geography].reset_index()\n",
    "        for date in dates: \n",
    "            x += 1\n",
    "            weekly_dataframe.iloc[x,1] = geography\n",
    "            weekly_dataframe.iloc[x,0] = date\n",
    "            if (state_data.loc[0, 'date'] < date) | (state_data.loc[0, 'date'] <= (date - timedelta(days=7))):\n",
    "                end_index = state_data.loc[state_data['date'] < date, 'date'].idxmax()\n",
    "                start_index = state_data.loc[(state_data['date'] <= (date - timedelta(days=7))), 'date'].idxmax()\n",
    "                selected_rows = state_data.iloc[start_index:end_index, len(state_data.columns)-1]\n",
    "                selected_rows = state_data.iloc[start_index:end_index, len(state_data.columns)-1]\n",
    "\n",
    "                if hospital_cases:\n",
    "                    average_cases = selected_rows.mean()\n",
    "                    weekly_dataframe.iloc[x,2] = average_cases\n",
    "\n",
    "                else:\n",
    "                    cumulative_sum = selected_rows.sum()\n",
    "                    weekly_dataframe.iloc[x,2] = cumulative_sum\n",
    "                \n",
    "\n",
    "    return(weekly_dataframe)\n",
    "\n",
    "def per_100k(dataset, date_column, value_column, geography_column, categories_to_create, populations, hospitalizations, threshold ):\n",
    "    geography_names = dataset[geography_column].unique()\n",
    "    dataset = dataset.pivot_table(index= date_column, columns=geography_column, values=value_column) # gets rid of week - 2 \n",
    "    dataset = dataset.reset_index()\n",
    "    for geography in geography_names: \n",
    "        for column in categories_to_create:\n",
    "            col_name_rate = geography + column\n",
    "            dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
    "            col_name_delta = geography  + column + '_delta_100k'\n",
    "            j = 0\n",
    "            for row in range(len(dataset[geography]) - 1): ## need to use j as an index as row is a datetime object \n",
    "                    if(j != 0):\n",
    "                        dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
    "                    j+=1\n",
    "        if hospitalizations: \n",
    "                col_name_threshold = geography + '_over_' + str(threshold) + '_100k'\n",
    "                dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
    "        dataset = dataset.drop(geography, axis=1)\n",
    "    # remove first row with week - 1\n",
    "    dataset = dataset[dataset[date_column] != dataset.loc[0,date_column] ] ## remove week 0 \n",
    "\n",
    "    return(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_by_state_April_2020 = {\n",
    "    'Alabama': 5024279,\n",
    "    'Alaska': 733391,\n",
    "    'Arizona': 7151502,\n",
    "    'Arkansas': 3011524,\n",
    "    'California': 39538223,\n",
    "    'Colorado': 5773714,\n",
    "    'Connecticut': 3605944,\n",
    "    'Delaware': 989948,\n",
    "    'District of Columbia': 689545,\n",
    "    'Florida': 21538187,\n",
    "    'Georgia': 10711908,\n",
    "    'Hawaii': 1455271,\n",
    "    'Idaho': 1839106,\n",
    "    'Illinois': 12812508,\n",
    "    'Indiana': 6785528,\n",
    "    'Iowa': 3190369,\n",
    "    'Kansas': 2937880,\n",
    "    'Kentucky': 4505836,\n",
    "    'Louisiana': 4657757,\n",
    "    'Maine': 1362359,\n",
    "    'Maryland': 6177224,\n",
    "    'Massachusetts': 7029917,\n",
    "    'Michigan': 10077331,\n",
    "    'Minnesota': 5706494,\n",
    "    'Mississippi': 2961279,\n",
    "    'Missouri': 6154913,\n",
    "    'Montana': 1084225,\n",
    "    'Nebraska': 1961504,\n",
    "    'Nevada': 3104614,\n",
    "    'New Hampshire': 1377529,\n",
    "    'New Jersey': 9288994,\n",
    "    'New Mexico': 2117522,\n",
    "    'New York': 20201249,\n",
    "    'North Carolina': 10439388,\n",
    "    'North Dakota': 779094,\n",
    "    'Ohio': 11799448,\n",
    "    'Oklahoma': 3959353,\n",
    "    'Oregon': 4237256,\n",
    "    'Pennsylvania': 13002700,\n",
    "    'Rhode Island': 1097379,\n",
    "    'South Carolina': 5118425,\n",
    "    'South Dakota': 886667,\n",
    "    'Tennessee': 6910840,\n",
    "    'Texas': 29145505,\n",
    "    'Utah': 3271616,\n",
    "    'Vermont': 643077,\n",
    "    'Virginia': 8631393,\n",
    "    'Washington': 7705281,\n",
    "    'West Virginia': 1793716,\n",
    "    'Wisconsin': 5893718,\n",
    "    'Wyoming': 576851\n",
    "}\n",
    "population_by_state_April_2020_abb = {}\n",
    "states_to_remove = []\n",
    "\n",
    "for state_name in population_by_state_April_2020:\n",
    "    state_abbreviation = convert_state_name_to_abbreviation(state_name)\n",
    "    if state_abbreviation:\n",
    "        population_by_state_April_2020_abb[state_abbreviation] = population_by_state_April_2020[state_name]\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data from https://healthdata.gov/Hospital/COVID-19-Reported-Patient-Impact-and-Hospital-Capa/g62h-syeh\n",
    "- Hospitalizations\n",
    "- Use total adult patients with covid and total pediatric patients with covid for total covid cases\n",
    "- Try and plot to match up \n",
    "- Also has previous day admissions - can try and get cumulative over 7 days to match with weekly admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "State_hospitalizations = pd.read_csv(\"/Users/rem76/Documents/COVID_projections/COVID-19_Reported_Patient_Impact_and_Hospital_Capacity_by_State_Timeseries__RAW_.csv\")\n",
    "# Remove non-states (except DC)\n",
    "\n",
    "State_hospitalizations = State_hospitalizations[(State_hospitalizations['state'] != 'PR') & (State_hospitalizations['state'] != 'VI') & (State_hospitalizations['state'] != 'GU')& (State_hospitalizations['state'] != 'AS')& (State_hospitalizations['state'] != 'None')]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall hospital numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  start_date_index_dates = int(np.where(dataset[date_column].unique() == start_date)[0])\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_threshold] = (dataset[col_name_rate] > threshold)*1\n"
     ]
    }
   ],
   "source": [
    "State_hospitalizations_total_hospitalizations = data_prep_dates(State_hospitalizations, columns = ['date', 'state', 'percent_of_inpatients_with_covid_numerator'], date_column = 'date', start_date = '2020/06/21', number_weeks = 26, is_weeks = False)\n",
    "## Start date is two week before the actual date of interest, allows us to calculate the weekly and change in weekly rate for our \"actual\" first week \n",
    "State_hospitalizations_total_hospitalizations.rename(columns={'percent_of_inpatients_with_covid_numerator': 'case_numbers'}, inplace=True)\n",
    "State_hospitalizations_total_hospitalizations = State_hospitalizations_total_hospitalizations.drop('index', axis=1)\n",
    "State_hospitalizations_total_hospitalizations_weekly = convert_daily_weekly(State_hospitalizations_total_hospitalizations, ['date', 'state', 'case_numbers'], date_column = 'date', geography_column = 'state', hospital_cases = True)\n",
    "State_hospitalizations_total_hospitalizations_weekly = per_100k(State_hospitalizations_total_hospitalizations_weekly, date_column = 'date', value_column = 'case_numbers', geography_column = 'state', categories_to_create = ['_hospitalizations'],populations =  population_by_state_April_2020_abb, hospitalizations = True, threshold = 15 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New admission numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  start_date_index_dates = int(np.where(dataset[date_column].unique() == start_date)[0])\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset[col_name_rate] = dataset[geography]/populations[geography] * 100000\n",
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataset.loc[j, col_name_delta] = dataset.loc[j, col_name_rate] - dataset.loc[j - 1, col_name_rate]\n"
     ]
    }
   ],
   "source": [
    "State_hospitalizations_new_admissions_daily = State_hospitalizations[['date', 'state','previous_day_admission_adult_covid_confirmed','previous_day_admission_adult_covid_suspected','previous_day_admission_pediatric_covid_confirmed','previous_day_admission_pediatric_covid_suspected']]\n",
    "State_hospitalizations['admissions'] = State_hospitalizations_new_admissions_daily[['previous_day_admission_adult_covid_confirmed','previous_day_admission_adult_covid_suspected','previous_day_admission_pediatric_covid_confirmed','previous_day_admission_pediatric_covid_suspected']].sum(axis=1)\n",
    "State_hospitalizations_new_admissions_daily = data_prep_dates(State_hospitalizations, columns = ['date', 'state', 'admissions'], date_column = 'date', start_date = '2020/06/21', number_weeks = 26, is_weeks = False)\n",
    "State_hospitalizations_new_admissions_weekly = convert_daily_weekly(State_hospitalizations_new_admissions_daily, ['date', 'state', 'admissions'], date_column = 'date', geography_column = 'state', hospital_cases = False)\n",
    "State_hospitalizations_new_admissions_weekly = per_100k(State_hospitalizations_new_admissions_weekly, date_column = 'date', value_column = 'admissions', geography_column = 'state', categories_to_create = ['_admissions'],populations =  population_by_state_April_2020_abb, hospitalizations = False, threshold = 15 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NY TIMES\n",
    "- Cases and deaths by state \n",
    "- Already per 100k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cases and deaths\n",
    "Note, here, the cases_avg is for the previous 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/1717743493.py:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  start_date_index_dates = int(np.where(dataset[date_column].unique() == start_date)[0])\n"
     ]
    }
   ],
   "source": [
    "State_data_cases_death = pd.read_csv(\"/Users/rem76/Documents/COVID_projections/us-states.csv\")\n",
    "State_data_cases_death = State_data_cases_death[(State_data_cases_death['state'] != 'Puerto Rico') & (State_data_cases_death['state'] != 'Virgin Islands') & (State_data_cases_death['state'] != 'Guam')& (State_data_cases_death['state'] != 'American Samoa')& (State_data_cases_death['state'] != 'Northern Mariana Islands')]\n",
    "\n",
    "x = 0\n",
    "for state_name in State_data_cases_death['state']:\n",
    "    State_data_cases_death.loc[x,'state']  = convert_state_name_to_abbreviation(state_name)\n",
    "    x += 1\n",
    "State_data_cases_death_weekly = data_prep_dates(State_data_cases_death, columns = ['date', 'state', 'cases_avg_per_100k', 'deaths_avg_per_100k'], date_column = 'date', start_date = '2020-06-21', number_weeks = 26, is_weeks = False)\n",
    "State_data_cases_death_weekly = State_data_cases_death_weekly[~State_data_cases_death_weekly['state'].isna()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_daily_weekly_NY_times(dataset, column_name, date_column, geography_column):\n",
    "    dataset[date_column] = pd.to_datetime(dataset[date_column])\n",
    "\n",
    "    dates = weekly_date_range(dataset)\n",
    "    \n",
    "    num_rows = len(dates)*len(dataset[geography_column].unique())\n",
    "    weekly_dataframe = pd.DataFrame(columns=dataset.columns[range(len(column_name))], index=range(num_rows))\n",
    "    weekly_dataframe.columns = column_name\n",
    "\n",
    "    x = -1\n",
    "    for geography in dataset[geography_column].unique():\n",
    "        state_data = dataset[dataset[geography_column] == geography].reset_index()\n",
    "        for date in dates: \n",
    "            x += 1\n",
    "            weekly_dataframe.iloc[x,1] = geography\n",
    "            weekly_dataframe.iloc[x,0] = date\n",
    "            weekly_dataframe.iloc[x, 2] = state_data.loc[state_data['date'] == date, column_name[2]]\n",
    "\n",
    "    return(weekly_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = State_data_cases_death_weekly\n",
    "column_name = ['date', 'state', 'death_avg_per_100k']\n",
    "date_column = 'date'\n",
    "geography_column = 'state'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>date</th>\n",
       "      <th>state</th>\n",
       "      <th>cases_avg_per_100k</th>\n",
       "      <th>deaths_avg_per_100k</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5682</td>\n",
       "      <td>2020-06-21</td>\n",
       "      <td>OK</td>\n",
       "      <td>5.51</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0       date state  cases_avg_per_100k  deaths_avg_per_100k  index\n",
       "0     5682 2020-06-21    OK                5.51                 0.25   5682"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_data = state_data.loc[state_data['date'] == date]\n",
    "date_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'death_avg_per_100k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/COVID_forecasting/lib/python3.11/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/COVID_forecasting/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/COVID_forecasting/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'death_avg_per_100k'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[489], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m weekly_dataframe\u001b[39m.\u001b[39miloc[x,\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m geography\n\u001b[1;32m     14\u001b[0m weekly_dataframe\u001b[39m.\u001b[39miloc[x,\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m date\n\u001b[0;32m---> 15\u001b[0m weekly_dataframe\u001b[39m.\u001b[39miloc[x, \u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m state_data\u001b[39m.\u001b[39mloc[state_data[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m date, column_name[\u001b[39m2\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/envs/COVID_forecasting/lib/python3.11/site-packages/pandas/core/indexing.py:1097\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1096\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[0;32m-> 1097\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_tuple(key)\n\u001b[1;32m   1098\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/COVID_forecasting/lib/python3.11/site-packages/pandas/core/indexing.py:1280\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[39mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1279\u001b[0m     tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_ellipsis(tup)\n\u001b[0;32m-> 1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_lowerdim(tup)\n\u001b[1;32m   1282\u001b[0m \u001b[39m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[0;32m~/miniconda3/envs/COVID_forecasting/lib/python3.11/site-packages/pandas/core/indexing.py:1000\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[39mfor\u001b[39;00m i, key \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tup):\n\u001b[1;32m    997\u001b[0m     \u001b[39mif\u001b[39;00m is_label_like(key):\n\u001b[1;32m    998\u001b[0m         \u001b[39m# We don't need to check for tuples here because those are\u001b[39;00m\n\u001b[1;32m    999\u001b[0m         \u001b[39m#  caught by the _is_nested_tuple_indexer check above.\u001b[39;00m\n\u001b[0;32m-> 1000\u001b[0m         section \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_axis(key, axis\u001b[39m=\u001b[39mi)\n\u001b[1;32m   1002\u001b[0m         \u001b[39m# We should never have a scalar section here, because\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         \u001b[39m#  _getitem_lowerdim is only called after a check for\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m         \u001b[39m#  is_scalar_access, which that would be.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m         \u001b[39mif\u001b[39;00m section\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim:\n\u001b[1;32m   1006\u001b[0m             \u001b[39m# we're in the middle of slicing through a MultiIndex\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m             \u001b[39m# revise the key wrt to `section` by inserting an _NS\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/COVID_forecasting/lib/python3.11/site-packages/pandas/core/indexing.py:1343\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[39m# fall thru to straight lookup\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m-> 1343\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_label(key, axis\u001b[39m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/COVID_forecasting/lib/python3.11/site-packages/pandas/core/indexing.py:1293\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_label\u001b[39m(\u001b[39mself\u001b[39m, label, axis: AxisInt):\n\u001b[1;32m   1292\u001b[0m     \u001b[39m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[0;32m-> 1293\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39mxs(label, axis\u001b[39m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/COVID_forecasting/lib/python3.11/site-packages/pandas/core/generic.py:4082\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   4080\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   4081\u001b[0m     \u001b[39mif\u001b[39;00m drop_level:\n\u001b[0;32m-> 4082\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[1;32m   4083\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   4084\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/COVID_forecasting/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/COVID_forecasting/lib/python3.11/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'death_avg_per_100k'"
     ]
    }
   ],
   "source": [
    "    dates = weekly_date_range(dataset)\n",
    "    \n",
    "    num_rows = len(dates)*len(dataset[geography_column].unique())\n",
    "    weekly_dataframe = pd.DataFrame(columns=dataset.columns[range(len(column_name))], index=range(num_rows))\n",
    "    weekly_dataframe.columns = column_name\n",
    "    dates = weekly_date_range(dataset)\n",
    "    weekly_dataframe.columns = column_name\n",
    "    x = -1\n",
    "    for geography in dataset[geography_column].unique():\n",
    "        state_data = dataset[dataset[geography_column] == geography].reset_index()\n",
    "        for date in dates: \n",
    "            x += 1\n",
    "            weekly_dataframe.iloc[x,1] = geography\n",
    "            weekly_dataframe.iloc[x,0] = date\n",
    "            weekly_dataframe.iloc[x, 2] = state_data.loc[state_data['date'] == date, column_name[2]]\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "State_data_cases_weekly = convert_daily_weekly_NY_times(State_data_cases_death_weekly, ['date', 'state', 'cases_avg_per_100k'], 'date', 'state',  hospital_cases = False)\n",
    "State_data_cases_weekly = State_data_cases_weekly[State_data_cases_weekly['date'] != State_data_cases_weekly.loc[0,'date'] ] ## remove week 0 \n",
    "State_data_cases_weekly = State_data_cases_weekly.pivot_table(index= 'date', columns='state', values='cases_avg_per_100k') # gets rid of week - 2 \n",
    "\n",
    "State_data_death_weekly = convert_daily_weekly_NY_times(State_data_cases_death_weekly, ['date', 'state', 'deaths_avg_per_100k'], 'date', 'state',  hospital_cases = False)\n",
    "State_data_death_weekly = State_data_death_weekly[State_data_death_weekly['date'] != State_data_death_weekly.loc[0,'date'] ] ## remove week 0 \n",
    "State_data_death_weekly = State_data_death_weekly.pivot_table(index= 'date', columns='state', values='deaths_avg_per_100k') # gets rid of week - 2 \n",
    "\n",
    "#State_data_cases_weekly['death_avg_per_100k'] = State_data_death_weekly['death_avg_per_100k']\n",
    "State_data_cases_death_weekly = pd.merge(State_data_cases_weekly, State_data_death_weekly, on='date')\n",
    "new_column_names = [col.replace('x', 'cases') for col in State_data_cases_death_weekly.columns]\n",
    "State_data_cases_death_weekly.rename(columns=dict(zip(State_data_cases_death_weekly.columns, new_column_names)), inplace=True)\n",
    "\n",
    "new_column_names = [col.replace('y', 'deaths') for col in State_data_cases_death_weekly.columns]\n",
    "State_data_cases_death_weekly.rename(columns=dict(zip(State_data_cases_death_weekly.columns, new_column_names)), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_46992/3270642030.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  all_state_weekly_data = pd.merge(State_data_cases_death_weekly, State_hospitalizations_total_hospitalizations_weekly, on='date')\n"
     ]
    }
   ],
   "source": [
    "all_state_weekly_data = pd.merge(State_data_cases_death_weekly, State_hospitalizations_total_hospitalizations_weekly, on='date')\n",
    "all_state_weekly_data = pd.merge(all_state_weekly_data, State_hospitalizations_new_admissions_weekly, on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_state_weekly_data.to_csv(\"all_state_weekly_data_cases_deaths_admissions_hospital.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COVID_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a72d884e172d11118bfdfb578e108fb6b63a679c74d3d7d2f9d493a9a72737c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
