{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from num2words import num2words\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, KFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score\n",
    "import word2number\n",
    "from word2number import w2n\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import random\n",
    "#from Functions import prep_training_test_data_period, prep_training_test_data, calculate_ppv_npv\n",
    "hfont = {'fontname':'Helvetica'}\n",
    "palette = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854', '#e5c494']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_training_test_data_period(\n",
    "    data, no_weeks, weeks_in_future, geography, weight_col, keep_output\n",
    "):\n",
    "    ## Get the weeks for the x and y datasets\n",
    "    x_weeks = []\n",
    "    y_weeks = []\n",
    "    y_weeks_to_check = []  # check these weeks to see if any of them are equal to 1\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_future\n",
    "        x_weeks.append(\"_\" + num2words(week) + \"_\")\n",
    "        for week_y in range(week + 1, test_week + 1):\n",
    "            y_weeks_to_check.append(\"_\" + num2words(week_y) + \"_\")\n",
    "        y_weeks.append(\"_\" + num2words(test_week) + \"_\")\n",
    "\n",
    "    ## Divide up the test/train split\n",
    "    # if is_geographic:\n",
    "    # Calculate the index to start slicing from\n",
    "    #    start_index = len(data['county']) // proportion[0] * proportion[1]\n",
    "    # Divide up the dataset based on this proportion\n",
    "    #    first_two_thirds = data['county'][:start_index]\n",
    "    #    last_third = data['county'][start_index:]\n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all = pd.DataFrame()\n",
    "    missing_data = []\n",
    "    ## Now get the training data\n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "        y_week = y_weeks[k]\n",
    "        k += 1\n",
    "\n",
    "        weeks_x = [col for col in data.columns if x_week in col]\n",
    "        columns_x = [geography] + weeks_x + [weight_col]\n",
    "        data_x = data[columns_x]\n",
    "\n",
    "        weeks_y = [col for col in data.columns if y_week in col]\n",
    "        columns_y = [geography] + weeks_y\n",
    "        data_y = data[columns_y]\n",
    "        ### now add the final column to the y data that has it so that it's if any week in the trhee week perdiod exceeded 15\n",
    "        train_week = w2n.word_to_num(x_week.replace(\"_\", \"\"))\n",
    "        target_week = w2n.word_to_num(y_week.replace(\"_\", \"\"))\n",
    "        y_weeks_to_check = []\n",
    "        for week_to_check in range(train_week + 1, target_week + 1):\n",
    "            y_weeks_to_check.append(\"_\" + num2words(week_to_check) + \"_\")\n",
    "\n",
    "        y_weeks_to_check = [week + \"beds_over_15_100k\" for week in y_weeks_to_check]\n",
    "        columns_to_check = [\n",
    "            col for col in data.columns if any(week in col for week in y_weeks_to_check)\n",
    "        ]\n",
    "        y_over_in_period = data[columns_to_check].apply(max, axis=1)\n",
    "        data_y = pd.concat([data_y, y_over_in_period], axis=1)\n",
    "        # ensure they have the same amount of data\n",
    "        # remove rows in test_data1 with NA in test_data2\n",
    "        data_x = data_x.dropna()\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        # remove rows in test_data2 with NA in test_data1\n",
    "        data_y = data_y.dropna()\n",
    "        data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "        missing_data.append(\n",
    "            (\n",
    "                (len(data[geography].unique()) - data_x_no_HSA)\n",
    "                / len(data[geography].unique())\n",
    "            )\n",
    "            * 100\n",
    "        )\n",
    "        # get weights\n",
    "        # weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "        X_week = data_x.iloc[:, 1 : len(columns_x)]  # take away y, leave weights for mo\n",
    "        y_week = data_y.iloc[:, -1]\n",
    "\n",
    "        y_week = y_week.astype(int)\n",
    "        weights = X_week.iloc[:, -1]\n",
    "        if keep_output:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 1\n",
    "            ]  # remove the weights and leave \"target\" for that week\n",
    "\n",
    "            # rename columns for concatenation\n",
    "            X_week.columns = range(1, len(data_x.columns) - 1)\n",
    "        else:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 2\n",
    "            ]  # remove the weights and  \"target\" for that week\n",
    "\n",
    "            X_week.columns = range(\n",
    "                1, len(data_x.columns) - 2\n",
    "            )  # remove the weights and  \"target\" for that week\n",
    "\n",
    "        y_week.columns = range(1, len(data_y.columns) - 2)\n",
    "        X_data = pd.concat([X_data, X_week])\n",
    "        y_data = pd.concat([y_data, y_week])\n",
    "\n",
    "        weights_all = pd.concat([weights_all, weights])\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return (X_data, y_data, weights_all, missing_data)\n",
    "\n",
    "def prep_training_test_data(\n",
    "    data, no_weeks, weeks_in_future, geography, weight_col, keep_output\n",
    "):\n",
    "    ## Get the weeks for the x and y datasets\n",
    "    x_weeks = []\n",
    "    y_weeks = []\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_future\n",
    "        x_weeks.append(\"_\" + num2words(week) + \"_\")\n",
    "        y_weeks.append(\"_\" + num2words(test_week) + \"_\")\n",
    "\n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all = pd.DataFrame()\n",
    "    missing_data = []\n",
    "    ## Now get the training data\n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "        y_week = y_weeks[k]\n",
    "        k += 1\n",
    "        weeks_x = [col for col in data.columns if x_week in col]\n",
    "        columns_x = [geography] + weeks_x + [weight_col]\n",
    "        data_x = data[columns_x]\n",
    "\n",
    "        weeks_y = [col for col in data.columns if y_week in col]\n",
    "        columns_y = [geography] + weeks_y\n",
    "        data_y = data[columns_y]\n",
    "        # ensure they have the same amount of data\n",
    "        # remove rows in test_data1 with NA in test_data2\n",
    "        data_x = data_x.dropna()\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        # remove rows in test_data2 with NA in test_data1\n",
    "        data_y = data_y.dropna()\n",
    "        data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "        missing_data.append(\n",
    "            (\n",
    "                (len(data[geography].unique()) - data_x_no_HSA)\n",
    "                / len(data[geography].unique())\n",
    "            )\n",
    "            * 100\n",
    "        )\n",
    "        # get weights\n",
    "        # weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "        X_week = data_x.iloc[:, 1 : len(columns_x)]  # take away y, leave weights for mo\n",
    "        y_week = data_y.iloc[:, -1]\n",
    "\n",
    "        y_week = y_week.astype(int)\n",
    "        weights = X_week.iloc[:, -1]\n",
    "        if keep_output:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 1\n",
    "            ]  # remove the weights and leave \"target\" for that week\n",
    "\n",
    "            # rename columns for concatenation\n",
    "            X_week.columns = range(1, len(data_x.columns) - 1)\n",
    "        else:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 2\n",
    "            ]  # remove the weights and  \"target\" for that week\n",
    "\n",
    "            X_week.columns = range(\n",
    "                1, len(data_x.columns) - 2\n",
    "            )  # remove the weights and  \"target\" for that week\n",
    "\n",
    "            # rename columns for concatenation\n",
    "        y_week.columns = range(1, len(data_y.columns) - 1)\n",
    "        X_data = pd.concat([X_data, X_week])\n",
    "        y_data = pd.concat([y_data, y_week])\n",
    "\n",
    "        weights_all = pd.concat([weights_all, weights])\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return (X_data, y_data, weights_all, missing_data)\n",
    "\n",
    "\n",
    "def prep_training_test_data_shifted(data, no_weeks, weeks_in_future, geography, weight_col, keep_output):\n",
    "## Get the weeks for the x and y datasets   \n",
    "    x_weeks = []  \n",
    "    y_weeks = []\n",
    "    y_weeks_to_check = [] #check these weeks to see if any of them are equal to 1\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_future\n",
    "        x_weeks.append('_' + num2words(week) + '_')\n",
    "        for week_y in range(week+2, test_week+2):\n",
    "                y_weeks_to_check.append('_' + num2words(week_y) + '_')\n",
    "        y_weeks.append('_' + num2words(test_week) + '_')\n",
    "    \n",
    "## Divide up the test/train split\n",
    "    #if is_geographic:\n",
    "        # Calculate the index to start slicing from\n",
    "    #    start_index = len(data['county']) // proportion[0] * proportion[1]\n",
    "        # Divide up the dataset based on this proportion\n",
    "    #    first_two_thirds = data['county'][:start_index]\n",
    "    #    last_third = data['county'][start_index:]\n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all =  pd.DataFrame()\n",
    "    missing_data = []\n",
    "    ## Now get the training data \n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "            y_week = y_weeks[k]\n",
    "            k +=1\n",
    "\n",
    "            weeks_x = [col for col in data.columns if x_week in col]\n",
    "            columns_x  = [geography] + weeks_x + [weight_col]\n",
    "            data_x = data[columns_x]\n",
    "\n",
    "            weeks_y = [col for col in data.columns if y_week in col]\n",
    "            columns_y  = [geography] + weeks_y\n",
    "            data_y = data[columns_y]\n",
    "            ### now add the final column to the y data that has it so that it's if any week in the trhee week perdiod exceeded 15\n",
    "            train_week = w2n.word_to_num(x_week.replace(\"_\", \"\"))\n",
    "            target_week =  w2n.word_to_num(y_week.replace(\"_\", \"\"))\n",
    "            y_weeks_to_check = []\n",
    "            for week_to_check in range(train_week + 1, target_week + 1):\n",
    "                y_weeks_to_check.append('_' + num2words(week_to_check) + '_')\n",
    "\n",
    "            y_weeks_to_check = [week + 'beds_over_15_100k' for week in y_weeks_to_check]\n",
    "            columns_to_check = [col for col in data.columns if any(week in col for week in y_weeks_to_check)]\n",
    "            y_over_in_period = data[columns_to_check].apply(max, axis=1)\n",
    "            data_y = pd.concat([data_y, y_over_in_period], axis=1)\n",
    "            # ensure they have the same amount of data\n",
    "            #remove rows in test_data1 with NA in test_data2\n",
    "            data_x = data_x.dropna()\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            # remove rows in test_data2 with NA in test_data1\n",
    "            data_y = data_y.dropna()\n",
    "            data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "            data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "            data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "            missing_data.append(((len(data[geography].unique()) - data_x_no_HSA)/len(data[geography].unique())) * 100)\n",
    "            # get weights \n",
    "            #weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "            X_week = data_x.iloc[:, 1:len(columns_x)]  # take away y, leave weights for mo\n",
    "            y_week = data_y.iloc[:, -1] \n",
    "            \n",
    "            y_week = y_week.astype(int)\n",
    "\n",
    "            weights = X_week.iloc[:, -1] \n",
    "            if keep_output:\n",
    "                X_week = X_week.iloc[:, :len(X_week.columns)-1] # remove the weights and leave \"target\" for that week\n",
    "\n",
    "                #rename columns for concatenation \n",
    "                X_week.columns = range(1, len(data_x.columns) -1)\n",
    "            else:\n",
    "                X_week = X_week.iloc[:, :len(X_week.columns)-2] # remove the weights and  \"target\" for that week\n",
    "\n",
    "                X_week.columns = range(1, len(data_x.columns) -2)# remove the weights and  \"target\" for that week\n",
    "\n",
    "            y_week.columns = range(1, len(data_y.columns) -2)\n",
    "            X_data = pd.concat([X_data, X_week])\n",
    "            y_data = pd.concat([y_data, y_week]) \n",
    "        \n",
    "            weights_all =  pd.concat([weights_all, weights]) \n",
    "\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return(X_data, y_data, weights_all, missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_leave_geo_out(data, geography_column, geo_split, no_iterations, cv, classifier, param_grid, no_iterations_param, no_weeks_train,no_weeks_test, weeks_in_future, weight_col, keep_output, time_period):\n",
    "        best_hyperparameters_per_iter = []\n",
    "        auROC_per_iter = []\n",
    "\n",
    "        for i in range(no_iterations):\n",
    "                #subset the HSAs from the full dataset \n",
    "                geo_names = data[geography_column].unique()\n",
    "                num_names_to_select = int(geo_split * len(geo_names))\n",
    "                geos_for_sample = random.sample(list(geo_names), num_names_to_select)\n",
    "                subset_HSAs_for_train = data[data[geography_column].isin(geos_for_sample)]\n",
    "                subset_HSAs_for_test = data[~data[geography_column].isin(geos_for_sample)]\n",
    "\n",
    "                #create training and test data\n",
    "                if time_period == 'period':\n",
    "                        X_sample_train, y_sample_train, weights_train, missing_data_train_HSA = prep_training_test_data_period(subset_HSAs_for_train, no_weeks = no_weeks_train, weeks_in_future = weeks_in_future,  geography = geography_column, weight_col = weight_col,keep_output = keep_output)\n",
    "                        X_sample_test, y_sample_test, weights_test, missing_data_train_HSA = prep_training_test_data_period(subset_HSAs_for_test, no_weeks = no_weeks_test, weeks_in_future = weeks_in_future,  geography = geography_column, weight_col = weight_col,keep_output = keep_output)\n",
    "                        weights_train = weights_train[0]\n",
    "                elif time_period == 'exact':\n",
    "                        X_sample_train, y_sample_train, weights_train, missing_data_train_HSA = prep_training_test_data(subset_HSAs_for_train, no_weeks = no_weeks_train, weeks_in_future = weeks_in_future,  geography = geography_column, weight_col =weight_col,keep_output = keep_output)\n",
    "                        X_sample_test, y_sample_test, weights_test, missing_data_train_HSA = prep_training_test_data(subset_HSAs_for_test, no_weeks = no_weeks_test, weeks_in_future = weeks_in_future,  geography = geography_column, weight_col = weight_col,keep_output = keep_output)\n",
    "                        weights_train = weights_train[0]\n",
    "                elif time_period == 'shifted':\n",
    "                        X_sample_train, y_sample_train, weights_train, missing_data_train_HSA = prep_training_test_data_shifted(subset_HSAs_for_train, no_weeks = no_weeks_train, weeks_in_future = weeks_in_future,  geography = geography_column, weight_col = weight_col,keep_output = keep_output)\n",
    "                        X_sample_test, y_sample_test, weights_test, missing_data_train_HSA = prep_training_test_data_shifted(subset_HSAs_for_test, no_weeks = no_weeks_test, weeks_in_future = weeks_in_future,  geography = geography_column, weight_col = weight_col,keep_output = keep_output)\n",
    "                        weights_train = weights_train[0]\n",
    "                random_search = RandomizedSearchCV(classifier, param_grid, n_iter=no_iterations_param, cv=cv, random_state=10)\n",
    "                random_search.fit(X_sample_train, y_sample_train, sample_weight = weights_train)\n",
    "                best_params = random_search.best_params_\n",
    "\n",
    "        # Create the Decision Tree classifier with the best hyperparameters\n",
    "                model = DecisionTreeClassifier(**best_params,random_state=10, class_weight='balanced')\n",
    "                model_fit = model.fit(X_sample_train, y_sample_train, sample_weight=weights_train)\n",
    "                y_pred = model_fit.predict_proba(X_sample_test)\n",
    "                # Evaluate the accuracy of the model\n",
    "                best_hyperparameters_per_iter.append(best_params)\n",
    "                auROC_per_iter.append(roc_auc_score(y_sample_test, y_pred[:,1]))\n",
    "        \n",
    "        return best_hyperparameters_per_iter[np.argmax(np.array(auROC_per_iter))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m HSA_weekly_data_all \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates_weekly.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "HSA_weekly_data_all = pd.read_csv(\"/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates_weekly.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COVID_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a72d884e172d11118bfdfb578e108fb6b63a679c74d3d7d2f9d493a9a72737c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
