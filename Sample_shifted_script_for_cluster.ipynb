{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction week 2\n",
      "3\n",
      "4\n",
      "5\n",
      "weeks to check ['_three_', '_four_', '_five_']\n",
      "4\n",
      "5\n",
      "6\n",
      "weeks to check ['_four_', '_five_', '_six_']\n",
      "5\n",
      "6\n",
      "7\n",
      "weeks to check ['_five_', '_six_', '_seven_']\n",
      "6\n",
      "7\n",
      "8\n",
      "weeks to check ['_six_', '_seven_', '_eight_']\n",
      "0\n",
      "3\n",
      "4\n",
      "5\n",
      "weeks to check ['_three_', '_four_', '_five_']\n",
      "4\n",
      "5\n",
      "6\n",
      "weeks to check ['_four_', '_five_', '_six_']\n",
      "5\n",
      "6\n",
      "7\n",
      "weeks to check ['_five_', '_six_', '_seven_']\n",
      "6\n",
      "7\n",
      "8\n",
      "weeks to check ['_six_', '_seven_', '_eight_']\n",
      "1\n",
      "3\n",
      "4\n",
      "5\n",
      "weeks to check ['_three_', '_four_', '_five_']\n",
      "4\n",
      "5\n",
      "6\n",
      "weeks to check ['_four_', '_five_', '_six_']\n",
      "5\n",
      "6\n",
      "7\n",
      "weeks to check ['_five_', '_six_', '_seven_']\n",
      "6\n",
      "7\n",
      "8\n",
      "weeks to check ['_six_', '_seven_', '_eight_']\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "weeks to check ['_three_', '_four_', '_five_']\n",
      "4\n",
      "5\n",
      "6\n",
      "weeks to check ['_four_', '_five_', '_six_']\n",
      "5\n",
      "6\n",
      "7\n",
      "weeks to check ['_five_', '_six_', '_seven_']\n",
      "6\n",
      "7\n",
      "8\n",
      "weeks to check ['_six_', '_seven_', '_eight_']\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "weeks to check ['_three_', '_four_', '_five_']\n",
      "4\n",
      "5\n",
      "6\n",
      "weeks to check ['_four_', '_five_', '_six_']\n",
      "5\n",
      "6\n",
      "7\n",
      "weeks to check ['_five_', '_six_', '_seven_']\n",
      "6\n",
      "7\n",
      "8\n",
      "weeks to check ['_six_', '_seven_', '_eight_']\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "weeks to check ['_three_', '_four_', '_five_']\n",
      "4\n",
      "5\n",
      "6\n",
      "weeks to check ['_four_', '_five_', '_six_']\n",
      "5\n",
      "6\n",
      "7\n",
      "weeks to check ['_five_', '_six_', '_seven_']\n",
      "6\n",
      "7\n",
      "8\n",
      "weeks to check ['_six_', '_seven_', '_eight_']\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "weeks to check ['_three_', '_four_', '_five_']\n",
      "4\n",
      "5\n",
      "6\n",
      "weeks to check ['_four_', '_five_', '_six_']\n",
      "5\n",
      "6\n",
      "7\n",
      "weeks to check ['_five_', '_six_', '_seven_']\n",
      "6\n",
      "7\n",
      "8\n",
      "weeks to check ['_six_', '_seven_', '_eight_']\n",
      "6\n",
      "3\n",
      "4\n",
      "5\n",
      "weeks to check ['_three_', '_four_', '_five_']\n",
      "4\n",
      "5\n",
      "6\n",
      "weeks to check ['_four_', '_five_', '_six_']\n",
      "5\n",
      "6\n",
      "7\n",
      "weeks to check ['_five_', '_six_', '_seven_']\n",
      "6\n",
      "7\n",
      "8\n",
      "weeks to check ['_six_', '_seven_', '_eight_']\n",
      "7\n",
      "3\n",
      "4\n",
      "5\n",
      "weeks to check ['_three_', '_four_', '_five_']\n",
      "4\n",
      "5\n",
      "6\n",
      "weeks to check ['_four_', '_five_', '_six_']\n",
      "5\n",
      "6\n",
      "7\n",
      "weeks to check ['_five_', '_six_', '_seven_']\n",
      "6\n",
      "7\n",
      "8\n",
      "weeks to check ['_six_', '_seven_', '_eight_']\n",
      "8\n",
      "3\n",
      "4\n",
      "5\n",
      "weeks to check ['_three_', '_four_', '_five_']\n",
      "4\n",
      "5\n",
      "6\n",
      "weeks to check ['_four_', '_five_', '_six_']\n",
      "5\n",
      "6\n",
      "7\n",
      "weeks to check ['_five_', '_six_', '_seven_']\n",
      "6\n",
      "7\n",
      "8\n",
      "weeks to check ['_six_', '_seven_', '_eight_']\n",
      "9\n",
      "3\n",
      "4\n",
      "5\n",
      "weeks to check ['_three_', '_four_', '_five_']\n",
      "4\n",
      "5\n",
      "6\n",
      "weeks to check ['_four_', '_five_', '_six_']\n",
      "5\n",
      "6\n",
      "7\n",
      "weeks to check ['_five_', '_six_', '_seven_']\n",
      "6\n",
      "7\n",
      "8\n",
      "weeks to check ['_six_', '_seven_', '_eight_']\n"
     ]
    }
   ],
   "source": [
    "#%reset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "from num2words import num2words\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, KFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score\n",
    "import word2number\n",
    "from word2number import w2n\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import random\n",
    "from matplotlib.patches import Polygon\n",
    "import shap\n",
    "\n",
    "from Functions import prep_training_test_data_period, prep_training_test_data, calculate_metrics,cross_validation_leave_geo_out, prep_training_test_data_shifted, add_labels_to_subplots, LOOCV_by_HSA_dataset, save_in_HSA_dictionary, prepare_data_and_model\n",
    "hfont = {'fontname':'Helvetica'}\n",
    "palette = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854', '#e5c494']\n",
    "\n",
    "\n",
    "\n",
    "######## FUNCTIONS ##########\n",
    "def add_labels_to_subplots(axs, hfont, height, fontsize):\n",
    "    labels_subplots = list(string.ascii_uppercase)\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.text(\n",
    "            ax.get_xlim()[0],\n",
    "            ax.get_ylim()[1] * height,\n",
    "            labels_subplots[i],\n",
    "            fontsize=fontsize,\n",
    "            **hfont,\n",
    "        )\n",
    "    return labels_subplots\n",
    "\n",
    "\n",
    "### this code it's exactly in  x weeks\n",
    "def merge_and_rename_data(data1, data2, on_column, suffix1, suffix2):\n",
    "    merged_data = pd.merge(\n",
    "        data1, data2, on=on_column, suffixes=(\"_\" + suffix1, \"_\" + suffix2)\n",
    "    )\n",
    "\n",
    "    new_column_names = [\n",
    "        col.replace(f\"_{on_column}_{suffix1}\", f\"_{suffix1}\").replace(\n",
    "            f\"_{on_column}_{suffix2}\", f\"_{suffix2}\"\n",
    "        )\n",
    "        for col in merged_data.columns\n",
    "    ]\n",
    "    merged_data.rename(\n",
    "        columns=dict(zip(merged_data.columns, new_column_names)), inplace=True\n",
    "    )\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def pivot_data_by_HSA(data, index_column, columns_column, values_column):\n",
    "    data_by_HSA = data[[index_column, columns_column, values_column]]\n",
    "    pivot_table = data_by_HSA.pivot_table(\n",
    "        index=index_column, columns=columns_column, values=values_column\n",
    "    )\n",
    "    return pivot_table\n",
    "\n",
    "\n",
    "def create_column_names(categories_for_subsetting, num_of_weeks):\n",
    "    column_names = [\"HSA_ID\"]\n",
    "\n",
    "    for week in range(1, num_of_weeks + 1):\n",
    "        week = num2words(week)\n",
    "        for category in categories_for_subsetting:\n",
    "            column_name = f\"week_{week}_{category}\"\n",
    "            column_names.append(column_name)\n",
    "\n",
    "    return column_names\n",
    "\n",
    "\n",
    "def create_collated_weekly_data(\n",
    "    pivoted_table, original_data, categories_for_subsetting, geography, column_names\n",
    "):\n",
    "    collated_data = pd.DataFrame(index=range(51), columns=column_names)\n",
    "\n",
    "    x = 0\n",
    "    for geo in original_data[geography].unique():\n",
    "        # matching_indices = [i for i, geo_col in enumerate(pivoted_table) if geo_col == geo]\n",
    "        collated_data.loc[x, geography] = geo\n",
    "        columns_to_subset = [\n",
    "            f\"{geo}_{category}\" for category in categories_for_subsetting\n",
    "        ]\n",
    "        j = 1\n",
    "        try:\n",
    "            for row in range(len(pivoted_table.loc[:, columns_to_subset])):\n",
    "                collated_data.iloc[\n",
    "                    x, j : j + len(categories_for_subsetting)\n",
    "                ] = pivoted_table.loc[row, columns_to_subset]\n",
    "                j += len(categories_for_subsetting)\n",
    "        except:\n",
    "            pass\n",
    "        x += 1\n",
    "\n",
    "    return collated_data\n",
    "\n",
    "\n",
    "def add_changes_by_week(weekly_data_frame, outcome_column):\n",
    "    for column in weekly_data_frame.columns[1:]:\n",
    "        # Calculate the difference between each row and the previous row\n",
    "        if outcome_column not in column.lower():  # want to leave out the outcome column\n",
    "            diff = weekly_data_frame[column].diff()\n",
    "\n",
    "            # Create a new column with the original column name and \"delta\"\n",
    "            new_column_name = column + \"_delta\"\n",
    "\n",
    "            column_index = weekly_data_frame.columns.get_loc(column)\n",
    "\n",
    "            # Insert the new column just after the original column\n",
    "            weekly_data_frame.insert(column_index + 1, new_column_name, diff)\n",
    "            weekly_data_frame[new_column_name] = diff\n",
    "    return weekly_data_frame\n",
    "\n",
    "\n",
    "def prep_training_test_data_shifted(\n",
    "    data, no_weeks, weeks_in_future, geography, weight_col, keep_output\n",
    "):\n",
    "    ## Get the weeks for the x and y datasets\n",
    "    x_weeks = []\n",
    "    y_weeks = []\n",
    "    y_weeks_to_check = []  # check these weeks to see if any of them are equal to 1\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_future\n",
    "        x_weeks.append(\"_\" + num2words(week) + \"_\")\n",
    "        for week_y in range(week + 2, test_week + 2):\n",
    "            y_weeks_to_check.append(\"_\" + num2words(week_y) + \"_\")\n",
    "        y_weeks.append(\"_\" + num2words(test_week) + \"_\")\n",
    "    ## Divide up the test/train split\n",
    "    # if is_geographic:\n",
    "    # Calculate the index to start slicing from\n",
    "    #    start_index = len(data['county']) // proportion[0] * proportion[1]\n",
    "    # Divide up the dataset based on this proportion\n",
    "    #    first_two_thirds = data['county'][:start_index]\n",
    "    #    last_third = data['county'][start_index:]\n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all = pd.DataFrame()\n",
    "    missing_data = []\n",
    "    ## Now get the training data\n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "        y_week = y_weeks[k]\n",
    "        k += 1\n",
    "\n",
    "        weeks_x = [col for col in data.columns if x_week in col]\n",
    "        columns_x = [geography] + weeks_x + [weight_col]\n",
    "        data_x = data[columns_x]\n",
    "\n",
    "        weeks_y = [col for col in data.columns if y_week in col]\n",
    "        columns_y = [geography] + weeks_y\n",
    "        data_y = data[columns_y]\n",
    "        ### now add the final column to the y data that has it so that it's if any week in the trhee week perdiod exceeded 15\n",
    "        train_week = w2n.word_to_num(x_week.replace(\"_\", \"\"))\n",
    "        target_week = w2n.word_to_num(y_week.replace(\"_\", \"\"))\n",
    "        y_weeks_to_check = []\n",
    "        for week_to_check in range(\n",
    "            train_week + 2, target_week + 2\n",
    "        ):  # have to ensure you skip the next week for getting the excess\n",
    "            y_weeks_to_check.append(\"_\" + num2words(week_to_check) + \"_\")\n",
    "        y_weeks_to_check = [week + \"beds_over_15_100k\" for week in y_weeks_to_check]\n",
    "        columns_to_check = [\n",
    "            col for col in data.columns if any(week in col for week in y_weeks_to_check)\n",
    "        ]\n",
    "        y_over_in_period = data[columns_to_check].apply(max, axis=1)\n",
    "        data_y = pd.concat([data_y, y_over_in_period], axis=1)\n",
    "        # ensure they have the same amount of data\n",
    "        # remove rows in test_data1 with NA in test_data2\n",
    "        data_x = data_x.dropna()\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        # remove rows in test_data2 with NA in test_data1\n",
    "        data_y = data_y.dropna()\n",
    "        data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "        missing_data.append(\n",
    "            (\n",
    "                (len(data[geography].unique()) - data_x_no_HSA)\n",
    "                / len(data[geography].unique())\n",
    "            )\n",
    "            * 100\n",
    "        )\n",
    "        # get weights\n",
    "        # weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "        X_week = data_x.iloc[:, 1 : len(columns_x)]  # take away y, leave weights for mo\n",
    "        y_week = data_y.iloc[:, -1]\n",
    "\n",
    "        y_week = y_week.astype(int)\n",
    "\n",
    "        weights = X_week.iloc[:, -1]\n",
    "        if keep_output:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 1\n",
    "            ]  # remove the weights and leave \"target\" for that week\n",
    "\n",
    "            # rename columns for concatenation\n",
    "            X_week.columns = range(1, len(data_x.columns) - 1)\n",
    "        else:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 2\n",
    "            ]  # remove the weights and  \"target\" for that week\n",
    "\n",
    "            X_week.columns = range(\n",
    "                1, len(data_x.columns) - 2\n",
    "            )  # remove the weights and  \"target\" for that week\n",
    "\n",
    "        y_week.columns = range(1, len(data_y.columns) - 2)\n",
    "        X_data = pd.concat([X_data, X_week])\n",
    "        y_data = pd.concat([y_data, y_week])\n",
    "\n",
    "        weights_all = pd.concat([weights_all, weights])\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return (X_data, y_data, weights_all, missing_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "### this code it's ANY in the x week period\n",
    "def prep_training_test_data(\n",
    "    data, no_weeks, weeks_in_future, geography, weight_col, keep_output\n",
    "):\n",
    "    ## Get the weeks for the x and y datasets\n",
    "    x_weeks = []\n",
    "    y_weeks = []\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_future\n",
    "        x_weeks.append(\"_\" + num2words(week) + \"_\")\n",
    "        y_weeks.append(\"_\" + num2words(test_week) + \"_\")\n",
    "\n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all = pd.DataFrame()\n",
    "    missing_data = []\n",
    "    ## Now get the training data\n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "        y_week = y_weeks[k]\n",
    "        k += 1\n",
    "        weeks_x = [col for col in data.columns if x_week in col]\n",
    "        columns_x = [geography] + weeks_x + [weight_col]\n",
    "        data_x = data[columns_x]\n",
    "\n",
    "        weeks_y = [col for col in data.columns if y_week in col]\n",
    "        columns_y = [geography] + weeks_y\n",
    "        data_y = data[columns_y]\n",
    "        # ensure they have the same amount of data\n",
    "        # remove rows in test_data1 with NA in test_data2\n",
    "        data_x = data_x.dropna()\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        # remove rows in test_data2 with NA in test_data1\n",
    "        data_y = data_y.dropna()\n",
    "        data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "        missing_data.append(\n",
    "            (\n",
    "                (len(data[geography].unique()) - data_x_no_HSA)\n",
    "                / len(data[geography].unique())\n",
    "            )\n",
    "            * 100\n",
    "        )\n",
    "        # get weights\n",
    "        # weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "        X_week = data_x.iloc[:, 1 : len(columns_x)]  # take away y, leave weights for mo\n",
    "        y_week = data_y.iloc[:, -1]\n",
    "\n",
    "        y_week = y_week.astype(int)\n",
    "        weights = X_week.iloc[:, -1]\n",
    "        if keep_output:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 1\n",
    "            ]  # remove the weights and leave \"target\" for that week\n",
    "\n",
    "            # rename columns for concatenation\n",
    "            X_week.columns = range(1, len(data_x.columns) - 1)\n",
    "        else:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 2\n",
    "            ]  # remove the weights and  \"target\" for that week\n",
    "\n",
    "            X_week.columns = range(\n",
    "                1, len(data_x.columns) - 2\n",
    "            )  # remove the weights and  \"target\" for that week\n",
    "\n",
    "            # rename columns for concatenation\n",
    "        y_week.columns = range(1, len(data_y.columns) - 1)\n",
    "        X_data = pd.concat([X_data, X_week])\n",
    "        y_data = pd.concat([y_data, y_week])\n",
    "\n",
    "        weights_all = pd.concat([weights_all, weights])\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return (X_data, y_data, weights_all, missing_data)\n",
    "\n",
    "\n",
    "def calculate_metrics(confusion_matrix):\n",
    "    # Extract values from the confusion matrix\n",
    "    TP = confusion_matrix[1, 1]\n",
    "    FP = confusion_matrix[0, 1]\n",
    "    TN = confusion_matrix[0, 0]\n",
    "    FN = confusion_matrix[1, 0]\n",
    "\n",
    "    # Calculate Sensitivity (True Positive Rate), Specificity (True Negative Rate),\n",
    "    # PPV (Precision), and NPV\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    ppv = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    npv = TN / (TN + FN) if (TN + FN) > 0 else 0.0\n",
    "\n",
    "    return sensitivity, specificity, ppv, npv\n",
    "\n",
    "\n",
    "def merge_and_rename_data(data1, data2, on_column, suffix1, suffix2):\n",
    "    merged_data = pd.merge(\n",
    "        data1, data2, on=on_column, suffixes=(\"_\" + suffix1, \"_\" + suffix2)\n",
    "    )\n",
    "\n",
    "    new_column_names = [\n",
    "        col.replace(f\"_{on_column}_{suffix1}\", f\"_{suffix1}\").replace(\n",
    "            f\"_{on_column}_{suffix2}\", f\"_{suffix2}\"\n",
    "        )\n",
    "        for col in merged_data.columns\n",
    "    ]\n",
    "    merged_data.rename(\n",
    "        columns=dict(zip(merged_data.columns, new_column_names)), inplace=True\n",
    "    )\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def pivot_data_by_HSA(data, index_column, columns_column, values_column):\n",
    "    data_by_HSA = data[[index_column, columns_column, values_column]]\n",
    "    pivot_table = data_by_HSA.pivot_table(\n",
    "        index=index_column, columns=columns_column, values=values_column\n",
    "    )\n",
    "    return pivot_table\n",
    "\n",
    "\n",
    "def add_changes_by_week(weekly_data_frame, outcome_column):\n",
    "    for column in weekly_data_frame.columns[1:]:\n",
    "        # Calculate the difference between each row and the previous row\n",
    "        if outcome_column not in column.lower():  # want to leave out the outcome column\n",
    "            diff = weekly_data_frame[column].diff()\n",
    "\n",
    "            # Create a new column with the original column name and \"delta\"\n",
    "            new_column_name = column + \"_delta\"\n",
    "\n",
    "            column_index = weekly_data_frame.columns.get_loc(column)\n",
    "\n",
    "            # Insert the new column just after the original column\n",
    "            weekly_data_frame.insert(column_index + 1, new_column_name, diff)\n",
    "            weekly_data_frame[new_column_name] = diff\n",
    "    return weekly_data_frame\n",
    "\n",
    "\n",
    "def determine_covid_outcome_indicator(\n",
    "    new_cases_per_100k, new_admits_per_100k, percent_beds_100k\n",
    "):\n",
    "    if new_cases_per_100k < 200:\n",
    "        if (new_admits_per_100k >= 10) | (\n",
    "            percent_beds_100k > 0.10\n",
    "        ):  # Changed .10 to 0.10\n",
    "            if (new_admits_per_100k >= 20) | (percent_beds_100k >= 15):\n",
    "                return \"High\"\n",
    "            else:\n",
    "                return \"Medium\"\n",
    "        else:\n",
    "            return \"Low\"\n",
    "    elif new_cases_per_100k >= 200:\n",
    "        if (new_admits_per_100k >= 10) | (\n",
    "            percent_beds_100k >= 0.10\n",
    "        ):  # Changed .10 to 0.10\n",
    "            return \"High\"\n",
    "        elif (new_admits_per_100k < 10) | (percent_beds_100k < 10):\n",
    "            return \"Medium\"\n",
    "\n",
    "\n",
    "def simplify_labels_graphviz(graph):\n",
    "    for node in graph.get_node_list():\n",
    "        if node.get_attributes().get(\"label\") is None:\n",
    "            continue\n",
    "        else:\n",
    "            split_label = node.get_attributes().get(\"label\").split(\"<br/>\")\n",
    "            if len(split_label) == 4:\n",
    "                split_label[3] = split_label[3].split(\"=\")[1].strip()\n",
    "\n",
    "                del split_label[1]  # number of samples\n",
    "                del split_label[1]  # split of sample\n",
    "            elif len(split_label) == 3:  # for a terminating node, no rule is provided\n",
    "                split_label[2] = split_label[2].split(\"=\")[1].strip()\n",
    "\n",
    "                del split_label[0]  # number of samples\n",
    "                del split_label[0]  # split of samples\n",
    "                split_label[0] = \"<\" + split_label[0]\n",
    "            node.set(\"label\", \"<br/>\".join(split_label))\n",
    "\n",
    "\n",
    "def generate_decision_tree_graph(classifier, class_names, feature_names):\n",
    "    dot_data = StringIO()\n",
    "    tree.export_graphviz(\n",
    "        classifier,\n",
    "        out_file=dot_data,\n",
    "        class_names=class_names,\n",
    "        feature_names=feature_names,\n",
    "        filled=True,\n",
    "        rounded=True,\n",
    "        special_characters=True,\n",
    "        proportion=False,\n",
    "        precision=0,\n",
    "        impurity=False,\n",
    "    )\n",
    "\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def cross_validation_leave_geo_out(\n",
    "    data,\n",
    "    geography_column,\n",
    "    geo_split,\n",
    "    no_iterations,\n",
    "    cv,\n",
    "    classifier,\n",
    "    param_grid,\n",
    "    no_iterations_param,\n",
    "    no_weeks_train,\n",
    "    no_weeks_test,\n",
    "    weeks_in_future,\n",
    "    weight_col,\n",
    "    keep_output,\n",
    "    time_period,\n",
    "):\n",
    "    best_hyperparameters_per_iter = []\n",
    "    auROC_per_iter = []\n",
    "\n",
    "    for i in range(no_iterations):\n",
    "        print(i)\n",
    "        # Subset the HSAs from the full dataset\n",
    "        geo_names = data[geography_column].unique()\n",
    "        num_names_to_select = int(geo_split * len(geo_names))\n",
    "        geos_for_sample = random.sample(list(geo_names), num_names_to_select)\n",
    "        subset_HSAs_for_train = data[data[geography_column].isin(geos_for_sample)]\n",
    "        subset_HSAs_for_test = data[~data[geography_column].isin(geos_for_sample)]\n",
    "\n",
    "        # Create training and test data\n",
    "        if time_period == \"period\":\n",
    "            (\n",
    "                X_sample_train,\n",
    "                y_sample_train,\n",
    "                weights_train,\n",
    "                missing_data_train_HSA,\n",
    "            ) = prep_training_test_data_period(\n",
    "                subset_HSAs_for_train,\n",
    "                no_weeks=no_weeks_train,\n",
    "                weeks_in_future=weeks_in_future,\n",
    "                geography=geography_column,\n",
    "                weight_col=weight_col,\n",
    "                keep_output=keep_output,\n",
    "            )\n",
    "            (\n",
    "                X_sample_test,\n",
    "                y_sample_test,\n",
    "                weights_test,\n",
    "                missing_data_train_HSA,\n",
    "            ) = prep_training_test_data_period(\n",
    "                subset_HSAs_for_test,\n",
    "                no_weeks=no_weeks_test,\n",
    "                weeks_in_future=weeks_in_future,\n",
    "                geography=geography_column,\n",
    "                weight_col=weight_col,\n",
    "                keep_output=keep_output,\n",
    "            )\n",
    "            weights_train = weights_train[0]\n",
    "        elif time_period == \"exact\":\n",
    "            (\n",
    "                X_sample_train,\n",
    "                y_sample_train,\n",
    "                weights_train,\n",
    "                missing_data_train_HSA,\n",
    "            ) = prep_training_test_data(\n",
    "                subset_HSAs_for_train,\n",
    "                no_weeks=no_weeks_train,\n",
    "                weeks_in_future=weeks_in_future,\n",
    "                geography=geography_column,\n",
    "                weight_col=weight_col,\n",
    "                keep_output=keep_output,\n",
    "            )\n",
    "            (\n",
    "                X_sample_test,\n",
    "                y_sample_test,\n",
    "                weights_test,\n",
    "                missing_data_train_HSA,\n",
    "            ) = prep_training_test_data(\n",
    "                subset_HSAs_for_test,\n",
    "                no_weeks=no_weeks_test,\n",
    "                weeks_in_future=weeks_in_future,\n",
    "                geography=geography_column,\n",
    "                weight_col=weight_col,\n",
    "                keep_output=keep_output,\n",
    "            )\n",
    "            weights_train = weights_train[0]\n",
    "        elif time_period == \"shifted\":\n",
    "            (\n",
    "                X_sample_train,\n",
    "                y_sample_train,\n",
    "                weights_train,\n",
    "                missing_data_train_HSA,\n",
    "            ) = prep_training_test_data_shifted(\n",
    "                subset_HSAs_for_train,\n",
    "                no_weeks=no_weeks_train,\n",
    "                weeks_in_future=weeks_in_future,\n",
    "                geography=geography_column,\n",
    "                weight_col=weight_col,\n",
    "                keep_output=keep_output,\n",
    "            )\n",
    "            (\n",
    "                X_sample_test,\n",
    "                y_sample_test,\n",
    "                weights_test,\n",
    "                missing_data_train_HSA,\n",
    "            ) = prep_training_test_data_shifted(\n",
    "                subset_HSAs_for_test,\n",
    "                no_weeks=no_weeks_test,\n",
    "                weeks_in_future=weeks_in_future,\n",
    "                geography=geography_column,\n",
    "                weight_col=weight_col,\n",
    "                keep_output=keep_output,\n",
    "            )\n",
    "            weights_train = weights_train[0]\n",
    "\n",
    "        # Check if y_sample_test contains only 1's\n",
    "        while (int(y_sample_test.sum().iloc[0]) / len(y_sample_test)) == 1:\n",
    "            print(\"All 1\")\n",
    "            # Subset the HSAs from the full dataset\n",
    "            geo_names = data[geography_column].unique()\n",
    "            num_names_to_select = int(geo_split * len(geo_names))\n",
    "            geos_for_sample = random.sample(list(geo_names), num_names_to_select)\n",
    "            subset_HSAs_for_train = data[data[geography_column].isin(geos_for_sample)]\n",
    "            subset_HSAs_for_test = data[~data[geography_column].isin(geos_for_sample)]\n",
    "\n",
    "            # Create training and test data\n",
    "            if time_period == \"period\":\n",
    "                (\n",
    "                    X_sample_train,\n",
    "                    y_sample_train,\n",
    "                    weights_train,\n",
    "                    missing_data_train_HSA,\n",
    "                ) = prep_training_test_data_period(\n",
    "                    subset_HSAs_for_train,\n",
    "                    no_weeks=no_weeks_train,\n",
    "                    weeks_in_future=weeks_in_future,\n",
    "                    geography=geography_column,\n",
    "                    weight_col=weight_col,\n",
    "                    keep_output=keep_output,\n",
    "                )\n",
    "                (\n",
    "                    X_sample_test,\n",
    "                    y_sample_test,\n",
    "                    weights_test,\n",
    "                    missing_data_train_HSA,\n",
    "                ) = prep_training_test_data_period(\n",
    "                    subset_HSAs_for_test,\n",
    "                    no_weeks=no_weeks_test,\n",
    "                    weeks_in_future=weeks_in_future,\n",
    "                    geography=geography_column,\n",
    "                    weight_col=weight_col,\n",
    "                    keep_output=keep_output,\n",
    "                )\n",
    "                weights_train = weights_train[0]\n",
    "            elif time_period == \"exact\":\n",
    "                (\n",
    "                    X_sample_train,\n",
    "                    y_sample_train,\n",
    "                    weights_train,\n",
    "                    missing_data_train_HSA,\n",
    "                ) = prep_training_test_data(\n",
    "                    subset_HSAs_for_train,\n",
    "                    no_weeks=no_weeks_train,\n",
    "                    weeks_in_future=weeks_in_future,\n",
    "                    geography=geography_column,\n",
    "                    weight_col=weight_col,\n",
    "                    keep_output=keep_output,\n",
    "                )\n",
    "                (\n",
    "                    X_sample_test,\n",
    "                    y_sample_test,\n",
    "                    weights_test,\n",
    "                    missing_data_train_HSA,\n",
    "                ) = prep_training_test_data(\n",
    "                    subset_HSAs_for_test,\n",
    "                    no_weeks=no_weeks_test,\n",
    "                    weeks_in_future=weeks_in_future,\n",
    "                    geography=geography_column,\n",
    "                    weight_col=weight_col,\n",
    "                    keep_output=keep_output,\n",
    "                )\n",
    "                weights_train = weights_train[0]\n",
    "            elif time_period == \"shifted\":\n",
    "                (\n",
    "                    X_sample_train,\n",
    "                    y_sample_train,\n",
    "                    weights_train,\n",
    "                    missing_data_train_HSA,\n",
    "                ) = prep_training_test_data_shifted(\n",
    "                    subset_HSAs_for_train,\n",
    "                    no_weeks=no_weeks_train,\n",
    "                    weeks_in_future=weeks_in_future,\n",
    "                    geography=geography_column,\n",
    "                    weight_col=weight_col,\n",
    "                    keep_output=keep_output,\n",
    "                )\n",
    "                (\n",
    "                    X_sample_test,\n",
    "                    y_sample_test,\n",
    "                    weights_test,\n",
    "                    missing_data_train_HSA,\n",
    "                ) = prep_training_test_data_shifted(\n",
    "                    subset_HSAs_for_test,\n",
    "                    no_weeks=no_weeks_test,\n",
    "                    weeks_in_future=weeks_in_future,\n",
    "                    geography=geography_column,\n",
    "                    weight_col=weight_col,\n",
    "                    keep_output=keep_output,\n",
    "                )\n",
    "                weights_train = weights_train[0]\n",
    "\n",
    "        random_search = RandomizedSearchCV(\n",
    "            classifier, param_grid, n_iter=no_iterations_param, cv=cv, random_state=10\n",
    "        )\n",
    "        random_search.fit(X_sample_train, y_sample_train, sample_weight=weights_train)\n",
    "        best_params = random_search.best_params_\n",
    "\n",
    "        # Create the Decision Tree classifier with the best hyperparameters\n",
    "        model = DecisionTreeClassifier(\n",
    "            **best_params, random_state=10, class_weight=\"balanced\"\n",
    "        )\n",
    "        model_fit = model.fit(\n",
    "            X_sample_train, y_sample_train, sample_weight=weights_train\n",
    "        )\n",
    "        y_pred = model_fit.predict_proba(X_sample_test)\n",
    "\n",
    "        # Evaluate the accuracy of the model\n",
    "        best_hyperparameters_per_iter.append(best_params)\n",
    "        auROC_per_iter.append(roc_auc_score(y_sample_test, y_pred[:, 1]))\n",
    "\n",
    "    return best_hyperparameters_per_iter[np.argmax(np.array(auROC_per_iter))]\n",
    "\n",
    "\n",
    "def LOOCV_by_HSA_dataset(dataframe, geo_ID, geo_ID_col):\n",
    "    training_dataframe = dataframe[dataframe[geo_ID_col] != geo_ID]\n",
    "    testing_dataframe = dataframe[dataframe[geo_ID_col] == geo_ID]\n",
    "    return training_dataframe, testing_dataframe\n",
    "\n",
    "\n",
    "def save_in_HSA_dictionary(\n",
    "    prediction_week,\n",
    "    ROC_by_week,\n",
    "    accuracy_by_week,\n",
    "    sensitivity_by_week,\n",
    "    specificity_by_week,\n",
    "    ppv_by_week,\n",
    "    npv_by_week,\n",
    "    ROC_by_HSA,\n",
    "    accuracy_by_HSA,\n",
    "    sensitivity_by_HSA,\n",
    "    specificity_by_HSA,\n",
    "    ppv_by_HSA,\n",
    "    npv_by_HSA,\n",
    "):\n",
    "    ROC_by_HSA[prediction_week] = ROC_by_week\n",
    "    accuracy_by_HSA[prediction_week] = accuracy_by_week\n",
    "    sensitivity_by_HSA[prediction_week] = sensitivity_by_week\n",
    "    specificity_by_HSA[prediction_week] = specificity_by_week\n",
    "    ppv_by_HSA[prediction_week] = ppv_by_week\n",
    "    npv_by_HSA[prediction_week] = npv_by_week\n",
    "\n",
    "\n",
    "def prep_training_test_data_shifted(\n",
    "    data, no_weeks, weeks_in_future, geography, weight_col, keep_output\n",
    "):\n",
    "    ## Get the weeks for the x and y datasets\n",
    "    x_weeks = []\n",
    "    y_weeks = []\n",
    "    y_weeks_to_check = []  # check these weeks to see if any of them are equal to 1\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_future\n",
    "        x_weeks.append(\"_\" + num2words(week) + \"_\")\n",
    "        for week_y in range(week + 2, test_week + 2):\n",
    "            y_weeks_to_check.append(\"_\" + num2words(week_y) + \"_\")\n",
    "        y_weeks.append(\"_\" + num2words(test_week) + \"_\")\n",
    "    ## Divide up the test/train split\n",
    "    # if is_geographic:\n",
    "    # Calculate the index to start slicing from\n",
    "    #    start_index = len(data['county']) // proportion[0] * proportion[1]\n",
    "    # Divide up the dataset based on this proportion\n",
    "    #    first_two_thirds = data['county'][:start_index]\n",
    "    #    last_third = data['county'][start_index:]\n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all = pd.DataFrame()\n",
    "    missing_data = []\n",
    "    ## Now get the training data\n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "        y_week = y_weeks[k]\n",
    "        k += 1\n",
    "\n",
    "        weeks_x = [col for col in data.columns if x_week in col]\n",
    "        columns_x = [geography] + weeks_x + [weight_col]\n",
    "        data_x = data[columns_x]\n",
    "\n",
    "        weeks_y = [col for col in data.columns if y_week in col]\n",
    "        columns_y = [geography] + weeks_y\n",
    "        data_y = data[columns_y]\n",
    "        ### now add the final column to the y data that has it so that it's if any week in the trhee week perdiod exceeded 15\n",
    "        train_week = w2n.word_to_num(x_week.replace(\"_\", \"\"))\n",
    "        target_week = w2n.word_to_num(y_week.replace(\"_\", \"\"))\n",
    "        y_weeks_to_check = []\n",
    "        for week_to_check in range(train_week + 2, target_week + 2):  # have to ensure you skip the next week for getting the excess\n",
    "            print(week_to_check)\n",
    "            y_weeks_to_check.append(\"_\" + num2words(week_to_check) + \"_\")\n",
    "        print(\"weeks to check\", y_weeks_to_check)\n",
    "        y_weeks_to_check = [week + \"beds_over_15_100k\" for week in y_weeks_to_check]\n",
    "        columns_to_check = [\n",
    "            col for col in data.columns if any(week in col for week in y_weeks_to_check)\n",
    "        ]\n",
    "        y_over_in_period = data[columns_to_check].apply(max, axis=1)\n",
    "        data_y = pd.concat([data_y, y_over_in_period], axis=1)\n",
    "        # ensure they have the same amount of data\n",
    "        # remove rows in test_data1 with NA in test_data2\n",
    "        data_x = data_x.dropna()\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        # remove rows in test_data2 with NA in test_data1\n",
    "        data_y = data_y.dropna()\n",
    "        data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "        missing_data.append(\n",
    "            (\n",
    "                (len(data[geography].unique()) - data_x_no_HSA)\n",
    "                / len(data[geography].unique())\n",
    "            )\n",
    "            * 100\n",
    "        )\n",
    "        # get weights\n",
    "        # weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "        X_week = data_x.iloc[:, 1 : len(columns_x)]  # take away y, leave weights for mo\n",
    "        y_week = data_y.iloc[:, -1]\n",
    "\n",
    "        y_week = y_week.astype(int)\n",
    "\n",
    "        weights = X_week.iloc[:, -1]\n",
    "        if keep_output:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 1\n",
    "            ]  # remove the weights and leave \"target\" for that week\n",
    "\n",
    "            # rename columns for concatenation\n",
    "            X_week.columns = range(1, len(data_x.columns) - 1)\n",
    "        else:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 2\n",
    "            ]  # remove the weights and  \"target\" for that week\n",
    "\n",
    "            X_week.columns = range(\n",
    "                1, len(data_x.columns) - 2\n",
    "            )  # remove the weights and  \"target\" for that week\n",
    "\n",
    "        y_week.columns = range(1, len(data_y.columns) - 2)\n",
    "        X_data = pd.concat([X_data, X_week])\n",
    "        y_data = pd.concat([y_data, y_week])\n",
    "\n",
    "        weights_all = pd.concat([weights_all, weights])\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return (X_data, y_data, weights_all, missing_data)\n",
    "\n",
    "\n",
    "def LOOCV_by_HSA_dataset(dataframe, geo_ID, geo_ID_col):\n",
    "    training_dataframe = dataframe[dataframe[geo_ID_col] != geo_ID]\n",
    "    testing_dataframe = dataframe[dataframe[geo_ID_col] == geo_ID]\n",
    "    return training_dataframe, testing_dataframe\n",
    "\n",
    "\n",
    "def save_in_HSA_dictionary(\n",
    "    prediction_week,\n",
    "    ROC_by_week,\n",
    "    accuracy_by_week,\n",
    "    sensitivity_by_week,\n",
    "    specificity_by_week,\n",
    "    ppv_by_week,\n",
    "    npv_by_week,\n",
    "    ROC_by_HSA,\n",
    "    accuracy_by_HSA,\n",
    "    sensitivity_by_HSA,\n",
    "    specificity_by_HSA,\n",
    "    ppv_by_HSA,\n",
    "    npv_by_HSA,\n",
    "):\n",
    "    ROC_by_HSA[prediction_week] = ROC_by_week\n",
    "    accuracy_by_HSA[prediction_week] = accuracy_by_week\n",
    "    sensitivity_by_HSA[prediction_week] = sensitivity_by_week\n",
    "    specificity_by_HSA[prediction_week] = specificity_by_week\n",
    "    ppv_by_HSA[prediction_week] = ppv_by_week\n",
    "    npv_by_HSA[prediction_week] = npv_by_week\n",
    "\n",
    "\n",
    "######### IMPORT DATA ##############\n",
    "\n",
    "HSA_weekly_data_all = pd.read_csv(\"/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates_weekly_incl_NA.csv\")\n",
    "\n",
    "\n",
    "########### SET UP FOR EXPANDING MODELS\n",
    "clf_full_period = DecisionTreeClassifier(random_state=10, class_weight=\"balanced\")\n",
    "\n",
    "\n",
    "no_iterations = 10\n",
    "geography_column = \"HSA_ID\"\n",
    "geo_split = 0.9\n",
    "time_period = \"shifted\"  # Choose 'period', 'exact', or 'shifted'\n",
    "size_of_test_dataset = 1\n",
    "train_weeks_for_initial_model = 1\n",
    "\n",
    "weeks_in_future = 3\n",
    "weight_col = \"weight\"\n",
    "keep_output = True\n",
    "\n",
    "no_iterations_param = 100  # Replace with the number of iterations for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": np.arange(2, 5, 1),\n",
    "    \"min_samples_split\": np.arange(\n",
    "        200, 2000, 50\n",
    "    ),  # [100, 200, 300, 400, 500], #np.arange(50, 200),\n",
    "    \"min_samples_leaf\": np.arange(200, 2000, 50),\n",
    "}  # 100, 200, 300, 400, 500], #np.arange(500, 200)\n",
    "#'ccp_alpha': np.arange(0.0001, 0.0035, 0.0001) }\n",
    "\n",
    "# Create the Decision Tree classifier\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=10, n_repeats=10, random_state=1\n",
    ")  ## 10-fold cross validations\n",
    "\n",
    "\n",
    "######### ACTUAL RUNS ############\n",
    "weeks_to_predict = [2]\n",
    "ROC_by_week_full_period = []\n",
    "sensitivity_by_week_full_period = []\n",
    "specificity_by_week_full_period = []\n",
    "ppv_by_week_full_period = []\n",
    "npv_by_week_full_period = []\n",
    "accuracy_by_week_full_period = []\n",
    "norm_MCC_by_week_full_period = []\n",
    "\n",
    "for prediction_week in weeks_to_predict:\n",
    "    print(\"prediction week\", prediction_week)\n",
    "    no_weeks_train = range(1, int(prediction_week + train_weeks_for_initial_model) + 1)\n",
    "    no_weeks_test = range(\n",
    "        int(prediction_week + train_weeks_for_initial_model) + 1,\n",
    "        int(prediction_week + train_weeks_for_initial_model + size_of_test_dataset) + 1,\n",
    "    )\n",
    "    (\n",
    "        X_train_full_period,\n",
    "        y_train_full_period,\n",
    "        weights_full_period,\n",
    "        missing_data_train_HSA,\n",
    "    ) = prep_training_test_data_shifted(\n",
    "        HSA_weekly_data_all,\n",
    "        no_weeks=no_weeks_train,\n",
    "        weeks_in_future=3,\n",
    "        geography=\"HSA_ID\",\n",
    "        weight_col=\"weight\",\n",
    "        keep_output=keep_output,\n",
    "    )\n",
    "\n",
    "    (\n",
    "        X_test_full_period,\n",
    "        y_test_full_period,\n",
    "        weights_test_full_period,\n",
    "        missing_data_test_HSA,\n",
    "    ) = prep_training_test_data_shifted(\n",
    "        HSA_weekly_data_all,\n",
    "        no_weeks=no_weeks_test,\n",
    "        weeks_in_future=3,\n",
    "        geography=\"HSA_ID\",\n",
    "        weight_col=\"weight\",\n",
    "        keep_output=keep_output,\n",
    "    )\n",
    "    weights_full_period = weights_full_period[0].to_numpy()\n",
    "    best_params = cross_validation_leave_geo_out(\n",
    "        HSA_weekly_data_all,\n",
    "        geography_column=geography_column,\n",
    "        geo_split=geo_split,\n",
    "        no_iterations=no_iterations,\n",
    "        cv=cv,\n",
    "        classifier=clf_full_period,\n",
    "        param_grid=param_grid,\n",
    "        no_iterations_param=no_iterations_param,\n",
    "        no_weeks_train=no_weeks_train,\n",
    "        no_weeks_test=no_weeks_test,\n",
    "        weeks_in_future=weeks_in_future,\n",
    "        weight_col=weight_col,\n",
    "        keep_output=keep_output,\n",
    "        time_period=time_period,\n",
    "    )\n",
    "    clf_full_period = DecisionTreeClassifier(\n",
    "        **best_params, random_state=10, class_weight=\"balanced\"\n",
    "    )\n",
    "    clf_full_period.fit(\n",
    "        X_train_full_period, y_train_full_period, sample_weight=weights_full_period\n",
    "    )\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = clf_full_period.predict(X_test_full_period)\n",
    "    y_pred_proba = clf_full_period.predict_proba(X_test_full_period)\n",
    "\n",
    "    # Evaluate the accuracy of the model\n",
    "    accuracy_by_week_full_period.append(accuracy_score(y_test_full_period, y_pred))\n",
    "    ROC_by_week_full_period.append(\n",
    "        roc_auc_score(y_test_full_period, y_pred_proba[:, 1])\n",
    "    )\n",
    "    conf_matrix = confusion_matrix(y_test_full_period, y_pred)\n",
    "\n",
    "    model_name_to_save = (\n",
    "        \"/Users/rem76/Documents/COVID_projections/Expanding_full_model_shifted/Full_model_shifted_NA/Full_model_\" + time_period + \"_\" + str(prediction_week) + \".sav\"\n",
    "    )\n",
    "\n",
    "    pickle.dump(clf_full_period, open(model_name_to_save, \"wb\"))\n",
    "    sensitvity, specificity, ppv, npv = calculate_metrics(conf_matrix)\n",
    "    specificity_by_week_full_period.append(specificity)\n",
    "    # Calculate sensitivity (true positive rate)\n",
    "    sensitivity_by_week_full_period.append(sensitvity)\n",
    "    norm_MCC_by_week_full_period.append(\n",
    "        (matthews_corrcoef(y_test_full_period, y_pred) + 1) / 2\n",
    "    )\n",
    "\n",
    "    ppv_by_week_full_period.append(ppv)\n",
    "    npv_by_week_full_period.append(npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8655855948464912]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROC_by_week_full_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x_weeks = []\n",
    "    y_weeks = []\n",
    "    y_weeks_to_check = []  # check these weeks to see if any of them are equal to 1\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_future\n",
    "        x_weeks.append(\"_\" + num2words(week) + \"_\")\n",
    "        for week_y in range(week + 2, test_week + 2):\n",
    "            y_weeks_to_check.append(\"_\" + num2words(week_y) + \"_\")\n",
    "        y_weeks.append(\"_\" + num2words(test_week) + \"_\")\n",
    "    ## Divide up the test/train split\n",
    "    # if is_geographic:\n",
    "    # Calculate the index to start slicing from\n",
    "    #    start_index = len(data['county']) // proportion[0] * proportion[1]\n",
    "    # Divide up the dataset based on this proportion\n",
    "    #    first_two_thirds = data['county'][:start_index]\n",
    "    #    last_third = data['county'][start_index:]\n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all = pd.DataFrame()\n",
    "    missing_data = []\n",
    "    ## Now get the training data\n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "        y_week = y_weeks[k]\n",
    "        k += 1\n",
    "\n",
    "        weeks_x = [col for col in data.columns if x_week in col]\n",
    "        columns_x = [geography] + weeks_x + [weight_col]\n",
    "        data_x = data[columns_x]\n",
    "\n",
    "        weeks_y = [col for col in data.columns if y_week in col]\n",
    "        columns_y = [geography] + weeks_y\n",
    "        data_y = data[columns_y]\n",
    "        ### now add the final column to the y data that has it so that it's if any week in the trhee week perdiod exceeded 15\n",
    "        train_week = w2n.word_to_num(x_week.replace(\"_\", \"\"))\n",
    "        target_week = w2n.word_to_num(y_week.replace(\"_\", \"\"))\n",
    "        y_weeks_to_check = []\n",
    "        for week_to_check in range(\n",
    "            train_week + 2, target_week + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " range(1, int(prediction_week + train_weeks_for_initial_model) + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COVID_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a72d884e172d11118bfdfb578e108fb6b63a679c74d3d7d2f9d493a9a72737c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
