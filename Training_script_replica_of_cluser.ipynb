{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "7\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "8\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "11\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "12\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "13\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "15\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "16\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "All 1\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "17\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "All 1\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "All 1\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "19\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "20\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "21\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "22\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "23\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "24\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "25\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "26\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "27\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "28\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "29\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "30\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "31\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "32\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "33\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "34\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "35\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "36\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "37\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "38\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "39\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "40\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "41\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "42\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "43\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "44\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "45\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "46\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "47\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "48\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "49\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "50\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "51\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "52\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "53\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "54\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "55\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "56\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "57\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "58\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "59\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "60\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "61\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "62\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "63\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "64\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "65\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "66\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "67\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "68\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "69\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "70\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "71\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "72\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "73\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "All 1\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "74\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "All 1\n",
      "5\n",
      "6\n",
      "7\n",
      "All 1\n",
      "8\n",
      "9\n",
      "75\n",
      "0\n",
      "1\n",
      "2\n",
      "All 1\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "All 1\n",
      "9\n",
      "76\n",
      "0\n",
      "1\n",
      "All 1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "77\n",
      "0\n",
      "1\n",
      "All 1\n",
      "2\n",
      "All 1\n",
      "3\n",
      "4\n",
      "All 1\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "78\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "79\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "80\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "81\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "82\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "83\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "84\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "85\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "86\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "87\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "88\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "89\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "90\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "91\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "92\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "93\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "94\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "95\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "96\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "97\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "98\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "100\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "101\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "102\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "103\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "104\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "105\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "106\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "107\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "108\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "109\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "110\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "111\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "112\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "113\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "114\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "115\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "116\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "117\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "118\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multi_class must be in ('ovo', 'ovr')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 930\u001b[0m\n\u001b[1;32m    916\u001b[0m (\n\u001b[1;32m    917\u001b[0m     X_test_full_period,\n\u001b[1;32m    918\u001b[0m     y_test_full_period,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     keep_output\u001b[39m=\u001b[39mkeep_output,\n\u001b[1;32m    928\u001b[0m )\n\u001b[1;32m    929\u001b[0m weights_full_period \u001b[39m=\u001b[39m weights_full_period[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto_numpy()\n\u001b[0;32m--> 930\u001b[0m best_params \u001b[39m=\u001b[39m cross_validation_leave_geo_out(\n\u001b[1;32m    931\u001b[0m     HSA_weekly_data_all,\n\u001b[1;32m    932\u001b[0m     geography_column\u001b[39m=\u001b[39mgeography_column,\n\u001b[1;32m    933\u001b[0m     geo_split\u001b[39m=\u001b[39mgeo_split,\n\u001b[1;32m    934\u001b[0m     no_iterations\u001b[39m=\u001b[39mno_iterations,\n\u001b[1;32m    935\u001b[0m     cv\u001b[39m=\u001b[39mcv,\n\u001b[1;32m    936\u001b[0m     classifier\u001b[39m=\u001b[39mclf,\n\u001b[1;32m    937\u001b[0m     param_grid\u001b[39m=\u001b[39mparam_grid,\n\u001b[1;32m    938\u001b[0m     no_iterations_param\u001b[39m=\u001b[39mno_iterations_param,\n\u001b[1;32m    939\u001b[0m     no_weeks_train\u001b[39m=\u001b[39mno_weeks_train,\n\u001b[1;32m    940\u001b[0m     no_weeks_test\u001b[39m=\u001b[39mno_weeks_test,\n\u001b[1;32m    941\u001b[0m     weeks_in_future\u001b[39m=\u001b[39mweeks_in_future,\n\u001b[1;32m    942\u001b[0m     weight_col\u001b[39m=\u001b[39mweight_col,\n\u001b[1;32m    943\u001b[0m     keep_output\u001b[39m=\u001b[39mkeep_output,\n\u001b[1;32m    944\u001b[0m     time_period\u001b[39m=\u001b[39mtime_period,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m clf \u001b[39m=\u001b[39m DecisionTreeClassifier(\n\u001b[1;32m    947\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbest_params, random_state\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, class_weight\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    948\u001b[0m )\n\u001b[1;32m    949\u001b[0m clf\u001b[39m.\u001b[39mfit(\n\u001b[1;32m    950\u001b[0m     X_train_full_period, y_train_full_period, sample_weight\u001b[39m=\u001b[39mweights_full_period\n\u001b[1;32m    951\u001b[0m )\n",
      "Cell \u001b[0;32mIn[2], line 675\u001b[0m, in \u001b[0;36mcross_validation_leave_geo_out\u001b[0;34m(data, geography_column, geo_split, no_iterations, cv, classifier, param_grid, no_iterations_param, no_weeks_train, no_weeks_test, weeks_in_future, weight_col, keep_output, time_period)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[39m# Evaluate the accuracy of the model\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     best_hyperparameters_per_iter\u001b[39m.\u001b[39mappend(best_params)\n\u001b[0;32m--> 675\u001b[0m     auROC_per_iter\u001b[39m.\u001b[39mappend(roc_auc_score(y_sample_test, y_pred[:, \u001b[39m1\u001b[39m]))\n\u001b[1;32m    677\u001b[0m \u001b[39mreturn\u001b[39;00m best_hyperparameters_per_iter[np\u001b[39m.\u001b[39margmax(np\u001b[39m.\u001b[39marray(auROC_per_iter))]\n",
      "File \u001b[0;32m~/miniconda3/envs/COVID_forecasting/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/COVID_forecasting/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:619\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    613\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPartial AUC computation not available in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmulticlass setting, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_fpr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    615\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m set to `None`, received `max_fpr=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    616\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minstead\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(max_fpr)\n\u001b[1;32m    617\u001b[0m         )\n\u001b[1;32m    618\u001b[0m     \u001b[39mif\u001b[39;00m multi_class \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmulti_class must be in (\u001b[39m\u001b[39m'\u001b[39m\u001b[39movo\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39movr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    620\u001b[0m     \u001b[39mreturn\u001b[39;00m _multiclass_roc_auc_score(\n\u001b[1;32m    621\u001b[0m         y_true, y_score, labels, multi_class, average, sample_weight\n\u001b[1;32m    622\u001b[0m     )\n\u001b[1;32m    623\u001b[0m \u001b[39melif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: multi_class must be in ('ovo', 'ovr')"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from num2words import num2words\n",
    "from word2number import w2n\n",
    "import pydotplus\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    KFold,\n",
    "    RepeatedStratifiedKFold,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    matthews_corrcoef,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    RocCurveDisplay,\n",
    ")\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# Now you can access the job number as args.job_number\n",
    "job_number = 1\n",
    "\n",
    "\n",
    "\n",
    "######## FUNCTIONS ##########\n",
    "def add_labels_to_subplots(axs, hfont, height, fontsize):\n",
    "    labels_subplots = list(string.ascii_uppercase)\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.text(\n",
    "            ax.get_xlim()[0],\n",
    "            ax.get_ylim()[1] * height,\n",
    "            labels_subplots[i],\n",
    "            fontsize=fontsize,\n",
    "            **hfont,\n",
    "        )\n",
    "    return labels_subplots\n",
    "\n",
    "\n",
    "### this code it's exactly in  x weeks\n",
    "def merge_and_rename_data(data1, data2, on_column, suffix1, suffix2):\n",
    "    merged_data = pd.merge(\n",
    "        data1, data2, on=on_column, suffixes=(\"_\" + suffix1, \"_\" + suffix2)\n",
    "    )\n",
    "\n",
    "    new_column_names = [\n",
    "        col.replace(f\"_{on_column}_{suffix1}\", f\"_{suffix1}\").replace(\n",
    "            f\"_{on_column}_{suffix2}\", f\"_{suffix2}\"\n",
    "        )\n",
    "        for col in merged_data.columns\n",
    "    ]\n",
    "    merged_data.rename(\n",
    "        columns=dict(zip(merged_data.columns, new_column_names)), inplace=True\n",
    "    )\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def pivot_data_by_HSA(data, index_column, columns_column, values_column):\n",
    "    data_by_HSA = data[[index_column, columns_column, values_column]]\n",
    "    pivot_table = data_by_HSA.pivot_table(\n",
    "        index=index_column, columns=columns_column, values=values_column\n",
    "    )\n",
    "    return pivot_table\n",
    "\n",
    "\n",
    "def create_column_names(categories_for_subsetting, num_of_weeks):\n",
    "    column_names = [\"HSA_ID\"]\n",
    "\n",
    "    for week in range(1, num_of_weeks + 1):\n",
    "        week = num2words(week)\n",
    "        for category in categories_for_subsetting:\n",
    "            column_name = f\"week_{week}_{category}\"\n",
    "            column_names.append(column_name)\n",
    "\n",
    "    return column_names\n",
    "\n",
    "\n",
    "def create_collated_weekly_data(\n",
    "    pivoted_table, original_data, categories_for_subsetting, geography, column_names\n",
    "):\n",
    "    collated_data = pd.DataFrame(index=range(51), columns=column_names)\n",
    "\n",
    "    x = 0\n",
    "    for geo in original_data[geography].unique():\n",
    "        # matching_indices = [i for i, geo_col in enumerate(pivoted_table) if geo_col == geo]\n",
    "        collated_data.loc[x, geography] = geo\n",
    "        columns_to_subset = [\n",
    "            f\"{geo}_{category}\" for category in categories_for_subsetting\n",
    "        ]\n",
    "        j = 1\n",
    "        try:\n",
    "            for row in range(len(pivoted_table.loc[:, columns_to_subset])):\n",
    "                collated_data.iloc[\n",
    "                    x, j : j + len(categories_for_subsetting)\n",
    "                ] = pivoted_table.loc[row, columns_to_subset]\n",
    "                j += len(categories_for_subsetting)\n",
    "        except:\n",
    "            pass\n",
    "        x += 1\n",
    "\n",
    "    return collated_data\n",
    "\n",
    "\n",
    "def add_changes_by_week(weekly_data_frame, outcome_column):\n",
    "    for column in weekly_data_frame.columns[1:]:\n",
    "        # Calculate the difference between each row and the previous row\n",
    "        if outcome_column not in column.lower():  # want to leave out the outcome column\n",
    "            diff = weekly_data_frame[column].diff()\n",
    "\n",
    "            # Create a new column with the original column name and \"delta\"\n",
    "            new_column_name = column + \"_delta\"\n",
    "\n",
    "            column_index = weekly_data_frame.columns.get_loc(column)\n",
    "\n",
    "            # Insert the new column just after the original column\n",
    "            weekly_data_frame.insert(column_index + 1, new_column_name, diff)\n",
    "            weekly_data_frame[new_column_name] = diff\n",
    "    return weekly_data_frame\n",
    "\n",
    "\n",
    "### exactly \n",
    "def prep_training_test_data(\n",
    "    data, no_weeks, weeks_in_future, geography, weight_col, keep_output\n",
    "):\n",
    "    ## Get the weeks for the x and y datasets\n",
    "    x_weeks = []\n",
    "    y_weeks = []\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_future\n",
    "        x_weeks.append(\"_\" + num2words(week) + \"_\")\n",
    "        y_weeks.append(\"_\" + num2words(test_week) + \"_\")\n",
    "\n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all = pd.DataFrame()\n",
    "    missing_data = []\n",
    "    ## Now get the training data\n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "        y_week = y_weeks[k]\n",
    "        k += 1\n",
    "        weeks_x = [col for col in data.columns if x_week in col]\n",
    "        columns_x = [geography] + weeks_x + [weight_col]\n",
    "        data_x = data[columns_x]\n",
    "\n",
    "        weeks_y = [col for col in data.columns if y_week in col]\n",
    "        columns_y = [geography] + weeks_y\n",
    "        data_y = data[columns_y]\n",
    "        # ensure they have the same amount of data\n",
    "        # remove rows in test_data1 with NA in test_data2\n",
    "        data_x = data_x.dropna()\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        # remove rows in test_data2 with NA in test_data1\n",
    "        data_y = data_y.dropna()\n",
    "        data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "        missing_data.append(\n",
    "            (\n",
    "                (len(data[geography].unique()) - data_x_no_HSA)\n",
    "                / len(data[geography].unique())\n",
    "            )\n",
    "            * 100\n",
    "        )\n",
    "        # get weights\n",
    "        # weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "        X_week = data_x.iloc[:, 1 : len(columns_x)]  # take away y, leave weights for mo\n",
    "        y_week = data_y.iloc[:, -1]\n",
    "\n",
    "        y_week = y_week.astype(int)\n",
    "        weights = X_week.iloc[:, -1]\n",
    "        if keep_output:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 1\n",
    "            ]  # remove the weights and leave \"target\" for that week\n",
    "\n",
    "            # rename columns for concatenation\n",
    "            X_week.columns = range(1, len(data_x.columns) - 1)\n",
    "        else:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 2\n",
    "            ]  # remove the weights and  \"target\" for that week\n",
    "\n",
    "            X_week.columns = range(\n",
    "                1, len(data_x.columns) - 2\n",
    "            )  # remove the weights and  \"target\" for that week\n",
    "\n",
    "            # rename columns for concatenation\n",
    "        y_week.columns = range(1, len(data_y.columns) - 1)\n",
    "        X_data = pd.concat([X_data, X_week])\n",
    "        y_data = pd.concat([y_data, y_week])\n",
    "\n",
    "        weights_all = pd.concat([weights_all, weights])\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return (X_data, y_data, weights_all, missing_data)\n",
    "\n",
    "### this code it's ANY in the x week period\n",
    "def prep_training_test_data_period(\n",
    "    data, no_weeks, weeks_in_future, geography, weight_col, keep_output\n",
    "):\n",
    "    ## Get the weeks for the x and y datasets\n",
    "    x_weeks = []\n",
    "    y_weeks = []\n",
    "    y_weeks_to_check = []  # check these weeks to see if any of them are equal to 1\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_future\n",
    "        x_weeks.append(\"_\" + num2words(week) + \"_\")\n",
    "        for week_y in range(week + 1, test_week + 1):\n",
    "            y_weeks_to_check.append(\"_\" + num2words(week_y) + \"_\")\n",
    "        y_weeks.append(\"_\" + num2words(test_week) + \"_\")\n",
    "\n",
    "    ## Divide up the test/train split\n",
    "    # if is_geographic:\n",
    "    # Calculate the index to start slicing from\n",
    "    #    start_index = len(data['county']) // proportion[0] * proportion[1]\n",
    "    # Divide up the dataset based on this proportion\n",
    "    #    first_two_thirds = data['county'][:start_index]\n",
    "    #    last_third = data['county'][start_index:]\n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all = pd.DataFrame()\n",
    "    missing_data = []\n",
    "    ## Now get the training data\n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "        y_week = y_weeks[k]\n",
    "        k += 1\n",
    "\n",
    "        weeks_x = [col for col in data.columns if x_week in col]\n",
    "        columns_x = [geography] + weeks_x + [weight_col]\n",
    "        data_x = data[columns_x]\n",
    "\n",
    "        weeks_y = [col for col in data.columns if y_week in col]\n",
    "        columns_y = [geography] + weeks_y\n",
    "        data_y = data[columns_y]\n",
    "        ### now add the final column to the y data that has it so that it's if any week in the trhee week perdiod exceeded 15\n",
    "        train_week = w2n.word_to_num(x_week.replace(\"_\", \"\"))\n",
    "        target_week = w2n.word_to_num(y_week.replace(\"_\", \"\"))\n",
    "        y_weeks_to_check = []\n",
    "        for week_to_check in range(train_week + 1, target_week + 1):\n",
    "            y_weeks_to_check.append(\"_\" + num2words(week_to_check) + \"_\")\n",
    "\n",
    "        y_weeks_to_check = [week + \"beds_over_15_100k\" for week in y_weeks_to_check]\n",
    "        columns_to_check = [\n",
    "            col for col in data.columns if any(week in col for week in y_weeks_to_check)\n",
    "        ]\n",
    "        y_over_in_period = data[columns_to_check].apply(max, axis=1)\n",
    "        data_y = pd.concat([data_y, y_over_in_period], axis=1)\n",
    "        # ensure they have the same amount of data\n",
    "        # remove rows in test_data1 with NA in test_data2\n",
    "        data_x = data_x.dropna()\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        # remove rows in test_data2 with NA in test_data1\n",
    "        data_y = data_y.dropna()\n",
    "        data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "        missing_data.append(\n",
    "            (\n",
    "                (len(data[geography].unique()) - data_x_no_HSA)\n",
    "                / len(data[geography].unique())\n",
    "            )\n",
    "            * 100\n",
    "        )\n",
    "        # get weights\n",
    "        # weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "        X_week = data_x.iloc[:, 1 : len(columns_x)]  # take away y, leave weights for mo\n",
    "        y_week = data_y.iloc[:, -1]\n",
    "\n",
    "        y_week = y_week.astype(int)\n",
    "        weights = X_week.iloc[:, -1]\n",
    "        if keep_output:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 1\n",
    "            ]  # remove the weights and leave \"target\" for that week\n",
    "\n",
    "            # rename columns for concatenation\n",
    "            X_week.columns = range(1, len(data_x.columns) - 1)\n",
    "        else:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 2\n",
    "            ]  # remove the weights and  \"target\" for that week\n",
    "\n",
    "            X_week.columns = range(\n",
    "                1, len(data_x.columns) - 2\n",
    "            )  # remove the weights and  \"target\" for that week\n",
    "\n",
    "        y_week.columns = range(1, len(data_y.columns) - 2)\n",
    "        X_data = pd.concat([X_data, X_week])\n",
    "        y_data = pd.concat([y_data, y_week])\n",
    "\n",
    "        weights_all = pd.concat([weights_all, weights])\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return (X_data, y_data, weights_all, missing_data)\n",
    "\n",
    "\n",
    "def calculate_metrics(confusion_matrix):\n",
    "    # Extract values from the confusion matrix\n",
    "    TP = confusion_matrix[1, 1]\n",
    "    FP = confusion_matrix[0, 1]\n",
    "    TN = confusion_matrix[0, 0]\n",
    "    FN = confusion_matrix[1, 0]\n",
    "\n",
    "    # Calculate Sensitivity (True Positive Rate), Specificity (True Negative Rate),\n",
    "    # PPV (Precision), and NPV\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    ppv = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    npv = TN / (TN + FN) if (TN + FN) > 0 else 0.0\n",
    "\n",
    "    return sensitivity, specificity, ppv, npv\n",
    "\n",
    "\n",
    "def merge_and_rename_data(data1, data2, on_column, suffix1, suffix2):\n",
    "    merged_data = pd.merge(\n",
    "        data1, data2, on=on_column, suffixes=(\"_\" + suffix1, \"_\" + suffix2)\n",
    "    )\n",
    "\n",
    "    new_column_names = [\n",
    "        col.replace(f\"_{on_column}_{suffix1}\", f\"_{suffix1}\").replace(\n",
    "            f\"_{on_column}_{suffix2}\", f\"_{suffix2}\"\n",
    "        )\n",
    "        for col in merged_data.columns\n",
    "    ]\n",
    "    merged_data.rename(\n",
    "        columns=dict(zip(merged_data.columns, new_column_names)), inplace=True\n",
    "    )\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def pivot_data_by_HSA(data, index_column, columns_column, values_column):\n",
    "    data_by_HSA = data[[index_column, columns_column, values_column]]\n",
    "    pivot_table = data_by_HSA.pivot_table(\n",
    "        index=index_column, columns=columns_column, values=values_column\n",
    "    )\n",
    "    return pivot_table\n",
    "\n",
    "\n",
    "def add_changes_by_week(weekly_data_frame, outcome_column):\n",
    "    for column in weekly_data_frame.columns[1:]:\n",
    "        # Calculate the difference between each row and the previous row\n",
    "        if outcome_column not in column.lower():  # want to leave out the outcome column\n",
    "            diff = weekly_data_frame[column].diff()\n",
    "\n",
    "            # Create a new column with the original column name and \"delta\"\n",
    "            new_column_name = column + \"_delta\"\n",
    "\n",
    "            column_index = weekly_data_frame.columns.get_loc(column)\n",
    "\n",
    "            # Insert the new column just after the original column\n",
    "            weekly_data_frame.insert(column_index + 1, new_column_name, diff)\n",
    "            weekly_data_frame[new_column_name] = diff\n",
    "    return weekly_data_frame\n",
    "\n",
    "\n",
    "def determine_covid_outcome_indicator(\n",
    "    new_cases_per_100k, new_admits_per_100k, percent_beds_100k\n",
    "):\n",
    "    if new_cases_per_100k < 200:\n",
    "        if (new_admits_per_100k >= 10) | (\n",
    "            percent_beds_100k > 0.10\n",
    "        ):  # Changed .10 to 0.10\n",
    "            if (new_admits_per_100k >= 20) | (percent_beds_100k >= 15):\n",
    "                return \"High\"\n",
    "            else:\n",
    "                return \"Medium\"\n",
    "        else:\n",
    "            return \"Low\"\n",
    "    elif new_cases_per_100k >= 200:\n",
    "        if (new_admits_per_100k >= 10) | (\n",
    "            percent_beds_100k >= 0.10\n",
    "        ):  # Changed .10 to 0.10\n",
    "            return \"High\"\n",
    "        elif (new_admits_per_100k < 10) | (percent_beds_100k < 10):\n",
    "            return \"Medium\"\n",
    "\n",
    "\n",
    "def simplify_labels_graphviz(graph):\n",
    "    for node in graph.get_node_list():\n",
    "        if node.get_attributes().get(\"label\") is None:\n",
    "            continue\n",
    "        else:\n",
    "            split_label = node.get_attributes().get(\"label\").split(\"<br/>\")\n",
    "            if len(split_label) == 4:\n",
    "                split_label[3] = split_label[3].split(\"=\")[1].strip()\n",
    "\n",
    "                del split_label[1]  # number of samples\n",
    "                del split_label[1]  # split of sample\n",
    "            elif len(split_label) == 3:  # for a terminating node, no rule is provided\n",
    "                split_label[2] = split_label[2].split(\"=\")[1].strip()\n",
    "\n",
    "                del split_label[0]  # number of samples\n",
    "                del split_label[0]  # split of samples\n",
    "                split_label[0] = \"<\" + split_label[0]\n",
    "            node.set(\"label\", \"<br/>\".join(split_label))\n",
    "\n",
    "\n",
    "def generate_decision_tree_graph(classifier, class_names, feature_names):\n",
    "    dot_data = StringIO()\n",
    "    tree.export_graphviz(\n",
    "        classifier,\n",
    "        out_file=dot_data,\n",
    "        class_names=class_names,\n",
    "        feature_names=feature_names,\n",
    "        filled=True,\n",
    "        rounded=True,\n",
    "        special_characters=True,\n",
    "        proportion=False,\n",
    "        precision=0,\n",
    "        impurity=False,\n",
    "    )\n",
    "\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def cross_validation_leave_geo_out(\n",
    "    data,\n",
    "    geography_column,\n",
    "    geo_split,\n",
    "    no_iterations,\n",
    "    cv,\n",
    "    classifier,\n",
    "    param_grid,\n",
    "    no_iterations_param,\n",
    "    no_weeks_train,\n",
    "    no_weeks_test,\n",
    "    weeks_in_future,\n",
    "    weight_col,\n",
    "    keep_output,\n",
    "    time_period,\n",
    "):\n",
    "    best_hyperparameters_per_iter = []\n",
    "    auROC_per_iter = []\n",
    "\n",
    "    for i in range(no_iterations):\n",
    "        print(i)\n",
    "        # Subset the HSAs from the full dataset\n",
    "        geo_names = data[geography_column].unique()\n",
    "        num_names_to_select = int(geo_split * len(geo_names))\n",
    "        geos_for_sample = random.sample(list(geo_names), num_names_to_select)\n",
    "        subset_HSAs_for_train = data[data[geography_column].isin(geos_for_sample)]\n",
    "        subset_HSAs_for_test = data[~data[geography_column].isin(geos_for_sample)]\n",
    "\n",
    "        # Create training and test data\n",
    "        if time_period == \"period\":\n",
    "            (\n",
    "                X_sample_train,\n",
    "                y_sample_train,\n",
    "                weights_train,\n",
    "                missing_data_train_HSA,\n",
    "            ) = prep_training_test_data_period(\n",
    "                subset_HSAs_for_train,\n",
    "                no_weeks=no_weeks_train,\n",
    "                weeks_in_future=weeks_in_future,\n",
    "                geography=geography_column,\n",
    "                weight_col=weight_col,\n",
    "                keep_output=keep_output,\n",
    "            )\n",
    "            (\n",
    "                X_sample_test,\n",
    "                y_sample_test,\n",
    "                weights_test,\n",
    "                missing_data_train_HSA,\n",
    "            ) = prep_training_test_data_period(\n",
    "                subset_HSAs_for_test,\n",
    "                no_weeks=no_weeks_test,\n",
    "                weeks_in_future=weeks_in_future,\n",
    "                geography=geography_column,\n",
    "                weight_col=weight_col,\n",
    "                keep_output=keep_output,\n",
    "            )\n",
    "            weights_train = weights_train[0]\n",
    "        elif time_period == \"exact\":\n",
    "            (\n",
    "                X_sample_train,\n",
    "                y_sample_train,\n",
    "                weights_train,\n",
    "                missing_data_train_HSA,\n",
    "            ) = prep_training_test_data(\n",
    "                subset_HSAs_for_train,\n",
    "                no_weeks=no_weeks_train,\n",
    "                weeks_in_future=weeks_in_future,\n",
    "                geography=geography_column,\n",
    "                weight_col=weight_col,\n",
    "                keep_output=keep_output,\n",
    "            )\n",
    "            (\n",
    "                X_sample_test,\n",
    "                y_sample_test,\n",
    "                weights_test,\n",
    "                missing_data_train_HSA,\n",
    "            ) = prep_training_test_data(\n",
    "                subset_HSAs_for_test,\n",
    "                no_weeks=no_weeks_test,\n",
    "                weeks_in_future=weeks_in_future,\n",
    "                geography=geography_column,\n",
    "                weight_col=weight_col,\n",
    "                keep_output=keep_output,\n",
    "            )\n",
    "            weights_train = weights_train[0]\n",
    "        elif time_period == \"shifted\":\n",
    "            (\n",
    "                X_sample_train,\n",
    "                y_sample_train,\n",
    "                weights_train,\n",
    "                missing_data_train_HSA,\n",
    "            ) = prep_training_test_data_shifted(\n",
    "                subset_HSAs_for_train,\n",
    "                no_weeks=no_weeks_train,\n",
    "                weeks_in_future=weeks_in_future,\n",
    "                geography=geography_column,\n",
    "                weight_col=weight_col,\n",
    "                keep_output=keep_output,\n",
    "            )\n",
    "            (\n",
    "                X_sample_test,\n",
    "                y_sample_test,\n",
    "                weights_test,\n",
    "                missing_data_train_HSA,\n",
    "            ) = prep_training_test_data_shifted(\n",
    "                subset_HSAs_for_test,\n",
    "                no_weeks=no_weeks_test,\n",
    "                weeks_in_future=weeks_in_future,\n",
    "                geography=geography_column,\n",
    "                weight_col=weight_col,\n",
    "                keep_output=keep_output,\n",
    "            )\n",
    "            weights_train = weights_train[0]\n",
    "\n",
    "        # Check if y_sample_test contains only 1's\n",
    "        while (int(y_sample_test.sum().iloc[0]) / len(y_sample_test)) == 1:\n",
    "            print(\"All 1\")\n",
    "            # Subset the HSAs from the full dataset\n",
    "            geo_names = data[geography_column].unique()\n",
    "            num_names_to_select = int(geo_split * len(geo_names))\n",
    "            geos_for_sample = random.sample(list(geo_names), num_names_to_select)\n",
    "            subset_HSAs_for_train = data[data[geography_column].isin(geos_for_sample)]\n",
    "            subset_HSAs_for_test = data[~data[geography_column].isin(geos_for_sample)]\n",
    "\n",
    "            # Create training and test data\n",
    "            if time_period == \"period\":\n",
    "                (\n",
    "                    X_sample_train,\n",
    "                    y_sample_train,\n",
    "                    weights_train,\n",
    "                    missing_data_train_HSA,\n",
    "                ) = prep_training_test_data_period(\n",
    "                    subset_HSAs_for_train,\n",
    "                    no_weeks=no_weeks_train,\n",
    "                    weeks_in_future=weeks_in_future,\n",
    "                    geography=geography_column,\n",
    "                    weight_col=weight_col,\n",
    "                    keep_output=keep_output,\n",
    "                )\n",
    "                (\n",
    "                    X_sample_test,\n",
    "                    y_sample_test,\n",
    "                    weights_test,\n",
    "                    missing_data_train_HSA,\n",
    "                ) = prep_training_test_data_period(\n",
    "                    subset_HSAs_for_test,\n",
    "                    no_weeks=no_weeks_test,\n",
    "                    weeks_in_future=weeks_in_future,\n",
    "                    geography=geography_column,\n",
    "                    weight_col=weight_col,\n",
    "                    keep_output=keep_output,\n",
    "                )\n",
    "                weights_train = weights_train[0]\n",
    "            elif time_period == \"exact\":\n",
    "                (\n",
    "                    X_sample_train,\n",
    "                    y_sample_train,\n",
    "                    weights_train,\n",
    "                    missing_data_train_HSA,\n",
    "                ) = prep_training_test_data(\n",
    "                    subset_HSAs_for_train,\n",
    "                    no_weeks=no_weeks_train,\n",
    "                    weeks_in_future=weeks_in_future,\n",
    "                    geography=geography_column,\n",
    "                    weight_col=weight_col,\n",
    "                    keep_output=keep_output,\n",
    "                )\n",
    "                (\n",
    "                    X_sample_test,\n",
    "                    y_sample_test,\n",
    "                    weights_test,\n",
    "                    missing_data_train_HSA,\n",
    "                ) = prep_training_test_data(\n",
    "                    subset_HSAs_for_test,\n",
    "                    no_weeks=no_weeks_test,\n",
    "                    weeks_in_future=weeks_in_future,\n",
    "                    geography=geography_column,\n",
    "                    weight_col=weight_col,\n",
    "                    keep_output=keep_output,\n",
    "                )\n",
    "                weights_train = weights_train[0]\n",
    "            elif time_period == \"shifted\":\n",
    "                (\n",
    "                    X_sample_train,\n",
    "                    y_sample_train,\n",
    "                    weights_train,\n",
    "                    missing_data_train_HSA,\n",
    "                ) = prep_training_test_data_shifted(\n",
    "                    subset_HSAs_for_train,\n",
    "                    no_weeks=no_weeks_train,\n",
    "                    weeks_in_future=weeks_in_future,\n",
    "                    geography=geography_column,\n",
    "                    weight_col=weight_col,\n",
    "                    keep_output=keep_output,\n",
    "                )\n",
    "                (\n",
    "                    X_sample_test,\n",
    "                    y_sample_test,\n",
    "                    weights_test,\n",
    "                    missing_data_train_HSA,\n",
    "                ) = prep_training_test_data_shifted(\n",
    "                    subset_HSAs_for_test,\n",
    "                    no_weeks=no_weeks_test,\n",
    "                    weeks_in_future=weeks_in_future,\n",
    "                    geography=geography_column,\n",
    "                    weight_col=weight_col,\n",
    "                    keep_output=keep_output,\n",
    "                )\n",
    "                weights_train = weights_train[0]\n",
    "\n",
    "        random_search = RandomizedSearchCV(\n",
    "            classifier, param_grid, n_iter=no_iterations_param, cv=cv, random_state=10\n",
    "        )\n",
    "        random_search.fit(X_sample_train, y_sample_train, sample_weight=weights_train)\n",
    "        best_params = random_search.best_params_\n",
    "\n",
    "        # Create the Decision Tree classifier with the best hyperparameters\n",
    "        model = DecisionTreeClassifier(\n",
    "            **best_params, random_state=10, class_weight=\"balanced\"\n",
    "        )\n",
    "        model_fit = model.fit(\n",
    "            X_sample_train, y_sample_train, sample_weight=weights_train\n",
    "        )\n",
    "        y_pred = model_fit.predict_proba(X_sample_test)\n",
    "\n",
    "        # Evaluate the accuracy of the model\n",
    "        best_hyperparameters_per_iter.append(best_params)\n",
    "        auROC_per_iter.append(roc_auc_score(y_sample_test, y_pred[:, 1]))\n",
    "\n",
    "    return best_hyperparameters_per_iter[np.argmax(np.array(auROC_per_iter))]\n",
    "\n",
    "\n",
    "def LOOCV_by_HSA_dataset(dataframe, geo_ID, geo_ID_col):\n",
    "    training_dataframe = dataframe[dataframe[geo_ID_col] != geo_ID]\n",
    "    testing_dataframe = dataframe[dataframe[geo_ID_col] == geo_ID]\n",
    "    return training_dataframe, testing_dataframe\n",
    "\n",
    "\n",
    "def save_in_HSA_dictionary(\n",
    "    prediction_week,\n",
    "    ROC_by_week,\n",
    "    accuracy_by_week,\n",
    "    sensitivity_by_week,\n",
    "    specificity_by_week,\n",
    "    ppv_by_week,\n",
    "    npv_by_week,\n",
    "    ROC_by_HSA,\n",
    "    accuracy_by_HSA,\n",
    "    sensitivity_by_HSA,\n",
    "    specificity_by_HSA,\n",
    "    ppv_by_HSA,\n",
    "    npv_by_HSA,\n",
    "):\n",
    "    ROC_by_HSA[prediction_week] = ROC_by_week\n",
    "    accuracy_by_HSA[prediction_week] = accuracy_by_week\n",
    "    sensitivity_by_HSA[prediction_week] = sensitivity_by_week\n",
    "    specificity_by_HSA[prediction_week] = specificity_by_week\n",
    "    ppv_by_HSA[prediction_week] = ppv_by_week\n",
    "    npv_by_HSA[prediction_week] = npv_by_week\n",
    "\n",
    "\n",
    "def prep_training_test_data_shifted(\n",
    "    data, no_weeks, weeks_in_future, geography, weight_col, keep_output\n",
    "):\n",
    "    ## Get the weeks for the x and y datasets\n",
    "    x_weeks = []\n",
    "    y_weeks = []\n",
    "    y_weeks_to_check = []  # check these weeks to see if any of them are equal to 1\n",
    "    for week in no_weeks:\n",
    "        test_week = int(week) + weeks_in_future\n",
    "        x_weeks.append(\"_\" + num2words(week) + \"_\")\n",
    "        for week_y in range(week + 2, test_week + 2):\n",
    "            y_weeks_to_check.append(\"_\" + num2words(week_y) + \"_\")\n",
    "        y_weeks.append(\"_\" + num2words(test_week) + \"_\")\n",
    "    ## Divide up the test/train split\n",
    "    # if is_geographic:\n",
    "    # Calculate the index to start slicing from\n",
    "    #    start_index = len(data['county']) // proportion[0] * proportion[1]\n",
    "    # Divide up the dataset based on this proportion\n",
    "    #    first_two_thirds = data['county'][:start_index]\n",
    "    #    last_third = data['county'][start_index:]\n",
    "    X_data = pd.DataFrame()\n",
    "    y_data = pd.DataFrame()\n",
    "    weights_all = pd.DataFrame()\n",
    "    missing_data = []\n",
    "    ## Now get the training data\n",
    "    k = 0\n",
    "    for x_week in x_weeks:\n",
    "        y_week = y_weeks[k]\n",
    "        k += 1\n",
    "\n",
    "        weeks_x = [col for col in data.columns if x_week in col]\n",
    "        columns_x = [geography] + weeks_x + [weight_col]\n",
    "        data_x = data[columns_x]\n",
    "\n",
    "        weeks_y = [col for col in data.columns if y_week in col]\n",
    "        columns_y = [geography] + weeks_y\n",
    "        data_y = data[columns_y]\n",
    "        ### now add the final column to the y data that has it so that it's if any week in the trhee week perdiod exceeded 15\n",
    "        train_week = w2n.word_to_num(x_week.replace(\"_\", \"\"))\n",
    "        target_week = w2n.word_to_num(y_week.replace(\"_\", \"\"))\n",
    "        y_weeks_to_check = []\n",
    "        for week_to_check in range(\n",
    "            train_week + 2, target_week + 2\n",
    "        ):  # have to ensure you skip the next week for getting the excess\n",
    "            y_weeks_to_check.append(\"_\" + num2words(week_to_check) + \"_\")\n",
    "            print(y_weeks_to_check)\n",
    "        y_weeks_to_check = [week + \"beds_over_15_100k\" for week in y_weeks_to_check]\n",
    "        columns_to_check = [\n",
    "            col for col in data.columns if any(week in col for week in y_weeks_to_check)\n",
    "        ]\n",
    "        y_over_in_period = data[columns_to_check].apply(max, axis=1)\n",
    "        data_y = pd.concat([data_y, y_over_in_period], axis=1)\n",
    "        # ensure they have the same amount of data\n",
    "        # remove rows in test_data1 with NA in test_data2\n",
    "        data_x = data_x.dropna()\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        # remove rows in test_data2 with NA in test_data1\n",
    "        data_y = data_y.dropna()\n",
    "        data_y = data_y[data_y[geography].isin(data_x[geography])]\n",
    "        data_x = data_x[data_x[geography].isin(data_y[geography])]\n",
    "        data_x_no_HSA = len(data_x[geography].unique())\n",
    "\n",
    "        missing_data.append(\n",
    "            (\n",
    "                (len(data[geography].unique()) - data_x_no_HSA)\n",
    "                / len(data[geography].unique())\n",
    "            )\n",
    "            * 100\n",
    "        )\n",
    "        # get weights\n",
    "        # weights = weight_data[weight_data[geography].isin(data_x[geography])][[geography, weight_col]]\n",
    "\n",
    "        X_week = data_x.iloc[:, 1 : len(columns_x)]  # take away y, leave weights for mo\n",
    "        y_week = data_y.iloc[:, -1]\n",
    "\n",
    "        y_week = y_week.astype(int)\n",
    "\n",
    "        weights = X_week.iloc[:, -1]\n",
    "        if keep_output:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 1\n",
    "            ]  # remove the weights and leave \"target\" for that week\n",
    "\n",
    "            # rename columns for concatenation\n",
    "            X_week.columns = range(1, len(data_x.columns) - 1)\n",
    "        else:\n",
    "            X_week = X_week.iloc[\n",
    "                :, : len(X_week.columns) - 2\n",
    "            ]  # remove the weights and  \"target\" for that week\n",
    "\n",
    "            X_week.columns = range(\n",
    "                1, len(data_x.columns) - 2\n",
    "            )  # remove the weights and  \"target\" for that week\n",
    "\n",
    "        y_week.columns = range(1, len(data_y.columns) - 2)\n",
    "        X_data = pd.concat([X_data, X_week])\n",
    "        y_data = pd.concat([y_data, y_week])\n",
    "\n",
    "        weights_all = pd.concat([weights_all, weights])\n",
    "\n",
    "    X_data.reset_index(drop=True, inplace=True)\n",
    "    y_data.reset_index(drop=True, inplace=True)\n",
    "    weights_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return (X_data, y_data, weights_all, missing_data)\n",
    "\n",
    "\n",
    "def LOOCV_by_HSA_dataset(dataframe, geo_ID, geo_ID_col):\n",
    "    training_dataframe = dataframe[dataframe[geo_ID_col] != geo_ID]\n",
    "    testing_dataframe = dataframe[dataframe[geo_ID_col] == geo_ID]\n",
    "    return training_dataframe, testing_dataframe\n",
    "\n",
    "\n",
    "def save_in_HSA_dictionary(\n",
    "    prediction_week,\n",
    "    ROC_by_week,\n",
    "    accuracy_by_week,\n",
    "    sensitivity_by_week,\n",
    "    specificity_by_week,\n",
    "    ppv_by_week,\n",
    "    npv_by_week,\n",
    "    ROC_by_HSA,\n",
    "    accuracy_by_HSA,\n",
    "    sensitivity_by_HSA,\n",
    "    specificity_by_HSA,\n",
    "    ppv_by_HSA,\n",
    "    npv_by_HSA,\n",
    "):\n",
    "    ROC_by_HSA[prediction_week] = ROC_by_week\n",
    "    accuracy_by_HSA[prediction_week] = accuracy_by_week\n",
    "    sensitivity_by_HSA[prediction_week] = sensitivity_by_week\n",
    "    specificity_by_HSA[prediction_week] = specificity_by_week\n",
    "    ppv_by_HSA[prediction_week] = ppv_by_week\n",
    "    npv_by_HSA[prediction_week] = npv_by_week\n",
    "\n",
    "\n",
    "######### IMPORT DATA ##############\n",
    "#HSA_weekly_data_all = pd.read_csv(\n",
    "#    \"/Users/rem76/Documents/COVID_projections/Exact_analysis_smaller_hyperparameters/Expanding_models_15_per_100k/hsa_time_data_all_dates_CDC_features_only_incl_NA.csv\"\n",
    "#)\n",
    "\n",
    "HSA_weekly_data_all = pd.read_csv(\"/Users/rem76/Documents/COVID_projections/hsa_time_data_all_dates_weekly_incl_NA.csv\")\n",
    "columns_to_remove = [col for col in HSA_weekly_data_all.columns if 'cases' in col]\n",
    "HSA_weekly_data_all = HSA_weekly_data_all.drop(columns=columns_to_remove)\n",
    "\n",
    "columns_to_remove = [col for col in HSA_weekly_data_all.columns if 'deaths' in col]\n",
    "HSA_weekly_data_all = HSA_weekly_data_all.drop(columns=columns_to_remove)\n",
    "\n",
    "\n",
    "\n",
    "########### SET UP FOR EXPANDING MODELS\n",
    "clf = DecisionTreeClassifier(random_state=10, class_weight=\"balanced\")\n",
    "\n",
    "\n",
    "no_iterations = 10\n",
    "geography_column = \"HSA_ID\"\n",
    "geo_split = 0.9\n",
    "time_period = \"exact\"  # Choose 'period', 'exact', or 'shifted'\n",
    "size_of_test_dataset = 1\n",
    "train_weeks_for_initial_model = 1\n",
    "\n",
    "weeks_in_future = 3\n",
    "weight_col = \"weight\"\n",
    "keep_output = True\n",
    "\n",
    "no_iterations_param = 6 # Replace with the number of iterations for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": np.arange(2, 5, 1)\n",
    "} \n",
    "# Create the Decision Tree classifier\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=10, n_repeats=10, random_state=1\n",
    ")  ## 10-fold cross validations\n",
    "\n",
    "\n",
    "######### ACTUAL RUNS ############\n",
    "weeks_to_predict = [job_number] \n",
    "ROC_by_week_full_period = []\n",
    "sensitivity_by_week_full_period = []\n",
    "specificity_by_week_full_period = []\n",
    "ppv_by_week_full_period = []\n",
    "npv_by_week_full_period = []\n",
    "accuracy_by_week_full_period = []\n",
    "norm_MCC_by_week_full_period = []\n",
    "weeks_to_predict = range(1, 123,1)\n",
    "for prediction_week in weeks_to_predict:\n",
    "    print(prediction_week)\n",
    "    no_weeks_train = range(1, int(prediction_week + train_weeks_for_initial_model) + 1)\n",
    "    no_weeks_test = range(\n",
    "        int(prediction_week + train_weeks_for_initial_model) + 1,\n",
    "        int(prediction_week + train_weeks_for_initial_model + size_of_test_dataset) + 1,\n",
    "    )\n",
    "    (\n",
    "        X_train_full_period,\n",
    "        y_train_full_period,\n",
    "        weights_full_period,\n",
    "        missing_data_train_HSA,\n",
    "    ) = prep_training_test_data(\n",
    "        HSA_weekly_data_all,\n",
    "        no_weeks=no_weeks_train,\n",
    "        weeks_in_future=weeks_in_future,\n",
    "        geography=geography_column,\n",
    "        weight_col=weight_col,\n",
    "        keep_output=keep_output,\n",
    "    )\n",
    "\n",
    "    (\n",
    "        X_test_full_period,\n",
    "        y_test_full_period,\n",
    "        weights_test_full_period,\n",
    "        missing_data_test_HSA,\n",
    "    ) = prep_training_test_data(\n",
    "        HSA_weekly_data_all,\n",
    "        no_weeks=no_weeks_test,\n",
    "        weeks_in_future=weeks_in_future,\n",
    "        geography=geography_column,\n",
    "        weight_col=weight_col,\n",
    "        keep_output=keep_output,\n",
    "    )\n",
    "    weights_full_period = weights_full_period[0].to_numpy()\n",
    "    best_params = cross_validation_leave_geo_out(\n",
    "        HSA_weekly_data_all,\n",
    "        geography_column=geography_column,\n",
    "        geo_split=geo_split,\n",
    "        no_iterations=no_iterations,\n",
    "        cv=cv,\n",
    "        classifier=clf,\n",
    "        param_grid=param_grid,\n",
    "        no_iterations_param=no_iterations_param,\n",
    "        no_weeks_train=no_weeks_train,\n",
    "        no_weeks_test=no_weeks_test,\n",
    "        weeks_in_future=weeks_in_future,\n",
    "        weight_col=weight_col,\n",
    "        keep_output=keep_output,\n",
    "        time_period=time_period,\n",
    "    )\n",
    "    clf = DecisionTreeClassifier(\n",
    "        **best_params, random_state=10, class_weight=\"balanced\"\n",
    "    )\n",
    "    clf.fit(\n",
    "        X_train_full_period, y_train_full_period, sample_weight=weights_full_period\n",
    "    )\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = clf.predict(X_test_full_period)\n",
    "    y_pred_proba = clf.predict_proba(X_test_full_period)\n",
    "\n",
    "    # Evaluate the accuracy of the model\n",
    "    accuracy_by_week_full_period.append(accuracy_score(y_test_full_period, y_pred))\n",
    "    ROC_by_week_full_period.append(\n",
    "        roc_auc_score(y_test_full_period, y_pred_proba[:, 1])\n",
    "    )\n",
    "    conf_matrix = confusion_matrix(y_test_full_period, y_pred)\n",
    "\n",
    "    model_name_to_save = (\n",
    "        \"/Users/rem76/Documents/COVID_projections/Exact_analysis_smaller_hyperparameters/No_cases_no_deaths/No_cases_no_deaths\" + time_period + \"_\" + str(prediction_week) + \".sav\"\n",
    "    )\n",
    "\n",
    "    pickle.dump(clf, open(model_name_to_save, \"wb\"))\n",
    "    sensitvity, specificity, ppv, npv = calculate_metrics(conf_matrix)\n",
    "    specificity_by_week_full_period.append(specificity)\n",
    "    # Calculate sensitivity (true positive rate)\n",
    "    sensitivity_by_week_full_period.append(sensitvity)\n",
    "    norm_MCC_by_week_full_period.append(\n",
    "        (matthews_corrcoef(y_test_full_period, y_pred) + 1) / 2\n",
    "    )\n",
    "\n",
    "    ppv_by_week_full_period.append(ppv)\n",
    "    npv_by_week_full_period.append(npv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COVID_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a72d884e172d11118bfdfb578e108fb6b63a679c74d3d7d2f9d493a9a72737c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
